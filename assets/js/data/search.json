[ { "title": "Netstat command to troubleshoot network issues", "url": "/posts/netstat/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-11-03 00:00:00 +0545", "snippet": "The netstat is used for Network Monitoring and Troubleshooting. It stands for the Network Statistics. The netstat is a command-line network utility that displays: network connections for tcp, udp routing tables a number of network interface network protocol statisticsTo identify no. of connection on a given port or IP. Syntax: netstat -[OPTIONS] Input: netstat -putan | grep &amp;lt;PORT/IP&amp;gt; where, t = tcp u = udp n = numerical addr l = listening ports p = PID To see all the sockets Syntax: netstat -aTo see all the TCP ports Syntax: netstat -atTo see all the TCP v6 ports Syntax: netstat -6atTO list all the UDP ports Syntax: netstat -auTo list all the listening ports Syntax: netstat -lTO view the numerical address Syntax: netstat -lnTo view the routing table Syntax: netstat -rTo view the PID of the programme of connection Syntax: netstat -pTo check the no of connection from a specific IP address. Syntax: netstat -an grep To get the list of all the interface Syntax: netstat -iWhich port a process is using? Syntax: netstat -ap grep How to see statistics by protocol? Syntax: netstat -s" }, { "title": "Linux Redirects", "url": "/posts/linux-redirects/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-11-03 00:00:00 +0545", "snippet": "There are 3 redirects: Basically, when we run a command in terminal, belwo three files are created. Company File Descriptor Standard Input (stdin) 0 Standard Output (stdout) 1 Standard Error (stderr) 2 Output (stdout-1) Output of a command is shown in terminal. To route output in file using &amp;gt;. This clears all the previous content and then overwrites with the new content. hostname &amp;gt; file_name To append output in existing file using &amp;gt;&amp;gt;. This appends the new content to the previous existing content. pwd » file_name Error (stderr - 2) If any command gives your error then it is considered as stderr-2 We can redirect the error to a file: Eg: cd /root/ 2&amp;gt;error_file This prevents the error from showing up in the terminal but writes all the error in a file. To redirect both standard output and error to a file:Eg:- Input: hostname &amp;gt;&amp;gt; std_err_out 2&amp;gt;&amp;amp;1 Input: cat std_err_out Output: &amp;lt;Host Name of the Computer&amp;gt; Input: cd /root/ &amp;gt;&amp;gt; std_err_out 2&amp;gt;&amp;amp;1 Input cat std_err_out Output: &amp;lt;Host Name Of The Computer&amp;gt; &amp;lt;Error&amp;gt; Input (stdin - 0) Input is used when feeding file contents to a file Eg:- cat &amp;lt; file_name cat &amp;lt;&amp;lt; EOF " }, { "title": "Cron Job Linux", "url": "/posts/cron/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-11-03 00:00:00 +0545", "snippet": "The software utility cron also known as cron job is a time-based job scheduler in Unix-like computer operating systems. Users who set up and maintain software environments use cron to schedule jobs to run periodicallly at fixed times, day, dates, or intervals.crontab commandThe crontab command is used to schedule commands to be executed periodically. Crontab uses a daemon, crond, which keeps running in background and checks once a minute to see if any of the scheduled jobs need to be executed. crontab -l: To show all the current jobs crontab -e: TO edit or add new jobsCron Job Format Visit this link for Cron Expression Examples: https://crontab.guru/" }, { "title": "Chmod and Permissions", "url": "/posts/chmod/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-11-03 00:00:00 +0545", "snippet": "Types of Permissions in Linux r: read w: write x: executePermissions (rwx) levels u: current user g: group in which user belongs to o: others a: allHow to chagne permissions?To change the permissions in the file or directory we use the chmod command. Syntax: chmod u+r &amp;lt;file_name&amp;gt; (For adding reading permission to the current user) chmid u-r &amp;lt;file_name&amp;gt; (For removing reading permission from the current user) chmod ugo+r &amp;lt;file_name&amp;gt; (For adding reading permisison to the current_user, group and others) chmod ugo+r &amp;lt;file_name&amp;gt; (For removing reading permisison from the current_user, group and others) chmod a+rwx &amp;lt;file_name&amp;gt; (adding permission for all) Syntax: Note: One user cannot change the permission of the files and directories of another user unless it’s the root user.Chmod Numeric ModeSyntax: chmod 756 &amp;lt;file_name&amp;gt; where, 7 = user, 5 = group 6 = other user" }, { "title": "Linux File Ownership Command (chown, chgrp)", "url": "/posts/File-Ownership/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-11-03 00:00:00 +0545", "snippet": " chown: To change the user ownership Input: chown -c &amp;lt;user&amp;gt; &amp;lt;filename&amp;gt; Ouput: Changed the owner ship of the filename to the given user. chgrp: To change the group ownership Input: chgrp -c &amp;lt;group&amp;gt; &amp;lt;filename&amp;gt; Ouput: Changed the owner ship of the filename to the given user." }, { "title": "Top Command In Linux", "url": "/posts/top/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-29 00:00:00 +0545", "snippet": "The top (table of processes) command shows a real-time view of the running processes in Linux and displays kernel-managed tasks.The command also provides a system information summmary that shows resource utilization, including CPU and memory usage.Some of the operations we can perform along with top commands are: top then c: shows commands absolute path top then k: kills a process by PID top then n: to change the no of task. displayed top then d,s: to change interval of refresh top then M: to sort the running processes by memory usage top then r: we can change the nice value of a PID top then u: to filter task by user top then f: Field Management top then h: help top then z or b: Toggle: ‘z’ color/mono; ‘b’ bold/reverse top then x y: `x’ sort field; ‘y’: runnnig tasks top then i: process which some memory usage" }, { "title": "PS Command In Linux", "url": "/posts/ps/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-29 00:00:00 +0545", "snippet": "ps stands for process status. This command is used to display all the running processes in the linux system. To display all the running processes in the current shell Syntax: ps [OPTIONS] Information about the information that is displayed upon using ps. PID: Unique Proces ID TTY: Terminal Type of user logged in to TIME: amount of CPU in min and sec that process has been running CMD: name of the command that launched the process To see all the running processes Syntax: ps -e OR Syntax: ps -A To see all the runnnig processes in full format Syntax: ps -ef To see the process by a user Syntax: ps -u &amp;lt;username&amp;gt; To see the process by a group Syntax: ps -G &amp;lt;groupname&amp;gt; To see the process tree Syntax: ps -ejH " }, { "title": "Process Management in Linux", "url": "/posts/manage_processes/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-29 00:00:00 +0545", "snippet": "Some process management commands in Linux jobs: Will show active jobs bg: Resume jobs to the background fg: Resume job to the foreground To resume a speciic jobs Syntax: bg %&amp;lt;job_id&amp;gt; (To resume the job in backround) gh %&amp;lt;job_id&amp;gt; (To resume the job in foreground) Nice valueEvery process has a nice value which range goes from -20 to 10. The lower the value is, the more priority that the process gets.``` # To check the nice value of a process Syntax: ps -l &amp;lt;PID&amp;gt; # To change the priority of a process by chaning its nice value Syntax: ps -n &amp;lt;nice vlaue&amp;gt; &amp;lt;PID&amp;gt;```nohupIf we want our process to keep running even after closing our terminal, we can use nohup.``` Syntax: nohup process &amp;amp;```" }, { "title": "Kill Command In Linux", "url": "/posts/kill/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-29 00:00:00 +0545", "snippet": "The kill command is used to terminate a process manually. Syntax: kill [OPTIONS] [PID] where, OPTION = signal name or no. PID = Process ID To see all the signal names Syntax: kill -l Some of the most widly used kill commands Syntax: kill PID kill -1 PID (to restart the process) kill -2 PID (interrupt from keyboard like `Ctrl+C`) kill -9 PID (forcefully terminate the process) kill -15 PID (kill process gracefully) " }, { "title": "Tar, Gzip and Gunzip", "url": "/posts/zips/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-28 00:00:00 +0545", "snippet": "gzip and gunzip Command to zip a file Syntax: &amp;lt;br/&amp;gt; gzip &amp;lt;file_name&amp;gt; to unzip a file Syntax: &amp;lt;br/&amp;gt; gunzip &amp;lt;file_name&amp;gt; OR gzip -d &amp;lt;file_name&amp;gt; tar Command To convert an entire directory into an archive file. different options available in with tar are:- c: compress f: file x: extract t: list the content of archive Syntax: &amp;lt;br/&amp;gt; tar cvf &amp;lt;filename&amp;gt;.tar file_path/folder_pathOutput: This creates an archive file with extension `.tar` Tar file is then compressed using gzip with extension filename.tar.gz To decompress and unzip a tar file Syntax: &amp;lt;br/&amp;gt; tar xvf &amp;lt;name.tar.gz&amp;gt; tar and zip at the same time Syntax: &amp;lt;br/&amp;gt; tar -zcvf &amp;lt;filename&amp;gt;.tar.gz file_path And tar -zxvf &amp;lt;filename&amp;gt;.tar.gz" }, { "title": "Links in Linux", "url": "/posts/links/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-28 00:00:00 +0545", "snippet": "In linux, a link is a connection between a file name and the actual data on the disk. We can also call it a shortcut. There are two types of links:- Soft Link and Hard LinkSoft LinkLinks will be removed if original file is removed or deleted. Syntax: ln -sHard LinkRenaming, deleting or removing the file will not affect the link Syntx: ln" }, { "title": "Environment Variables in Linux", "url": "/posts/env_var/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-28 00:00:00 +0545", "snippet": "Linux Environment variable is a dynamic-named value, which may be used by one or more programs running. It can also be defined as a set of Rules and Values to build an environment. Syntax: VARIABLE_NAME=VALUEHow to view Environment Variables? To view all the environment variables Syntax: #printenv or env To view only one environment variable Syntax: #echo $variable_name How to set Environment Variables? To set the environment variable temporary Syntax: export TESTVAR=1 To set the environment variable permanently First open up the .bashrc file set the Environment variable run source .bashrc " }, { "title": "Wild Cards Tutorial", "url": "/posts/wildcards/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-12 00:00:00 +0545", "snippet": "Wildcards are characters that can be used as a substitute for any of a class of characters in a search. Suppose there are hundreds and thousands of files and we have to find only xml files then in such cases we can use wild cards. Input: ls -l *.xml Output: Lists all files with &quot;xml&quot; extensionSome common wild cards are:- *: zero or more characters ?: single character []: range of character ^: beginning of the line $: end of the lineSuppose we want to create 20 different files with names file1.txt file2.txt …file20.txt Input: touch file{1..20}.txt Output: Creates 20 different files with names file1.txt, file2.txt upto file20.txtRemoving all files with filenames file1..file20.txt Input: rm file*.txt Output: Removes all files with extension .txt that begins with string `file`.Suppose we have to list every files in which first character can be anything but must be followed by 123 Input: ls -l ?123 Output: Lists every file whose first character is followed by 123Suppose we have to list all files that starts with te and ends with t but has a random character in between Input: ls -l te?t Output: Lists every files that starst with te and ends with t with a random character in betweenSuppose we have to list all files that starts with either a or b or c followed by 123 Input: ls -l [abc]123Suppose we have to list all files that starts with either any alphabet followed by 123 Input: ls -l [a-z]123Suppose we have to list all files that have a number in its filename Input: ls -l *[0-9]*Suppose we have a file names called names that has a bunch of name it it and want to print every line that start with R #names Leonard Raj Sheldon Howard Bernadette Amy Penny Input: cat names | grep ^R Output: RajSuppose we have a file names called names that has a bunch of name it it and want to print every line that ends with y```#namesLeonardRajSheldonHowardBernadetteAmyPenny ``` Input: cat names | grep y$ Output: Raj" }, { "title": "Pipe Tutorial", "url": "/posts/pipe/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-12 00:00:00 +0545", "snippet": "Pipes are used to redirect a stream from one program to another program. The output of one command redirect it to the input of another. It allows us to combine multiple commands.Pipe SyntaxWe use | symbol to separate two commands. By using the | operator, the output of the first command is passed/redirected to the second command.Syntax: command1 | command2 To find the no of files present in a directory. Input: ls -1 | wc -l To combine both file content using cat and sort it Suppose we have two files:- namex.txt and country.txt #namesLeonardSheldonRajAmyHowardBernadettePennyLeonard #countrySpain England Italy France Brazil Argentina Now, to combine both file content using cat and sort it. Input: cat names.txt country.txt | sort &amp;gt; abc.txtOutput: AmyArgentinaBernadetteBrazilEnglandFranceHowardItalyLeonardLeonardPennyRajSheldonSpain``` Find unique records from a file To use the uniq command first the data must be in a sorted order. Input: cat namex.txt | sort | uniq Output: Amy Bernadette Howard Leonard Penny Raj Sheldon How to see only range of lines in a file Input: cat fakenames.txt | head -5 Output: 1,Gregory Mueller 2,Mark Jacobs 3,Amanda Cooper 4,Isaac Conrad 5,Ashley Berg Input: cat fakenames.txt | head -10 | tail -5 Output: 6,Nicholas Clark 7,Jamie Wright 8,Kevin Landry 9,Stefanie Johnson 10,Annette Johnson How to use more and less command We use these command when there is large amount of output/data to see in the terminal. The more command displays the output/data from the start and then shows page by page afterwards. Input: ls -1 | more The less command displays the output/data from the start and then shows in a file type. Input: ls -1 | less we can also search by presing the / and the typing the search keyword. tee command in Linux The tee command reads the standard input and copies it both to stdOutput and to a file. We can see the information going through pipeline. ```Input: ls | tee files.txt``` XARGS command in Linux The xargs command converts the standard input into command line arguement. Input: ls | xargs echo Output: country.txt fakenames.txt files.txt names.txt sorted_names_country.txt Input: ls | xargs echo &quot;Hello&quot; Output: Hello country.txt fakenames.txt files.txt names.txt sorted_names_country.txt Example: Suppose we have a file named “FileNames.txt” #FileNammes file1 file2 file3 file4 file5 Now, suppose we want to take those names from ‘FileNames.txt’ and create a new file for each of those filenames. Input: cat FileNames.txt | xargs touch Here, the output of cat FileNaes.txt is converted into an command line argument using the xargs and passed it to the touch command using the | operator. " }, { "title": "Grep Tutorial", "url": "/posts/grep/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-12 00:00:00 +0545", "snippet": "grep stands for Global Regular Expression Print. This command is used to search for particular string or keyword from a file and print those lines matching a pattern in the shell. It checks line by line and print those lines matching a pattern. It can be use anywhere like with files, searching for file, directories, etc.Grep command syntax: grep [OPTION].. Pattern [File]..Examples Basic Example #csv1 id,firstname,lastname,email,profession 100,Mady,Cloris,Mady.Cloris@gmail.com,police officer 101,Nita,Eiser,Nita.Eiser@gmail.com,worker 102,Ada,Rosette,Ada.Rosette@gmail.com,worker 103,Demetris,Teryn,Demetris.Teryn@gmail.com,worker 104,Mahalia,Sperling,Mahalia.Sperling@gmail.com,firefighter 105,Deirdre,Lemuela,Dierdre.Lemeuela@gmail.com,firefighter 106,Cyndie,Chem,Cyndie.Chem@gmail.com,police officer 107,Modestia,Engdah,Modestia.Engdah@gmail.com,doctor 108,Jaime,Corabella,Jaime.Corabella@gmaio.com,police officer 109,Gusty,Jehu,Gusty.Jehu@gmail.com,doctor Now, to search for Nita in the following file we use:Syntax: grep {keyword/pattern} {filename} grep Nita csv1 Output: 101,Nita,Eiser,Nita.Eiser@gmail.com,worker To ignore the upper and lower case while searching we use: Syntax: grep -i {keyword/pattern} {filename} grep -i nita csv1 Output: 101,Nita,Eiser,Nita.Eiser@gmail.com,worker To search everything except given pattern/keyword Syntax: grep -v {keyword/pattern} {filename} grep -v Nita csv1 Output: id,firstname,lastname,email,profession 100,Mady,Cloris,Mady.Cloris@gmail.com,police officer 102,Ada,Rosette,Ada.Rosette@gmail.com,worker 103,Demetris,Teryn,Demetris.Teryn@gmail.com,worker 104,Mahalia,Sperling,Mahalia.Sperling@gmail.com,firefighter 105,Deirdre,Lemuela,Dierdre.Lemeuela@gmail.com,firefighter 106,Cyndie,Chem,Cyndie.Chem@gmail.com,police officer 107,Modestia,Engdah,Modestia.Engdah@gmail.com,doctor 108,Jaime,Corabella,Jaime.Corabella@gmaio.com,police officer 109,Gusty,Jehu,Gusty.Jehu@gmail.com,doctor To print how many times (count) given keyword is present in the file Syntax: grep -c {keyword/pattern} {filename} grep -c “police officer” csv1 Output: 3 To search for exact match of given keyword in a file Syntax: grep -w {keyword/pattern} {filename} grep -w Nit csv1 Output: {blank} grep -w Nita csv1 Output: 101,Nita,Eiser,Nita.Eiser@gmail.com,worker To print the line no. of matches of given keyword in a file Syntax: grep -n {keyword/patttern} {filename} grep -n Nita csv1 Output: 3:101,Nita,Eiser,Nita.Eiser@gmail.com,worker12:Nitas To search a given keyword in multiple files Syntax: grep {keyword/pattern} {file1} {file2} #file1 Hi My name is Sheldon Welcome to grep tutorial #file2 Hello My name is Leonard Welcome to grep tutorial grep -n name file1 file2 Output: file1:My name is Sheldonfile2:My name is Leonard To supress file names while searching a given keyword in multiple files Syntax: grep -h {keyword/pattern} {file1} {file2} grep -h name file1 file2 Output: My name is SheldonMy name is Leonard To search multiple keyword in a file Syntax: grep -e {keyword 1/ pattern 1} -e {keyword 2 / pattern 2} {filename} grep -w -e Nita -e Jaime csv1 Output: 101,Nita,Eiser,Nita.Eiser@gmail.com,worker108,Jaime,Corabella,Jaime.Corabella@gmaio.com,police officer To search multiple keywords in multiple files Syntax: grep -e {keyword1} -e {keyword2} {file1} {file2} grep -n -e Sheldon -n -e Leoanrd file1 file2 Output: file1:2:My name is Sheldonfile2:2:My name is Leonard To only print filenames which matches given keyword Syntax: grep -l {keyword} {file1} {file2} grep -l -e Sheldon -e Leonard file1 file2 csv1 Output: file1file2 To get the keyword/pattern from a file and match with another file Syntax: grep -f {file with pattern} {file to search} grep -n -f keyword.txt csv1 file1 file2 Output: csv1:2:100,Mady,Cloris,Mady.Cloris@gmail.com,police officer csv1:10:108,Jaime,Corabella,Jaime.Corabella@gmaio.com,police officer file1:2:My name is Sheldon file2:2:My name is Leonard To print the matching line which start with given keyword Syntax: grep ^{keyword} {file} grep -n ^101 csv1 Output: 5:103,Demetris,Teryn,Demetris.Teryn@gmail.com,worker To print the matching line which ends with given keyword Syntax: grep {keyword}$ {file} grep -n doctor$ csv1 Output: 9:107,Modestia,Engdah,Modestia.Engdah@gmail.com,doctor11:109,Gusty,Jehu,Gusty.Jehu@gmail.com,doctor Suppose we have 100 files in a directory (dirA) and we need to search a keyword in all the files Syntax: grep -R {keyword} dirA/ grep -n -R -f keyword.txt . Output: ./csv1:2:100,Mady,Cloris,Mady.Cloris@gmail.com,police officer./csv1:10:108,Jaime,Corabella,Jaime.Corabella@gmaio.com,police officer./file1:2:My name is Sheldon./file2:2:My name is Leonard./keyword.txt:1:Mady./keyword.txt:2:Jaime./keyword.txt:3:Starla./keyword.txt:4:Selia./keyword.txt:5:Sheldon./keyword.txt:6:Leonard We can use egrep command for the multiple keywords search Syntax: egrep “key1|key2|key3” file egrep -n “Leonard Sheldon Jaime” csv1 file1 file2 Output: csv1:10:108,Jaime,Corabella,Jaime.Corabella@gmaio.com,police officerfile1:2:My name is Sheldonfile2:2:My name is Leonard If you just want to search but don’t want to print on the terminal Syntax: grep -q {keyword} {file} grep -q Sheldon file1 Output: None Here, the nothing is returned as output to the terminal. So it may be confusing to the user to know if the command is executed successfully or not. To figure this out, we see the exit status. We can see the exit status using echo $? command. If the echo $? command returns 0 then the last commad was executed successfully and if it returned 1 then it was not executed successfully. For Example:Input: grep -q Sheldon file1Output: NoneInput: echo $?Output:0 Input: grep -q Sheldon file2Output: NoneInput: echo $?Output:1 If you wnat to supress error message Syntax: grep -s {keyword} {file} grep -qs nita csv " }, { "title": "Vi Editor", "url": "/posts/vi/", "categories": "Linux, Editor", "tags": "python", "date": "2022-10-11 00:00:00 +0545", "snippet": "A text editor which allows us to create and manipulate data in linux files.Most Common Kyes in VI Edidor i: Insert O: Insert on next line a: insert after a space Esc: Escape out of any mode :q!: quit without saving :wq!: quit and save r: replace d: delete u: undo x: remove one character backspace: /: forward search ?: backward seach" }, { "title": "Linux File System", "url": "/posts/Linux-File-System/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-10 00:00:00 +0545", "snippet": "Everything in Linux is a FileEven the basic commands that we use are stored as a file in the linux system. We can see where those files are located using the `whereis` command:For eg:- ```Input: whereis lsOutput: ls: /usr/bin/ls /usr/share/man/man1/ls.1.gz```This means that the `ls` command is stored as a file in location `/urs/bin`File Structure Description /boot = Having files used by boot loader (ex:grub) /dev = System devices files (ex: speakers, keyboard etc.) /etc = Has configuration files /usr/bin = Binaries /usr/sbin = System binaries of the root directory /opt = Installation of optional add-on applications (third party applications) /proc = Running process /usr/lib = C Program library files needed by commands and apps /tmp = Having temporary files /home = Directories of users /root = Home directory of root user /var = System logs /run = System daemons that start very early (ex: systemd and udev) to store temporary runtime files like PID /mnt = To mount external filesystem (ex: NFS) /media = For CDROM Mounts" }, { "title": "Basic Linux Commands", "url": "/posts/Linux-Basic-Commands/", "categories": "Linux, Basic Linux Commands", "tags": "python", "date": "2022-10-10 00:00:00 +0545", "snippet": " Inside the linux terminal:- the $ represents the normal user the # represents the root or admin user To check the hostname we can use the hostname command For Eg:- ``` Input: hostname Output: {returns the hostname} ``` To check the current logged in user we can use the whoami command For Eg:- ``` Input: whoami Output: {returns the current logged in user} ``` To check the IP on Linux we can use the ip addr command IP address is an unique address that identifies a device on the internet or a local network For Eg:- ``` Input: ip addr Output: {returns the ip address} ``` To print out the working directory we can use the pwd command For Eg:- ``` Input: pwd Output: {prints the working directory} ``` To make a folder on linux we can use the mkdir command For Eg:- ``` Input: mkdir {directory_name} ``` To change location or to move to another directory we can use the cd command For Eg:- ``` Input: cd {directory_name} ``` To move back to another directory we can use the cd command For Eg:- ``` Input: cd .. ``` Here, the .. means that we should move one step back. To clear the screen we use the clear command For Eg:- ``` Input: clear ``` To search for our folder/file inside a specific location we can use the findcommand To search for folder For Eg:- ``` Input: find path -name {folder/directory name} ``` To search for file For Eg:- ``` Input: find . -type f -name {filename} ``` We can also use the locate command for finding files or folders To create a file we can use the touch command For Eg:- ``` Input: touch {file_name} ``` Removing a directory To remove or delete a directory we can use the rmdir command For Eg:- Input: rmdir {directory_name} To remove the directory and all the other files inside of it. For Eg:- Input: rm -r {directory_name} To view more information about the files we can use the ls -ltr command For Eg:- Input: ls -ltr Here, the -l means we use a long listing format. -t means we sort by time, newest first, and -r means in the reverse order while sorting. To view more information about the command we can use either the man command or the --help arguement For Eg:- Input: ls --help OR Input: man ls To edit or write into a file we can use the vi editor. For Eg:- Input: vi {file_name} To start editing we need to go to insert mode for which we must press i key. Then we can insert the text as we like. To escape from the insert mode we can press the escape key. Now, to save the file, we can press :wq where wq means save and quit. To print the file content into the shell we use cat command. For Eg:- Input: cat {file_name} To count the no.of words and lines we use the wc command. To count only the no of lines we use wc -l For Eg:- Input: wc -l {file_name} To compare two files we use the diff command. For Eg:- ``` Input: diff {file_1} {file_2} ``` To compree and decompress files We use the tar do to the packaging of files. For Eg:- ``` Input: tar {options} {tar_file_name} {file_1} {file_2} Eg: tar cvf files.tar file1 file2 Output: files.tar ``` Now, we need to compress the files.tar using the `gzip` command For Eg:- ``` Input: gzip {tar_file} Eg:- gzip files.tar Output: files.tar.gz ``` Now, to decompress the compresseed file we use the gunzip command For Eg:- ``` Input: gunzip {zipped_file} Eg:- gunzip gzip files.tar.gz Output: files.tar ``` And lastly, we need to untar the tar files For Eg:- ``` Input tar xvf {tar_file} Eg:- tar xvf files.tar Output: file1, file2s ``` To copy file rom one folder to another we use the cp command. For Eg:- Input: cp {source_file} {destination_path} Eg:- cp files.tar.gz folder1/ To rename a file we use the mv command For Eg:- ``` Input: mv {old_file_name} {new_file_name} ``` Here, what we do is move the contents of the old file to the new file with a new name and then delete the old file. In linux, we use such indirect renaming of a file. To split and combine the files To combine a file For Eg:- Let’s suppose we want to create a new file- filec with the contents of two different files: filea and fileb. To do this we use the &amp;gt; operator. For Example:- ``` Input: cat filea fileb &amp;gt; filec ``` To split a file For Eg:- Let’s suppose we want to split the content of a filea into two files fileb and filec then we use the split command. For Example:- ``` Input: split -l 1 filea ``` To search for words in a file and show them in a console we use the grep command For Example:- Input cat {file_name} | grep {word} Here, the | is the pipe operator which is used to chain the operations. Here, the ‘cat {file_name}’ returns some output, and its output is sent as an input to the ‘grep {word}` command. To read the start and end of the files we use the head and tail command respectively For Example:- ``` Input: head -2 {filename} Output: prints the first two line from the file. Input: tail -2 {filename} Output: prints the last two line from the file. ``` To sort the file we use the sort command For Example:- Input: sort {file_name} To prin only the unique value we use the uniq command For Example:- Input: sort {file_name} | uniq " }, { "title": "Python Decorators", "url": "/posts/python9/", "categories": "Python", "tags": "python", "date": "2022-06-23 00:00:00 +0545", "snippet": "In Python, a decorator takes a function and adds some functionality and returns it.Everything in python is an objects. In Python, everything is an object—yes, including classes. Simply said, names defined by us are identifiers attached to these objects. There are no exceptions, functions are also objects (with attributes). The same function object can have multiple names linked to it. def first(msg): print(msg) first(&#39;Hello&#39;) second = first second(&#39;Hello&#39;) Output: &#39;Hello&#39; &#39;Hello&#39;Both the first and second methods produce the same result when the code is executed. Here, the same function object is referred to by both the first and second names.Functions can be passed as arguments to another function. Higher order functions are another name for such functions that accept other functions as arguments. Some of the examples of such functions are:- map, filter, and reduce. def inc(x): return x + 1 def dec(x): return x - 1 def operate(func, x): result = func(x) return result print(operate(inc,3)) Output: 4 print(operate(dec,3)) Output: 2Furthermore, a function can return another function. def is_callled(): def is_returned(): print(&#39;Hello&#39;) return is_returned new = is_called() new() Output: &#39;Hello&#39;Here, is_returned() is a nested function which is defined and returned each time we call is_called().Getting back to DecoratorsFunctions and methods are called callable as they can be called. In actuality, the term “callable” refers to any object that implements the special call() method. So a decorator is a callable that returns another callable in the most basic sense. A decorator basically accepts a function, adds some functionality, and then returns it. def make_pretty(func): def inner(): print(&#39;I got decorated.&#39;) func() return inner def ordinary(): print(&#39;I am ordinary.&#39;) ordinary() Output: &#39;I am ordinary.&#39; #let&#39;s decorate this orindary function pretty = make_pretty(ordinary) pretty() Output: I got decorated. I am ordinary. In the example shown above, make_pretty() is a decorator. pretty = make_pretty(ordinary)The function ordinary() got decorated and the returned function was given the name pretty.We can see that the decorator function expanded the original function’s capabilities. This is comparable to gift-wrapping.As a wrapper, the decorator serves. The actual gift inside the adorned object retains its original essence. But now, it appears lovely (since it got decorated).In most cases, we rename a function and decorate it as, ordinary = make_pretty(ordinary)This is a common construct and for this reason, Python has a syntax to simplify this.We can use the @ symbol along with the name of the decorator function and place it above the definition of the function to be decorated. For example: @make_pretty def ordinary(): print(&#39;I am ordinary.&#39;)is equivalent to def ordinary(): print(&#39;I am ordinary.&#39;) oridnary = make_pretty(ordinary)This is just a syntactic sugar to implement decorators.Decorating Functions With ParametersThe decorator mentioned above was straightforward and merely utilized functions without any parameters.What if we had functions that accepted the following parameters: def divide(a,b): return a/bThis function has two parameters, a and b. We know it will give an error if we pass in b as 0. divide(2,5) Output: 0.4 divide(2,0) Output: Traceback (most recent call last): ... ZeroDivisionError: division by zeroNow writing a decorator to check for this case that causes error like: def smart_divide(func): def inner(a,b): print(f&#39;I am going to divide {a} by {b}.&#39;) if b==0: print(f&#39;Whoopsies! cannot divide by zero.&#39;) return return func(a,b) return inner @smart_divide def divide(a,b): return a/b divide(2,5) Output: &#39;I am going to divide 2 by 5.&#39; 0.4 divide(2,0) Output: &#39;I am going to divide 2 and 5.&#39; &#39;Whoopsies! cannot divide by zero.&#39;This new implementation will return None if the error condition arises.We can decorate functions that take parameters in this way. A careful observer will notice that the parameters of the decorator’s nested inner() function match those of the functions it decorates. With this in mind, we can now create universal decorators that function with any quantity of parameters.In Python, this magic is done as function(*args, **kwargs). In this way, args will be the tuple of positional arguments and kwargs will be the dictionary of keyword arguments. An example of such a decorator will be: def works_for_all(func): def inner(*args, **kwargs): print(&#39;I can decorate any function&#39;) return func(*args, **kwargs) return innerChaining Decorators In PythonPython allows for the chaining of several decorators. This means that a function may be decorated more than once by the same or distinct decorators. The decorators are simply positioned above the desired function. def star(func): def inner(*args, **kwargs): print(&#39;*&#39;*30) func(*args, **kwargs) print(&#39;*&#39;*30) return inner def percent(func): def inner(*args, **kwargs): print(&#39;%&#39;*30) func(*args, **kwargs) print(&#39;%&#39;*30) @start @percent def printer(msg): print(msg) printer(&#39;Hello&#39;) Output: ****************************** %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Hello %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ******************************The above sytanx of: @star @percent def printer(msg): print(msg)is equivalent to def printer(msg): print(msg) printer = star(percent(printer))The order in which we chain decorators matter. If we had reversed the order as, @percent @star def printer(msg): print(msg)The output would be: %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ****************************** Hello ****************************** %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%" }, { "title": "Python Closures", "url": "/posts/python8/", "categories": "Python", "tags": "python", "date": "2022-06-23 00:00:00 +0545", "snippet": "Nested FunctionWhen a function is defined inside another function, then it is called Nested Funciton. The Nested Function can access the variables of the enclosing scope. In Python, these non-ocal variables are read-only be default and we must declare them explicitely as non-local in order to modify them.Example: def print_msg(msg): # This is the outer enclosing function def printer(): # This is the nested function print(msg) printer() print_msg(&#39;Hello&#39;) Output: &#39;Hello&#39;Here, we can see that the printer() function can access the non-local msg variable of the enclosing function.Defining a Closure FunctionIn the example above, what would happen if the last line of the function print_msg() returned the printer() function instead of calling it? This means the function was defined as follows: def print_msg(msg): # This is the outer enclosing function def printer(): # This is the nested function print(msg) printer() another = print_msg(&#39;Hello&#39;) another() Output: &#39;Hello&#39;Here, we can see that the print_msg() function was called with the string ‘Hello’ and the returned function was bounded to the name another. And on calling another(), the message was still remembered although we had already finished executing the print_msg() function.This technique by which some data gets attached to the code is called Closure In Python.This value in the enclosing scope is remembered even when the varibale goes out of scope or the function itself is removed from the current namespace. del print_msg anoter() print_msg(&#39;Hello&#39;) Output: Traceback (most recent call last): ... NameError: name &#39;print_msg&#39; is not definedHere, the returned function still works even when the originalf function was deleted.Criteria For ClosuresFollowing are the criteria that must be met when defininig a Closure in Python:- We must have a nested function The nested function must refer to a value defined in the enclosing function. The enclosing function must return the nested function.When do we use closures?Closures offer some sort of data concealment and can be substitutes for the use of global variables. Additionally, it can offer an object-oriented solution to the issue.Closures can offer an alternative and more elegant way when there are only a few methods (one method in most situations) to be implemented in a class. However, it is preferable to implement a class as the number of characteristics and methods increases.Example: def make_multiplier_of(n): def multiplier(x): return x * n return multiplier # Multiplier of 3 times3 = make_multiplier_of(3) # Multiplier of 5 times5 = make_multiplier_of(5) print(times(9)) Output: 27 print(times5(9)) Output: 45 print(timest(times(2))) Output: 30 Note: Python Decorators make an extensive use of closures as well.Finally, it is important to notice that it is possible to determine the values that are encompassed in the closure function.All function objects have a __closure__ attribute that returns a tuple of cell objects if it is a closure function. We can identify times3 and times5 as closure functions by using the aforementioned example. make_multiplier_of.__closure__ times3.__closure__ Output: (&amp;lt;cell at 0x0000000002D155B8: int object at 0x000000001E39B6E0&amp;gt;,)The cell objects has the attribute cell_contents which stores the closed value. times3.__closure__[0].cell_contents Output: 3 times5.__closure__[0].cell_contents Output: 5" }, { "title": "Python Generators", "url": "/posts/python7/", "categories": "Python", "tags": "python", "date": "2022-06-23 00:00:00 +0545", "snippet": "When builing up an iterator in Python, we have to do a lot of work like implementing a class with __iter__() and __next__() methods in a class, keep track of the internal states, and raise StopIteration error whenever there are no values to be returned. This is both lengthy and counterintuitive. Generators comes to the rescue in such situations. Python generators are a simple way of creating iterators. Generators handles all the work implicitly mentioned above while creating iterators. Simple speaking, a generator is a function that returns an object (iterator) which we can iterate over (one value at a time).Create Generators In PythonMaking a generator in Python is not too difficult. It is just as simple as defining a nomral function, but with a yield statement instead of a return statement as you would in a normal function. A function becomes a generator function if it has atleast one yield statement (it may also include other yield or return statements). A function will both yield and return some value. In contrast to a return statment, which completely terminates a function, a yield statment only stops the function while saving all of its state and continues from there on subsequent calls.Differences between a Generator Function and a Normal FunctionHere, is how a generator functino differs from a normal function. Generator function contains one or more yield statments. When called, it returns an object (iterator) but does not start execution immediately. Methods like __iter__() and __next__() are implemented automaticaly. So we can iterate through the items using next(). Once the function yields, the function is paused and the control is transferred to the caller. Local variables and their states are rememered between successive calls. Finally, when the function terminates, StopIteration is raised automatically on further calls.Example: def my_gen(): n = 1 print(&#39;This is printed first. &#39;) # Generator function contains yield statements yield n n+=1 print(&#39;This is printed second.&#39;) yield n n+=1 print(&#39;This is printed at last&#39;) yield n # It returns on object bud does not start execution immediately. a = my_gen() # We can iterate through the items using next(). print(next(a)) Output: This is printed first 1 print(next(a)) Output: This is printed second 2 print(next(a)) Output: This is printed at last 3 #Finally, when the function terminates, StopIteration is raised automatically print(next(a)) Output: Traceback (most recent call last): ... StopIteration &amp;gt;&amp;gt;&amp;gt; next(a) Traceback (most recent call last): ... StopIterationIn the example above, it’s interesting to note that variable n’s value is retained between calls. In contrast to regular functions, when the function yields, the local variables are preserved. The generator object can also only be iterated once.We can also use generators with for loops directly. This is because a for loop takes an iterator and iterates over it using next() function. It automatically ends when StopIteration is raised. # A simple generator function def my_gen(): n = 1 print(&#39;This is printed first&#39;) # Generator function contains yield statements yield n n += 1 print(&#39;This is printed second&#39;) yield n n += 1 print(&#39;This is printed at last&#39;) yield n # Using for loop for item in my_gen(): print(item) Output: This is printed first 1 This is printed second 2 This is printed at last 3Python Generators with a LoopNormally, generators functions are implemented with a loop having a suitbale termination condition.Example: def rev_string(my_str): length = len(my_str) for i in range(length-1,-1,-1): yield my_str[i] # For loop to reverse the string for char in rev_string(&#39;hello&#39;): print(char) Output: o l l e hPython Generator ExpressionGenerator expressions make it simple to design simple generators instantly.It facilitates the creation of generators. Similar to the lambda functions which create anonymous functions, generator expressions create anonymous generator functions.In Python, a list comprehension has a syntax that is comparable to that of a generator expression. However, round parenthesis are used in place of the square brackets.A list comprehension creates the full list, but a generator expression only produces one item at a time. This is the main distinction between the two types of expressions.They have lazy execution ( producing items only when asked for ). Because of this, a generator expression uses substantially less memory than a comparable list comprehension. #Initialize the list my_list = [1,3,6,10] #square each term using list comprehension list_ = [x**2 for x in my_list] # same thing can be done using a generator expression but generator expressions are surrounded by parenthesis() generator = (x**2 for x in my_list) print(list_) Output: [1,9,36,100] print(generator) Output: &amp;lt;generator object &amp;lt;genexpr&amp;gt; at 0x7f5d4eb4bf50&amp;gt;We can see above that the generator expression did not produce the required result immediately. Instead, it returned a generator object, which produces items only on demand.We can get items from an generator object like: # Initialize the list my_list = [1,3,6,10] a = (x**2 for x in my_list) print(next(a)) print(next(a)) print(next(a)) print(next(a)) next(a) Output: 1 9 36 100 Traceback (most recent call last): File &quot;&amp;lt;string&amp;gt;&quot;, line 15, in &amp;lt;module&amp;gt; StopIterationWhy to use Python Generators?There are several reasons that make generators a powerful implementation. Easy to Implement: Generators can be created easily compared to iterators. Example: def PowTwoGen(max=0): n = 0 while n &amp;lt; max: yield 2**n n+=1 Memory Efficient: In a typical function, the full sequence is created in memory before the result is returned. If the sequence has a lot of items, this would be overkill. Since it only produces one item at a time, generator implementation of such sequences is advantageous and memory friendly. Represent Infinite Stream: Generators are excellent mediums to represent an infinite stream of data. Infinite streams cannot be stored in memory, and since generators produce only one item at a time, they can represent an infinite stream of data. Pipelining Generators Multiple generators can be used to pipeline a series of operations. Example: Let’s say we have a generator that generates the Fibonacci sequence of numbers. For squaring numbers, we also have another generator. By connecting the output of different generator functions, we may determine the sum of squares of all the numbers in the Fibonacci series in the manner shown below. def fibonacci_numbers(nums): x, y = 0, 1 for _ in range(nums): x, y = y, x+y yield x def square(nums): for num in nums: yield num**2 print(sum(square(fibonacci_numbers(10)))) Output: 4895 " }, { "title": "Python Iterators", "url": "/posts/python6/", "categories": "Python", "tags": "python", "date": "2022-06-23 00:00:00 +0545", "snippet": "In Python, iterators are object that can be iterated upon. For an object to be an iterator it must have two special methods: __iter__ and __next__.In Python, iterators are used extensively. While being concealed in plain sight, they are neatly implemented within for loops, comprehensions, generators, etc. A Python iterator object technically needs to implement the iterator protocol, which is made up of the two special methods iter() and next().If we can create an iterator from an object, it is said to be iterable. The majority of Python’s built-in containers, including list, tuple, string, etc., are iterables.The iter() function (which in turn calls the iter() method) returns an iterator from them.We use the next() function to manually iterate through all the items of an iterator. When we read the end and there is no more data to be returned, it will raise the StopIteration Exception. Example: #define a list heroes = [&#39;Batman&#39;, &#39;Superman&#39;, &#39;Spiderman&#39;, &#39;Ironman&#39;] #get an iterator using iter() my_iter = iter(heroes) print(next(my_iter)) Output: &#39;Batman&#39; print(next(my_iter)) Output: &#39;Superman&#39; # next(obj) is the same as obj.__next__() print(my_iter.__next_()) Output: &#39;Spiderman&#39; print(my_iter.__next__()) Output: &#39;Ironman&#39; #Since, iter object has been exhausted, calling next on the iter obj will raise an error - no items left print(next(my_iter)) Output: Traceback (most recent call last): File &quot;&amp;lt;string&amp;gt;&quot;, line 24, in &amp;lt;module&amp;gt; next(my_iter) StopIterationWorking of for loop for IteratorsAs we know, the for loop can iterate over any iterable. Taking a cloaser look at the working of the for loop in Python we get to know that: #create an iterator object from that iterable iter_obj = iter(iterable) #infinite loop while True: try: #get the next item element = next(iter_obj) #do something with the lement except StopIteration: #if StopIteration is raised, break from loop breakInternally, the for loop creates an iterator object, iter_obj in this case by callin the iter() on the iterable. Within the for loop is an infinite while loop that continues until the StopIteration condition is met. Inside the while loop, it calles the next() to get the next element and executes the body of the for loop with this value.Building Custom IteratorsIn Python, creating an iterator from scarch is simple, we just need to implement __iter__() and __next__() functions. The iterator object itself is returned by the __iter__() method. Some initialization can be done if necessary. The next element in the series is returned by the __next__() method. It must raise StopIteration error at the conclusion and on subsequent calls.Example: class PowTwo: &quot;&quot;&quot;Class to implement an iterator of powers of two&quot;&quot;&quot; def __init__(self, max=0): self.max = max def __iter__(self): self.n=0 return self def __next__(self): if self.n &amp;lt;= self.max: result = 2**self.n self.n+=1 return result else: rase StopIteration #create an object numbers = PowTwo(3) #create in iterable from the object i = iter(numbers) #using next to get to the next iterator element print(next(i)) print(next(i)) print(next(i)) print(next(i)) print(next(i)) Output: 1 2 4 8 Traceback (most recent call last): File &quot;/home/bsoyuj/Desktop/Untitled-1.py&quot;, line 32, in &amp;lt;module&amp;gt; print(next(i)) File &quot;&amp;lt;string&amp;gt;&quot;, line 18, in __next__ raise StopIteration StopIterationPython Infinite IteratorsThe item in an iterator object does not necessarily need to be exhausted.Iterators are limitless in number (which never ends). When working with these iterators, we must be cautious. We can build out own infinite iterators.Example: class InfiniteIterator: &quot;&quot;&quot;Infinite Iterator to return all odd numbers&quot;&quot;&quot;&quot; def __iter__(self): self.num=1 return self def __next__(self): num = self.num self.num+=2 return num a = iter(InfiniteIterator()) print(next(a)) Output: 1 print(next(a)) Output: 3 print(next(a)) Output: 5Be careful to include a terminating condition, when iterating over these types of infinite iterators.The advantage of using iterators is that they save resources. Like shown above, we could get all the odd numbers without storing the entire number system in memory. We can have infinite items (theoretically) in finite memory." }, { "title": "Python Operator Overloading", "url": "/posts/python5/", "categories": "Python", "tags": "python", "date": "2022-06-22 00:00:00 +0545", "snippet": "Built-in classes can be used with Python operators.However, the same operator responds differently to several types.For instance, the + operator will combine two lists, concatenate two strings, or perform arithmetic addition on two numbers.Operator overloading is a Python feature that enables the same operator to have several meanings depending on the context.So what happens when we use them with objects of a user-defined class? Let us consider the following class, which tries to simulate a point in 2-D coordinate system. class Point: def __init__(self, x=0, y=0): self.x = x self.y = y p1 = Point(1,2) p2 = Point(2,3) print(p1+p2) Output: Traceback (most recent call last): File &quot;&amp;lt;string&amp;gt;&quot;, line 9, in &amp;lt;module&amp;gt; print(p1+p2) TypeError: unsupported operand type(s) for +: &#39;Point&#39; and &#39;Point&#39;Here, we can see that a TypeError was thrown because Python was unable to combine two Point objects. However, using operator overloading in Python, we can complete this work.Python Special FunctionsClass functions that begin with double underscore __ are called special functions in Python. These functions are not the typical functions that we define for a class. The init() function we defined above is one of them. It gets called every time we create a new object of that class.There are numerous other special functions in Python.Using special functions, we can make our class compatible with built-in functions. p1 = Point(2,3) print(p1) Output: &amp;lt;__main__.Point object at 0x00000000031F8CC0&amp;gt;Let’s say that instead of printing what we got, we want the print() function to print the coordinates of the Point object. In our class, we may define a str() method that regulates how the item is printed.Let’s examine how we can accomplish this: class Point: def __init__(self, x = 0, y = 0): self.x = x self.y = y def __str__(self): return &quot;({0},{1})&quot;.format(self.x,self.y)Trying the print() function again. class Point: def __init__(self, x=0, y=0): self.x = x self.y = y def __str__(self): return &quot;({0}, {1})&quot;.format(self.x, self.y) p1 = Point(2, 3) print(p1) Output: (2,3)Turns out, that this same method is invoked when we use the built-in function str() or format(). str(p1) Output: &#39;(2,3)&#39; format(p1) Output: &#39;(2,3)&#39;So, when we use str(p1) or format(p1), Python internally calls the p1.str() method. Hence the name, special functions.Overloading the + OperatorWe must include the add() function in the class in order to overload the + operator. class Point: def __init__(self, x=0, y=0): self.x = x self.y = y def __str__(self): return &quot;({0},{1})&quot;.format(self.x, self.y) def __add__(self, other): x = self.x + other.x y = self.y + other.y return Point(x, y) p1 = Point(1,2) p2 = Point(2,3) print(p1+p2) Output: (3,5)In the above program, what exactly happens is that when we use p1+p2, Python class p1.__add__(p2) which in turn is Point.__add__(p1,p2). After this, the addition operation is carried out the way we specified.Similary, we can overload other operators as well. The special function that we need to implement is tabulated below. Operator Expression Internally Addition p1 + p2 p1.__add__(p2) Subtraction p1 - p2 p1.__sub__(p2) Multiplication p1 * p2 p1.__mul__(p2) Power p1 ** p2 p1.__pow__(p2) Division p1 / p2 p1.__truediv__(p2) Floor Division p1 // p2 p1.__floordiv__(p2) Remainder(modulo) p1 %p 2 p1.__mod__(p2) Bitwise Left Shift p1 &amp;lt;&amp;lt; p2 p1.__lshift__(p2) Bitwise Right Shift p1 &amp;gt;&amp;gt; p2 p1.__rshift__(p2) Bitwise AND p1 &amp;amp; p2 p1.__and__(p2) Bitwise OR p1 | p2 p1.__or__(p2) Bitwise XOR p1 ^ p2 p1.__xor__(p2) Bitwise NOT ~p1 p1.__invert__(p2) Overloading Comparison OperatorsOperator overloading is not restricted to arithmetic operators in Python. Additionally, we can overload comparison operators.Suppose, we wanted to implement the &amp;lt; symbol to the Point class. To achieve this, let’s compare the distances between these places and the origin and then output the result. # overloading the less than operator class Point: def __init__(self, x=0, y=0): self.x = x self.y = y def __str__(self): return &quot;({0},{1})&quot;.format(self.x, self.y) def __lt__(self, other): self_mag = (self.x ** 2) + (self.y ** 2) other_mag = (other.x ** 2) + (other.y ** 2) return self_mag &amp;lt; other_mag p1 = Point(1,1) p2 = Point(-2,-3) p3 = Point(1,-1) # use less than print(p1&amp;lt;p2) print(p2&amp;lt;p3) print(p1&amp;lt;p3) Output: True False FalseSimilarly, the special functions that we need to implement, to overload other comparison operators are tabulated below. Operator Expression Internally Less than p1 &amp;lt; p2 p1.lt(p2) Less than or equal to p1 &amp;lt;= p2 p1.le(p2) Equal to p1 == p2 p1.eq(p2) Not Equal to p1 != p2 p1.ne(p2) Greater than p1 &amp;gt; p2 p1.gt(p2) Greater than or equal to p1 &amp;gt;= p2 p1.ge(p2) " }, { "title": "Python Inheritance", "url": "/posts/python4/", "categories": "Python", "tags": "python", "date": "2022-06-22 00:00:00 +0545", "snippet": "By using inheritance, we may create classes that inherit all the features of their parent classes and allow us to add new ones.Inheritance in PythonInheritance in python refers to defining a new class with little or no modification to an existing class. The new class is called derived (or child) class and the one from which it inherits is called the base(or parent) class. Syntax:```python class BaseClass: Body of base classclass DerivedClass(BaseClass): Body of derived class ``` The derived class inherits the attributes and methods from the base class where new features can be added to it. This results in re-usability of code. Example of Inheritance in PythonLet us define a polygon, a closed figure with 3 or more sides. class Polygon: def __init__(self, no_of_sides): self.n = no_of_sides self.sides = [0 for i in range(no_of_sides)] def inputSides(self): self.sides = [float(input(f&#39;Enter side {i+1}: &#39;)) for i in range(self.n)] def dispSides(self): for i in range(self.n): print(f&#39;Side {i+1} is {self.sides[i]}&#39;)This class provides data attributes for storing sides, a list of the n sides and the magnitude of each side. The inputSides() method takes in the magnitude of each side and dispSides() displays these side lengths.A triangle is a 3-sided polygon. Consequently, we can develop a class called Triangle that descended from Polygon. The Triangle class now has access to all of the Polygon class’s characteristics.We don’t have to define them once more (code reusability).Following is a definition of a triangle. class Triangle(Polygon): def __init__(self): super().__init__(self,3) def findArea(self): a,b,c = self.sides #calculate the semi-perimiter s = (a+b+c)/2 area = (s*(s-a)*(s-b)*(s-c))**0.5 print(f&#39;Area of the triangle: {area}&#39;)However, class Triangle has a new method findArea() to find and print the area of the triangle. Here is a sample run. t = Triangle() t.inputSides() Enter side 1: 3 Enter side 2: 5 Enter side 3: 4 t.dispSides() Side 1 is 3.0 Side 2 is 5.0 Side 3 is 4.0 t.findArea() The area of the triangle is 6.0As we can see, despite without explicitly defining methods like inputSides() and dispSides() for class Triangle, we were still able to use them.If an attribute is not found in the class itself, the search continues to the base class. This repeats recursively, if the base class is itself derived from other classes.Method Overriding In PythonIn the above example, observe how the __init__() function was defined in the Triangle and Polygon classes.When this occurs, the derived class’s method supersedes the base class’s. This means that the Triangle’s __init__() method is given preference over the Polygon’s init method.Generally when overriding a base method, we tend to extend the definition rather than simply replace it. The same is being done by calling the method in base class from the one in derived class (calling Polygon.init() from init() in Triangle).Utilizing the integrated feature super would be a better choice (). so, super(). Polygon is similar to __init (3). It is advised to use __init__(self,3).Two built-in functinos isinstance() and issubclass() are used to check inheritances. The function isinstance() retursnn True if the object is an instance of the class or other classes derived from it. Each and every class in Python inherits from the base class object. isinstance(t, Triangle) Output: True isinstance(t, Polygon) Output: True isinstance(t, int) Output: False isinstance(t, object) Output: TrueSimilarly, issubclass() is used to check for class inheritance. issubclass(Polygon, Triangle) Output: False issubclass(Triangle, Polygon) Output: True issubclass(bool, int) Output: TruePython Multiple InheritanceIn Python, a class can be derived from more than one base class. This is caleed Multiple Inheritance. In Multiple Inheritance, the features of all the base classes are inherited into the derived class. The syntax for multiple inheritance is similar to single inheritance. class Base1: pass class Base2: pass class MultiDerived(Base1, Base2): passHere, the MultiDerived class is derived from Base1 and Base2 classes which means it inherits attributes and featuers from both of the base classes.Python Multilevel InheritanceIn Python, we can also inherit from a derived class, it is called Multilevel Inheritance. In such type of inheritance, featuers of the base class as well as the dervied classes are inherited into the new derived class.Example: class Base: pass class Derived1(Base): pass class Derived2(Derived1): passHere, the Derived1 class is derived from the Base class, and the Derived2 class is derived from the Derived1 class.Method Resolution Order In PythonEvery class in Python is derived from the object class. It is Python’s most fundamental type. Since all objects are instances of the object class, all other classes built-in or user-defined—are technically derived classes. print(issubclass(list, object)) Output: True print(isinstance(5.5, object)) Output: True print(isinstance(&#39;Hello&#39;, object)) Output: TrueIn the case of multiple inheritance, the current class is first checked for any specified attribute. If not found, the search continues into parent classes in depth-first,left-right fashion without searching the same class twice.Therefore, the search order in the MultiDerived class example above is [MultiDerived, Base1, Base2, object]. The criteria used to determine this order are known as Method Resolution Order, and this order is also referred to as linearization of MultiDerived class (MRO).Both local precedence ordering and monotonicity must be prevented by MRO. It guarantees that a class will always appear before of its parents. The order is the same for tuples of base classes when there are many parents.MRO of a class can be viewed as the __mro__ attribute or the mro() method. The former returns a tuple while the latter returns a list. MultiDerived.__mro__ ( &amp;lt;class &#39;__main__.MultiDerived&#39;&amp;gt;, &amp;lt;class &#39;__main__.Base1&#39;&amp;gt;, &amp;lt;class &#39;__main__.Base2&#39;&amp;gt;, &amp;lt;class &#39;object&#39;&amp;gt; ) MultiDerived.mro() [ &amp;lt;class &#39;__main__.MultiDerived&#39;&amp;gt;, &amp;lt;class &#39;__main__.Base1&#39;&amp;gt;, &amp;lt;class &#39;__main__.Base2&#39;&amp;gt;, &amp;lt;class &#39;object&#39;&amp;gt; ]Here is a little more complex multiple inheritance example and its visualization along with the MRO. # Demonstration of MRO class X: pass class Y: pass class Z: pass class A(X,Y): pass class B(Y,Z): pass class M(B,A,Z): pass # Output: # [ # &amp;lt;class &#39;__main__.M&#39;&amp;gt;, &amp;lt;class &#39;__main__.B&#39;&amp;gt;, # &amp;lt;class &#39;__main__.A&#39;&amp;gt;, &amp;lt;class &#39;__main__.X&#39;&amp;gt;, # &amp;lt;class &#39;__main__.Y&#39;&amp;gt;, &amp;lt;class &#39;__main__.Z&#39;&amp;gt;, # ] print(M.mro()) Output: [ &amp;lt;class &#39;__main__.M&#39;&amp;gt;, &amp;lt;class &#39;__main__.B&#39;&amp;gt;, &amp;lt;class &#39;__main__.A&#39;&amp;gt;, &amp;lt;class &#39;__main__.X&#39;&amp;gt;, &amp;lt;class &#39;__main__.Y&#39;&amp;gt;, &amp;lt;class &#39;__main__.Z&#39;&amp;gt;, &amp;lt;class &#39;object&#39;&amp;gt; ]" }, { "title": "Python Object &amp; Class 2", "url": "/posts/python3/", "categories": "Python", "tags": "python", "date": "2022-06-22 00:00:00 +0545", "snippet": "Constructors In PythonClass functions that begin with double underscore __ are called special functions as they have special meaning.The __init__() method is of great significance.Every time a new object of that class is created, this specific function is called. In object-oriented programming, this kind of function is also known as a constructor (OOP). Typically, we utilize it to initialize each variable.class ComplexNumber: def __init__(self, r=0, i=0): self.real = r self.imag = i def get_data(self): print(f&#39;{self.real}+{self.imag}j&#39;)# Create a new ComplexNumber objectnum1 = ComplexNumber(2, 3)# Call get_data() method# Output: 2+3jnum1.get_data()# Create another ComplexNumber object# and create a new attribute &#39;attr&#39;num2 = ComplexNumber(5)num2.attr = 10# Output: (5, 0, 10)print((num2.real, num2.imag, num2.attr))# but c1 object doesn&#39;t have attribute &#39;attr&#39;# AttributeError: &#39;ComplexNumber&#39; object has no attribute &#39;attr&#39;print(num1.attr)Output:2+3j(5,0,10)Traceback (most recent call last): File &quot;&amp;lt;string&amp;gt;&quot;, line 27, in &amp;lt;module&amp;gt; print(num1.after)AttributeError: &#39;ComplexNumber&#39; object has no attribute &#39;attr&#39;We created a new class to represent complex numbers in the example above.It has two functions: get_data() to appropriately show the number and __init__() to initialize the variables (defaults to zero). In the step above, it’s noteworthy to observe that attributes for an object can be produced instantly. For object num2, we added a new attribute called attr and read it as well. But this does not provide object num1 that attribute.Deleting Attributes and ObjectsUsing the del statement, any attribute of an object may be erased whenever desired.To see the results, run the following command in the Python shell.num1 = ComplexNumber(2,3)del num1.imagnum1.get_data()Output: ... AttributeError: &#39;ComplexNumber&#39; object has no attribute &#39;imag&#39;del ComplexNumber.get_datanum1.get_data()Output: ... Traceback (most recent call last): ... AttributeError: &#39;ComplexNumber&#39; object has no attribute &#39;get_data&#39;We can even delete the object itself, using the del statemtn.c1 = ComplexNumber(1,3)del c1c1Output: Traceback (most recent call last): ... NameError: name `c1` is not definedActually, it is more complicated than that. When we do c1 = ComplexNumber(1,3), a new instance object is created in memory and the name c1 binds with it.On the command del c1, this binding is removed and the name c1 is deleted from the corresponding namespace. The object however continues to exist in memory and if no other name is bound to it, it is later automatically destroyed.This automatic destruction of unreferenced objects in Python is also called garbage collection." }, { "title": "Python Object &amp; Class", "url": "/posts/python2/", "categories": "Python", "tags": "python", "date": "2022-06-22 00:00:00 +0545", "snippet": "Object Oriented ProgrammingPython, a multi-paradigm programming language supports several coding techniques. Making objects is one of the common ways to tackle a programming language. This is known an Objected Oriented approach to programming.In python, any object has two characteristics: attributes behaviorExample:Suppose a car has the following properties: make, model, year, odometer_reading as attributes read_odometer, get_desriptive_name as methodsClassA class can be defined as the blueprint for an obect. We can think of class as a blueprint with labels. It contains all the details about the make, modle, year, odometer_reading, etc. Based on these descriptions, we can study about the car. Here, a car is an object.The example for class of car can be:class Car: passHere, we define the empty class Car` using the class keyword.We create instances from classes.An instance is a particular object that was made from a certain class.ObjectAn instantiation of a class results in an object (instance).Only the object’s description is defined when a class is created.As a result, no storage or RAM is allocated.An example of a car class object is:obj = Car()Here, obj is an object of a class Car.Let’s say we know specifics about cars.We will now demonstrate how to create the Car class and its objects.Example 1: Creating Class and Object in Pythonclass Car; #class attribute type=&#39;electric&#39; #instance attribute def __init__(self, make, model, year): self.make = make self.model = model self.year = year#instantiate the Car classcar1 = Car(&#39;Audi&#39;, &#39;A4&#39;, 2016)car2 = Car(&#39;Toyta&#39;, &#39;T1&#39;, 2017)#access the class attributesprint(f&#39;Car1 is of type: {car1.__class__.type}&#39;)print(f&#39;Car2 is of type: {car2.__class__.type}&#39;)#access the instance attributesprint(f&#39;{car1.make}&#39;s model is {car1.model} and it was made in {car1.year}&#39;)print(f&#39;{car2.make}&#39;s model is {car2.model} and it was made in {car2.year}&#39;)Output:Audi is of type: electricAudi is of type: electricAudi&#39;s model is A4 and it was made in 2016Toyota&#39;s model is T1 and it was made in 2017In the above program, we created a class with the name Car. Then we defined the class attribute type and the instance attributes make, model and year. The class attribute is defined outside the __init__ method and the instance attributes are defined within the __init__ method of the class. The __init__ method is the one that is called first whenever the object is first created.We then create two instances car1 and car2 of the Car class.We can access the class attribute using __class__.species. Class attributes are the same for all instances of a class. Similarly, we use car1.make, car1.model and car1.year to access the instance characteristics. But each instance of a class has a unique set of instance characteristics.MethodsMethods are simple the functions that are defined within the body of a class. They are used to define the behaviors of an object.Example2: Creating Methods in Pythonclass Car(): #instance attributes def __init__(self, make, model, year): self.make = make self.model = model self.year = year self.odometer_reading = 0 #instance methods def get_descriptive_name(self): long_name = f&quot;{self.year} {self.make} {self.model}&quot; return long_name#instantiate the objectmy_new_car = Car(&#39;Audi&#39;, &#39;A4&#39;, 2016)#calling our instance methodsprint(my_new_car.get_descriptive_name())Output: Audi A4 2016InheritanceBy leveraging the details of an existing class without changing it, a new class can be created through inheritance. A derived class has just been created (or child class).The current class is a base class in a similar way (or parent class).Example 3: Use of inheritance in Pythonclass Car(): def __init__(self, make, model,year): self.make = make self.model = model self.year = year self.odometer_reading = 0 def get_descriptive_name(self): long_name = str(self.year) + &quot; &quot; + self.make + &quot; &quot; + self.model return long_name.title() def read_odometer(self): print(&quot;This car has run {} miles on it&quot;.format(str(self.odometer_reading))) def update_odometer(self, mileage): if mileage &amp;lt; self.odometer_reading: print(&quot;You can&#39;t roll back an odometer&quot;) else: self.odometer_reading = mileage def increment_odometer(self, miles): self.odometer_reading += miles def fill_gas_tank(self): &quot;&quot;&quot;Electric cars don&#39;t have gas tanks.&quot;&quot;&quot; print(&quot;This car doesn&#39;t need a gas refill!&quot;)class Battery(): def __init__(self, battery_size = 70): self.battery_size = battery_size def describe_battery(self): print(&quot;This car has a {} -kwh batteyr.&quot;.format(self.battery_size)) def get_range(self): if self.battery_size == 70: range = 240 elif self.battery_size == 85: range = 270 message = &quot;This car can go approximately &quot; + str(range) message += &quot; miles on a full charge.&quot; print(message) def upgrade_battery(self): if self.battery_size != 85: self.battery_size = 85class ElectricCar(Car): def __init__(self,make,model,year): super().__init__(make,model,year) self.battery = Battery() def get_descriptive_name(self): long_name = &quot;This is an electric car &quot; + str(self.year) + &quot; &quot; + self.make + &quot; &quot; + self.model return long_name.title() def fill_gas_tank(self): print(&quot;This car doesn&#39;t need a gas tank!&quot;) def is_this_electric(self): print(&#39;Yes, this is electric&#39;)my_tesla = ElectricCar(&#39;tesla&#39;, &#39;model s&#39;, 2016)my_tesla.read_odometer()print(my_tesla.get_descriptive_name())my_tesla.fill_gas_tank()Output:This car has run 0 miles on it.This Is An Electric Car 2016 Tesla Model SThis car doesn&#39;t need a gas tank!Yes, this is electric.In the above program, we created three classes i.e Car (parent class), Battery, and ElectricCar (child class). The child class inherits the functions of the parent class. We can see this from the read_odometer() method.Again, the child class modifies the behavior of the parent class. We can see this from them get_descriptive_name() method. Furthermore, we extedn the functions of the parent class by creating a new is_this_electric() method.In the __init__() method of the child class, we also employ the super() function. This enables us to call the parent class’s __init__() method form the child class.EncapsulationWe can limit access to methods and variables in Python by using OOP. Encapsulation is the process of preventing direct data change. In Python, we use the underscore prefix to indicateprivate attributes, such as single _ or double __ underscores.Example 4: Data Encapsulation In Pythonclass IceCream: def __init__(self): self.__maxprice = 200 def sell(self): print(f&#39;Selling Price: {self.__maxprice}&#39;) def setmaxPrice(self, price): self.__maxprice = priceicecream = IceCream()icecream.sell()# change the priceicecream.__maxprice = 250icecream.sell()# using setter functionicecream.setmaxPrice(250)icecream.sell()Output:Selling Price: 200Selling Price: 200Selling Price: 250In the above program, we defined an IceCream class. We used __init__() method to store the maximum selling price of IceCream.icecream.__maxprice = 250Here, we have tried to modify the value of __maxprice outside of the class. However, since __maxprice is a private variable, this modification is not seen on output. As practiced aboce, we have to change the value of __maxprice using a setter function like setmaxPrice() which takes price as its parameter.PolymorphismIn Object Oriented Programming, the ability to use a common interface for many forms is known as polymorphism (data types). Consider that there are various shape alternatives when we need to color a form (rectangle, square, circle). However, we could color any form using the same technique. This concept is known as PolymorphismExample 5: Using Polymorphism in Pythonclass Car(): def __init__(self, make, model,year): self.make = make self.model = model self.year = year self.odometer_reading = 0 def get_descriptive_name(self): long_name = str(self.year) + &quot; &quot; + self.make + &quot; &quot; + self.model print(long_name.title())class ElectricCar(Car): def __init__(self,make,model,year): super().__init__(make,model,year) def get_descriptive_name(self): long_name = &quot;This is an electric car &quot; + str(self.year) + &quot; &quot; + self.make + &quot; &quot; + self.model print(long_name.title())#common interfacedef get_description(obj): obj.get_descriptive_name()#instantiate objectscar1 = Car(&#39;Audi&#39;, &#39;A1&#39;, 2016)car2 = ElectricCar(&#39;Tesla&#39;, &#39;T1&#39;, 2020)#passing the objectget_description(car1)get_description(car2)Output:2016 Audi A1This Is An Electric Car 2020 Tesla T1In the above program, we defined two classes:- Car and ElectricCar. The get_descriptive_name() method is share by all of them. However, each Car shares a different version of the function.Now, to use Polymorphism, we develop a common interface called get_description(), which accepts any object and calls the objects get_descriptive_name() method. As a result, the get_description() method successfully executed when supplied it the car1 and car2 objects." }, { "title": "Built-in Data Structures, Functions and Files", "url": "/posts/python1/", "categories": "Python", "tags": "python", "date": "2022-06-22 00:00:00 +0545", "snippet": "Data Structures and SequencesTupleA tuple is fixed-length, immutable sequence of Python objects. The easiest way to create one is with a comma-separated sequence of valuestup = 4,5,6tupOutput: (4, 5, 6)When you’re defining tuples in more complicated expressions, it’s often necessary toenclose the values in parentheses, as in this example of creating a tuple of tuples:nested_tup = (4,5,6), (7,8)nested_tupOutput: ((4, 5, 6), (7, 8))To convert any sequence or iterator to a tuple, we use the tuple functiontuple([4,0,2])Output:(4, 0, 2)Elements in a tuple can be accessed with square brackets [] as with most other sequence types.tupOutput: (4, 5, 6)tup[0]Output: 4tup[2]Output: 6While the objects stores in a tuple may be mutable themselves, once the tuple is created it’s not possible to modify which object is stored in each slottup = tuple([&#39;foo&#39;, [1,2], True])tup[2] = FalseOutput: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &amp;lt;ipython-input-13-b89d0c4ae599&amp;gt; in &amp;lt;module&amp;gt;() ----&amp;gt; 1 tup[2] = False TypeError: &#39;tuple&#39; object does not support item assignmentIf an object inside a tuple is mutable, such as lists, you can modify it in-placetup[1].append(3)tupOutput: (&#39;foo&#39;, [1, 2, 3], True)We can concatenate tuples using the + operator to produce longer tuples(4, None, &#39;foo&#39;) + (6,0) + (&#39;bar&#39;,)Output: (4, None, &#39;foo&#39;, 6, 0, &#39;bar&#39;)Multiplying a tuple by an integer, as with lists, has the effect of concatenating together that many copies of the tuple(&#39;foo&#39;, &#39;bar&#39;) * 4Output: (&#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;foo&#39;, &#39;bar&#39;)Unpacking TuplesIf we try to assign a tuple-like expressino of variables, Python will attempt to unpack the value on the righthand side of the equals sing:tup = (4,5,6)a, b, c = tupaOutput: 4bOutput: 5cOutput: 6Even sequences with nested tuples can be unpackedtup = 4,5,(6,7)a,b,(c,d) = tupaOutput: 4bOutput: 5cOutput: 6dOutput: 7In python swap can be done like thisa, b = 1,2aOutput: 1bOutput: 2a, b = b, aaOutput: 2bOutput: 1A common use of variable unpacking is iterating over sequences of tuples or listsseq = [(1,2,3), (4,5,6), (7,8,9)]for a,b,c, in seq: print(f&quot;a: {a}, b: {b} &amp;amp; c: {c}&quot;)Output: a: 1, b: 2 &amp;amp; c: 3a: 4, b: 5 &amp;amp; c: 6a: 7, b: 8 &amp;amp; c: 9Another common use is returning multiple values form a function.The python language recently acquired some more advanced tuple unpacking to help with situations where you may want to pluck a few elements from the beginning of a tuple. This uses the special syntax *rest, which is also used in function signatures to capture an arbitrarily long list of positional arguementsvalues = 1,2,3,4,5a, b, *rest = valuesaOutput: 1bOutput: 2restOutput: [3, 4, 5]It is conventional among python programmers to use underscore (_) for unwanted variables instead of resta, b, *_ = values_Output: [3, 4, 5]Tuple MethodsSince the size and contents of a tuple cannot be modified, it is very light on instance methods. A particularly useful one is count which counts the number of occurences of a value.a = (1,2,2,2,3,4,2)a.count(1)Output: 1a.count(2)Output: 4a.index(4)Output: 5a.index(2)Output: 1Here, the index method in a tuple only returns the index of the first match object if the values are repeated.ListIn contrast to tuples, lists are variable length and their contents can be modified in-place. We can define them using square brackets [] or using the list type functiona_list = [2,3,7, None]tup = (&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;)b_list = list(tup)b_listOutput: [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;]b_list[1] = &#39;peekaboo&#39;b_listOutput: [&#39;foo&#39;, &#39;peekaboo&#39;, &#39;baz&#39;]The list function is frequently used in data processing as a way to materialize an interator or generator expression.gen = range(10)genOutput: range(0, 10)list(gen)Ouptut: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]Adding and Removing ElementsElements can be appended to the end of the list with the append methodb_listOutput: [&#39;foo&#39;, &#39;peekaboo&#39;, &#39;baz&#39;]b_list.append(&#39;dwarf&#39;)b_listOutput: [&#39;foo&#39;, &#39;peekaboo&#39;, &#39;baz&#39;, &#39;dwarf&#39;]Using insert an element can be inserted at a specific location in the listb_list.insert(1, &#39;red&#39;)b_listOutput: [&#39;foo&#39;, &#39;red&#39;, &#39;peekaboo&#39;, &#39;baz&#39;, &#39;dwarf&#39;] Note: The insertion index must be between 0 and the length of the list, inclusiveThe inverse operation to insert is pop, which removes and returns an element at a particular indexb_list.pop(2)Output: &#39;peekaboo&#39;b_listOutput: [&#39;foo&#39;, &#39;red&#39;, &#39;baz&#39;, &#39;dwarf&#39;]If the index is non provided to the pop function then the last element in the list will be removedb_list.pop()Output: &#39;dwarf&#39;b_listOutput: [&#39;foo&#39;, &#39;red&#39;, &#39;baz&#39;]Elements can also be removed by value with remove, which locates the first such value and removes it from the listb_list.append(&#39;foo&#39;)b_listOutput: [&#39;foo&#39;, &#39;red&#39;, &#39;baz&#39;, &#39;foo&#39;]b_list.remove(&#39;foo&#39;)b_listOutput: [&#39;red&#39;, &#39;baz&#39;, &#39;foo&#39;]To check if a list contains a value, we use the in keyword.&#39;dwarf&#39; in b_listOutput: False&#39;red&#39; in b_listOutput: True&#39;dwarf&#39; not in b_listOutput: TrueChecking whether a list contains a value is a lot slower than doing so with dicts and sets, as Python makes a linear scan across the values of the list, whereas it can check the others in constant time.Concatenating and combining listsSimilar to tuples, adding two lists together with + concatenates them[4, None, &#39;foo&#39;] + [7,8,(2,3)]Output: [4, None, &#39;foo&#39;, 7, 8, (2, 3)]If we have a list already defined, we can append multiple elements or another lists ot it using the extend methodx = [4, None, &#39;foo&#39;]x.extend([7,8,(2,3)])xOutput: [4, None, &#39;foo&#39;, 7, 8, (2, 3)] Note: List concatenation by addition is comparatively expensive operation since a new list must be created and the objects coped over. Using extend to append elements to an existing list, especially if we are building up a large list, is usually preferableSortingWe can sort a list in-place by calling its sort functiona = [7,2,5,1,3]a.sort()aOutput: [1, 2, 3, 5, 7]sort has a few options that will occasionally come in handy. One is the ability to pass a secondary sort key-that is, a function that produces a value to use to sort the objectsb = [&#39;saw&#39;, &#39;small&#39;, &#39;he&#39;, &#39;foxed&#39;, &#39;six&#39;]b.sort(key=len)bOutput: [&#39;he&#39;, &#39;saw&#39;, &#39;six&#39;, &#39;small&#39;, &#39;foxed&#39;]Binary Search and maintaining a sorted listThe built-in bisect module implements binary search and insertion into a sorted list. bisect.bisect finds the location where an element should be inserted to keep it sorted, while bisect.insort actually inserts the element into that locationimport bisectc = [1,2,2,2,3,4,7]bisect.bisect(c,2)Output: 4bisect.bisect(c,5)Output: 6bisect.insort(c,6)cOutput: [1, 2, 2, 2, 3, 4, 6, 7] Note: The bisect module functions do not check whether the list is sorted, as doing so would be computataionally expensive. Thus, using them with an unsorted list will succeed without erorr but may lead to incorrect results.SlicingWe can select sections of most sequence types by using slice notatoin, which in its basic form consits of start:stop(exclusive) passed to thte indexing operator[]:seq = [7,2,3,7,5,6,0,1]len(seq)Output:8seq[1:5]Output:[2, 3, 7, 5]Slices can also be assigned to with a sequenceseq[3:4]= [6,3]seqOutput:[7, 2, 3, 6, 3, 5, 6, 0, 1]len(seq)Output:9While the element at the start index is included, the stop index is not included, so that the number of elements in the result is stop - startEither the start or stop can be omitted, in which case they default to the start of the sequence and the end of the sequence, respectivelyseq[:5]Output: [7, 2, 3, 6, 3]Negative indices slice the sequence relative to the end:seq[-4:]Output:[5, 6, 0, 1]seq[-6:-2]Output:[6, 3, 5, 6]A step can be used after a second colon to say, take every other elementseqOutput:[7, 2, 3, 6, 3, 5, 6, 0, 1]seq[::2]Output: [7, 3, 3, 6, 1]A clever use of this is to pass -1, which has the useful effect of reversing a list or tupleseq[::-1]Output:[1, 0, 6, 5, 3, 6, 3, 2, 7]Built-in Sequence Functionsenumerate - It’s common when interating over a sequence to want to keep track of the index of the current item. The enumerate is a built in python function, which returns a sequnece of (index, value) tuplesSyntax: i = 0 for i, value in enumerate(collection): # do something with value i+=1When we are indexing data, a helpful pattern that uses enumerate is computing a dict mapping the values of a sequence(which are assumed to be unique) to their locations in the sequencesome_list = [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;]mapping = {}for index, value in enumerate(some_list): mapping[index] = valuemappingOutput: {0: &#39;foo&#39;, 1: &#39;bar&#39;, 2: &#39;baz&#39;}sorted - The sorted function returns a new sorted list from the elements of any sequencesorted([7,1,2,6,0,3,2])Output: [0, 1, 2, 2, 3, 6, 7]The sorted function acccepts the same arguements as the sort method on listswords = [&#39;the&#39;, &#39;sorted&#39;, &#39;function&#39;, &#39;accepts&#39;, &#39;the&#39;, &#39;same&#39;]sorted(words,key = len)Ouput:[&#39;the&#39;, &#39;the&#39;, &#39;same&#39;, &#39;sorted&#39;, &#39;accepts&#39;, &#39;function&#39;]zip - It pairs up the elements of a number of lists, tuples or other sequences to create a list of tuplesseq1 = [&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;]seq2 = [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]zipped = zip(seq1, seq2)zippedOutput: &amp;lt;zip at 0x1cba23b5f88&amp;gt;list(zipped)Output:[(&#39;foo&#39;, &#39;one&#39;), (&#39;bar&#39;, &#39;two&#39;), (&#39;baz&#39;, &#39;three&#39;)] zip can take an arbitrary number of sequences, and the number of elements it produces is determined by the shortest sequence:seq3 = [&#39;False&#39;, &#39;True&#39;]list(zip(seq1, seq2, seq3))Output: [(&#39;foo&#39;, &#39;one&#39;, &#39;False&#39;), (&#39;bar&#39;, &#39;two&#39;, &#39;True&#39;)]A very common use of zip is simultaneously iterating over multiple sequences, possibly also combined with enumerateseq1Output:[&#39;foo&#39;, &#39;bar&#39;, &#39;baz&#39;]seq2Output:[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]for i, (a,b) in enumerate(zip(seq1, seq2)): print(f&quot;{i}: {a}, {b}&quot;)Output:0: foo, one1: bar, two2: baz, threeGiven a zipped sequence, zip can be applied in a clever way to unzip the sequence. Another way to think about this converting a list of rows into a list of columns.pitchers = [(&#39;Nolan&#39;, &#39;Ryan&#39;), (&#39;Roger&#39;, &#39;Clemens&#39;), (&#39;Schilling&#39;, &#39;Curt&#39;)]first_name, last_name = zip(*pitchers)first_nameOutput: (&#39;Nolan&#39;, &#39;Roger&#39;, &#39;Schilling&#39;)last_nameOutput: (&#39;Ryan&#39;, &#39;Clemens&#39;, &#39;Curt&#39;)reversed - reversed iterates over the elements of a sequence in reverse orderlist(reversed(range(10)))Output:[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]dictdict is likely the most important built-in python data structure. A more common name for it is hash map or associative array. It is a flexibly sized collection of key-value pairs, where key and value are Python objects. One approach to creating one is to use curly braces {} and colons to seperate keys and values.empty_dict = {}d1 = {&#39;a&#39;: &#39;some value&#39;, &#39;b&#39;:[1,2,3,4]}d1Output: { &#39;a&#39;: &#39;some value&#39;, &#39;b&#39;: [1, 2, 3, 4] }We can access, insert or set elements using the same sytanx as for accesing elements of a list or tupled1[7] = &#39;an integer&#39;d1Output: { &#39;a&#39;: &#39;some value&#39;, &#39;b&#39;: [1, 2, 3, 4], 7: &#39;an integer&#39; }d1[&#39;b&#39;]Output:[1, 2, 3, 4]We can check if a dict contains a key using the same syntax used for checking whether a list or tuple contains a value&#39;b&#39; in d1Output: True&#39;b&#39; in d1.keys()Output:TrueWe can delete the values either using the del keyword or the pop method(which simultaneously returns the value and deletes the key)d1[5] = &#39;some value&#39;d1Output: { &#39;a&#39;: &#39;some value&#39;, &#39;b&#39;: [1, 2, 3, 4], 7: &#39;an integer&#39;, 5: &#39;some value&#39; }d1[&#39;dummy&#39;] = &#39;another value&#39;d1Output: { &#39;a&#39;: &#39;some value&#39;, &#39;b&#39;: [1, 2, 3, 4], 7: &#39;an integer&#39;, 5: &#39;some value&#39;, &#39;dummy&#39;: &#39;another value&#39; }del d1[5]d1Output: { &#39;a&#39;: &#39;some value&#39;, &#39;b&#39;: [1, 2, 3, 4], 7: &#39;an integer&#39;, &#39;dummy&#39;: &#39;another value&#39; }ret = d1.pop(&#39;dummy&#39;)retOutput:&#39;another value&#39;d1Output: { &#39;a&#39;: &#39;some value&#39;, &#39;b&#39;: [1, 2, 3, 4], 7: &#39;an integer&#39; }The keys and values method give you iterators of the dict’s keys and values, respectively. While the key-value pairs are not in any particular order, these functions output the keys and values in the same orderlist(d1.keys())Output: [&#39;a&#39;, &#39;b&#39;, 7]list(d1.values())Output:[&#39;some value&#39;, [1, 2, 3, 4], &#39;an integer&#39;]We can merge one dict into another using the update methodd1.update({&#39;b&#39; : &#39;foo&#39;, &#39;c&#39;: 12})d1Output: { &#39;a&#39;: &#39;some value&#39;, &#39;b&#39;: &#39;foo&#39;, 7: &#39;an integer&#39;, &#39;c&#39;: 12 }The update methods changes dicst in-place, so any existing keys in the data passed to the update will have their old values discardedCreating dicts from sequencesIt is common to occasionally end up with two sequences that you want to pair up element-wise in a dict. mapping = {} for key, value in zip(key_list, value_list): mapping[key]= valueSince a dict is essentially a collection of 2 tuples, the dict function accepts a list of 2-tuplesmapping = dict(zip(range(5), reversed(range(5))))mappingOutput:{0: 4, 1: 3, 2: 2, 3: 1, 4: 0}Default ValuesIt is very common to have logic like: if key in some_dct: value = some_dict[key] else: value = default_value Thus, the dict methods get and pop can take default value to be returned, so that the above if-else block can be written as simply as: value = some_dict.get(key,default_value)get by default will return None if they key is not present, while pop will raise an exceptionwords = [&#39;apple&#39;, &#39;bat&#39;, &#39;bat&#39;, &#39;atom&#39;, &#39;book&#39;]by_letter= {}for word in words: letter = word[0] if letter not in by_letter: by_letter[letter] = [word] else: by_letter[letter].append(word)by_letterOutput:{&#39;a&#39;: [&#39;apple&#39;, &#39;atom&#39;], &#39;b&#39;: [&#39;bat&#39;, &#39;bar&#39;, &#39;book&#39;]}The setdefault dict method is for precisely this purpose. The preceding for loop can be rewritten as:for word in words: letter = word[0] by_letter.setdefault(letter,[]).append(word)by_letterOutput: { &#39;a&#39;: [&#39;apple&#39;, &#39;atom&#39;], &#39;b&#39;: [&#39;bat&#39;, &#39;bar&#39;, &#39;book&#39;] }The built-in collections module has a useful class, defaultdict, which makes this even easier. TO create one, you pass a type or function for generating the default valu for each slot in the dict:from collections import defaultdictby_letter = defaultdict(list)for word in words: by_letter[word[0]].append(word)by_letterOutput: defaultdict(list, {&#39;a&#39;: [&#39;apple&#39;, &#39;atom&#39;], &#39;b&#39;: [&#39;bat&#39;, &#39;bar&#39;, &#39;book&#39;]})Valid dict key typesWhile the values of a dict can be any Python object, the keys generally have to be immutable objcets like scalar types(int, float, string) or tuples(all the objects in the tuple need to be immutable, too). The technical term here is hashabiility. We can check whether an object is hashable(cann be used as a key in dict) with the hash functionhash(&#39;string&#39;)Output: -8095158123584499513hash((1,2,(2,3)))Output: 1097636502276347782hash((1,2,[2,3]))Output: --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &amp;lt;ipython-input-190-576218ff90d3&amp;gt; in &amp;lt;module&amp;gt;() ----&amp;gt; 1 hash((1,2,[2,3])) TypeError: unhashable type: &#39;list&#39;To use list as a key, one option is to convert it to a tuple, which can be hashes as long as its elements also can:d = {}d[tuple([1,2,3])] = 5dOutput: {(1, 2, 3): 5}SetA set is an unordered collection of unique elements. We can think of them like dicts, buy keys only, no values. A set can be created in two ways: via the set function or via a set literal with curly bracesset([2,2,2,1,3,3])Output: {1, 2, 3}{2,2,2,1,3,3,}Output:{1, 2, 3}Sets support mathematical set operations like union, intersection, difference, and symmetric difference.a = {1,2,3,4,5}b = {3,4,5,6,7,8}The union of these two sets is the set of distinct elements occuring in etiher set. This can be computed with either the unioni method or the | binary operator.a.union(b)Output {1, 2, 3, 4, 5, 6, 7, 8}a | bOutput: {1, 2, 3, 4, 5, 6, 7, 8}The intersection contains the elements occuring in both sets. The &amp;amp; operator or the intersection method can be used.a.intersection(b)Output: {3, 4, 5}a &amp;amp; bOutput: {3, 4, 5}aOutput: {1, 2, 3, 4, 5}c = a.copy()cOutput: {1, 2, 3, 4, 5}c|= bcOutput: {1, 2, 3, 4, 5, 6, 7, 8}d = a.copy()dOutput: {1, 2, 3, 4, 5}d&amp;amp;=bdOutput: {3, 4, 5}Like dicts, set elements generally must be immutable. To have list-like elemetns you must convert it to a tuple:my_data = [1,2,3,4]my_set= {tuple(my_data)}my_setOutput: {(1, 2, 3, 4)}We can also check if a set is a subset of (is contained in) or a superset of (contains all elements of) another seta_set = {1,2,3,4,5}{1,2,3}.issubset(a_set)Output:Truea_set.issuperset({1,2,3})Output:TrueSets are equal if and only if their contents are equal:{1,2,3} == {3,2,1}Output: TrueList, Set and Dict ComprehensionsList comprehension allows us to concisely from a new list by filtering the elements of a collection, transforming the elements passing the filter in one concide expression of the form: Syntax: [expr for val in collection if condition] This is equivalent to the following for loop: result = [] for val in collection: if condition: result.append(expr)strings = [&#39;a&#39;, &#39;as&#39;, &#39;bat&#39;, &#39;car&#39;, &#39;dove&#39;, &#39;python&#39;][x.upper() for x in strings if len(x) &amp;gt; 2]Output: [&#39;BAT&#39;, &#39;CAR&#39;, &#39;DOVE&#39;, &#39;PYTHON&#39;]Set and dict comprehensions are a natural extension, producing sets and dicts in an idiomatically similar way instead of lists. A dict comprehension looks like this: dict_comp = {key-expr: value-expr for value in collection if conditionA set comprehension looks like the equivalent list comprehension except with curly braces instead of square brackets: set_comp = {expr for value in collection if condition}stringsOutput: [&#39;a&#39;, &#39;as&#39;, &#39;bat&#39;, &#39;car&#39;, &#39;dove&#39;, &#39;python&#39;]unique_lengths = {len(x) for x in strings}unique_lengthsOutput: {1, 2, 3, 4, 6}We could also express this more functionally using the map functionset(map(len, strings))Output: {1, 2, 3, 4, 6}As a simple dict comprehesion example, we could create a lookup map of these strings to their locations in the list:loc_mapping = {value: index for index, value in enumerate(strings)}loc_mappingOutput: {&#39;a&#39;: 0, &#39;as&#39;: 1, &#39;bat&#39;: 2, &#39;car&#39;: 3, &#39;dove&#39;: 4, &#39;python&#39;: 5}Nested List Comprehensionsall_data = [[&#39;John&#39;, &#39;Emily&#39;, &#39;Micheal&#39;, &#39;Mary&#39;, &#39;Steven&#39;], [&#39;Maria&#39;, &#39;Juan&#39;, &#39;Javier&#39;, &#39;Natalia&#39;, &#39;Pillar&#39;]]To get a single list containing all names with two or more e’s in them we could do this with a simple for loop:names_of_interest = []for names in all_data: enough_es = [name for name in names if name.count(&#39;e&#39;)&amp;gt;=2] names_of_interest.extend(enough_es)names_of_interestOutput: [&#39;Steven&#39;]But we can actually wrap this whole operation up in a single nested list comprehension like:result = [name for names in all_data for name in names if name.count(&#39;e&#39;)&amp;gt;=2]resultOutput: [&#39;Steven&#39;]At first, nested list comprehensions are a bit hard to wrap your head around. The for parts of the list comprehension are arranged according to the order of nesting, and any filter condition is pull at the end as before. Here is another example where we ‘flatten’ a list of tuple of integers into a simple list of integerssome_tuples = [(1,2,3),(4,5,6),(7,8,9)]flattened = [x for tup in some_tuples for x in tup]flattenedOutput: [1, 2, 3, 4, 5, 6, 7, 8, 9]Kepp in mind that the order of the for expression would be the same if we wrote a nested for loop instead of a list comprehension:flattened = []for tup in some_tuples: for x in tup: flattened.append(x)Building a list of lists using list comprehension[[x for x in tup]for tup in some_tuples]Output: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]Functionsdef my_function(x,y, z= 1.5): if z &amp;gt; 1: return z * (x+y) else: return z/ (x + y)There is no issue with multiple return statements. If Python reaches the end of a function without encountering a return statemetn, None is returned automatically.Each function can have positional arugments and keyword arguements. Keyword arguements are most commonly used to speccify default values or optional arguements. In the preceding fucntion, x and y are positional arguements while z is a keyword arguments. This means that the function can be called in any of these ways: my_function(5,6,z=0.7) my_function(3.14,7,3.5) my_function(10,20my_function(5,6,z=0.7)Output: 0.06363636363636363my_function(3.14,7,3.5)Output: 35.49my_function(10,20)Output: 45.0The main restriction on function arguments is that the keyword arguments must always follows the positional arguements (if any).Namespaces, Scope and Local FunctionsFunctions can access variables in two different scopes: global and local. An alternativeand more descriptive name describing a variable scope in Python is a namespace. Anyvariables that are assigned within a function by default are assigned to the localnamespace. The local namespace is created when the function is called and immedi‐ately populated by the function’s arguments. After the function is finished, the localnamespace is destroyed (with some exceptions that are outside the purview of thischapter).def func(): z = [] for i in range(5): z.append(i)When func() is called, the empty list z is created, five elements are appended, and then z is destroyed when the function exits.Suppose instead we had declared z as follows:z = []def func(): for i in range(5): z.append(i)Assigning variables outside of the function’s scope is possible, but those variables must be declared as global via the global keyworda = Nonedef bind_a_variable(): global a a = []bind_a_variableOutput: &amp;lt;function __main__.bind_a_variable&amp;gt;print(a)Output: NoneReturning Multiple Valuesdef f(): a = 5 b = 6 c = 7 return a,b,ca,b,c, = f()aOutput: 5bOutput: 6cOutput: 7A potentially attractive alternative to returning multiple values like before might be to return a dict insteaddef f(): a = 5 b = 6 c = 7 return {&#39;a&#39;: a, &#39;b&#39;: b, &#39;c&#39;: c}f()Output: {&#39;a&#39;: 5, &#39;b&#39;: 6, &#39;c&#39;: 7}Functions Are ObjectsSince Python functions are objects, many constructs can be easily expressed that are difficult to do in other languages.states = [&#39; Alababa&#39;, &#39;Georgiga!&#39;, &#39;Georgia&#39;, &#39;georgia&#39;, &#39;FlOrIda&#39;, &#39;south carolina##&#39;, &#39;West virginia?&#39;]To convert it into standard data we have to strip whitespace, remove puncutation symbols and standardize on propercapitalizaiton. One way to do this is to use built-in string methods along with the ‘re’ standarad library module for regular expressionsimport redef clean_strings(strings): result = [] for value in strings: value = value.strip() value = re.sub(&#39;[!#?]&#39;, &#39;&#39;, value) value = value.title() result.append(value) return resultclean_strings(states)Output: [&#39;Alababa&#39;, &#39;Georgiga&#39;, &#39;Georgia&#39;, &#39;Georgia&#39;, &#39;Florida&#39;, &#39;South Carolina&#39;, &#39;West Virginia&#39;]An alternative approach that we may find useful is to make a list of the operations we want to apply to a particular set of stringsdef remove_punctuation(value): return re.sub(&#39;[!#?]&#39;, &#39;&#39;, value)clean_ops = [str.strip, remove_punctuation, str.title]def clean_strings(strings, ops): result = [] for value in strings: for function in ops: value = function(value) result.append(value) return resultclean_strings(states, clean_ops)Output: [&#39;Alababa&#39;, &#39;Georgiga&#39;, &#39;Georgia&#39;, &#39;Georgia&#39;, &#39;Florida&#39;, &#39;South Carolina&#39;, &#39;West Virginia&#39;]A more functional pattern like this enables you to easily modify how the strings aretransformed at a very high level. The clean_strings function is also now more reus‐able and generic|Anonymous or Lambda FunctionsPython has a support for so-called anonymous or lambda functions, which are way of writing functions consisting of a single statment, the result of which is the return value. They are defined with the lambda keyword, which has no meaning other than “we are declarign an anonymous function”def short_function(x): return x * 2equivalent_anonymous = lambda x : x * 2Lambda functions are especially convenient in data analysis because, there are many cases where data transformation functions will take functions as arguments. It is often less typing (and clearer) to pass a lambda function as opposed to writing a full-out function declaration or even assigning the lambda function to a local variable.def apply_to_list(some_list, f): return [f(x) for x in some_list]ints = [4,0,1,5,6]apply_to_list(ints, lambda x : x * 2)Output: [8, 0, 2, 10, 12]strings = [&#39;foo&#39;, &#39;card&#39;, &#39;bar&#39;, &#39;aaaa&#39;, &#39;abab&#39;]strings.sort(key = lambda x: len(set(list(x))))stringsOutput:[&#39;aaaa&#39;, &#39;foo&#39;, &#39;abab&#39;, &#39;bar&#39;, &#39;card&#39;] Note: One reason lambda functions are called anonymous functions is that, unlike functions declared with the def keyword, the function object itself is never given an explicit name attributeCurrying: Partial Argument ApplicatoinCurrying is computer science jargon that means deriving new functions from existing ones by partial argument application.def add_numbers(x,y): return x + yadd_numbers(3,4)Output:7Using this function, we could derive a new function of one varibale, add_five, that adds 5 to its arguementsadd_five = lambda y: add_numbers(5,y)add_five(6)Output: 11Here, the second argument to add_numbers is said to be curried.The built-in functools module can simplify this process using the partial functionfrom functools import partialadd_five = partial(add_numbers,5)add_five(4)Output: 9Generatorssome_dict = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}for key in some_dict: print(key)Output: a b cdict_iterator = iter(some_dict)dict_iteratorOutput: &amp;lt;dict_keyiterator at 0x1fa490413b8&amp;gt;An iterator is any object that will yeild objects to the Python interpreter when used in a context like a for looplist(dict_iterator)Output: [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]itertools moduleThe standard library itertools module has a collection of generators for many common data algorithms. For example, groupby takes any sequence and a function, grouping consecutive elements in the sequence by return value of the function. Here’s an example:import itertoolsfirst_letter = lambda x: x[0]names = [&#39;Alan&#39;, &#39;Adam&#39;, &#39;Wes&#39;, &#39;Will&#39;, &#39;Albert&#39;, &#39;Steven&#39;]for letter, names in itertools.groupby(names, first_letter): print(f&quot;Letter : {letter} - {list(names)}&quot;)Output: Letter : A - [&#39;Alan&#39;, &#39;Adam&#39;] Letter : W - [&#39;Wes&#39;, &#39;Will&#39;] Letter : A - [&#39;Albert&#39;] Letter : S - [&#39;Steven&#39;]Errors and Exception Handlingfloat(&#39;1.2345&#39;)Output: 1.2345float(&#39;something&#39;) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &amp;lt;ipython-input-55-2649e4ade0e6&amp;gt; in &amp;lt;module&amp;gt;() ----&amp;gt; 1 float(&#39;something&#39;) ValueError: could not convert string to float: &#39;something&#39;def attempt_float(x): try: return float(x) except Exception as exc: return f&quot;Excpetions occured as: {exc}&quot;attempt_float(&#39;1.2345&#39;)Output:1.2345attempt_float(&#39;something&#39;)Output: &quot;Excpetions occured as: could not convert string to float: &#39;something&#39;&quot;We might want to suppress ValueError, since a TypeError might indicate a legitimate bug in your program. To do that, write the exception type after except:def attempt_float(x): try: return float(x) except ValueError: return xattempt_float((1,2)) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &amp;lt;ipython-input-60-102527222085&amp;gt; in &amp;lt;module&amp;gt;() ----&amp;gt; 1 attempt_float((1,2)) &amp;lt;ipython-input-59-6209ddecd2b5&amp;gt; in attempt_float(x) 1 def attempt_float(x): 2 try: ----&amp;gt; 3 return float(x) 4 except ValueError: 5 return x TypeError: float() argument must be a string or a number, not &#39;tuple&#39;We can catch multiple exception types by writing a tuple of exception types inteaddef attempt_float(x): try: return float(x) except (TypeError, ValueError): return xIn some cases, we may not want to suppress an exception, but you want some code to be executed regardless of whether the code in the try block succees or not, To do this, we use finally: f = open(path, &#39;w&#39;) try: write_to_file(f) finally: f.close()Here, the file handle f will always get closed. Similarly, you can have code that executes only if the try: block succeeds using else: f = open(path, &#39;w&#39;) try: write_to_file(f) except: print(&#39;Failed&#39;) else:: print(&#39;Succeeded&#39;) finally: f.close()" }, { "title": "Introduction to Pandas", "url": "/posts/pandas1/", "categories": "pandas", "tags": "pandas", "date": "2022-06-22 00:00:00 +0545", "snippet": "Getting Started with PandasPandas contains data structures and data manipulation tools designed to make data cleaning and analysis fast and easy in Python.While pandas adopts many coding idioms from NumPy, the biggest difference is that pandas is designed for working with tabular or heterogeneous data. NumPy, by contract, is best suited for working with homogeneous numerical array data.import pandas as pdfrom pandas import Series, DataFrameimport numpy as npIntroduction to Pandas Data StructuresSeries - is a one dimensional array-like object containing a sequence of values and an associated array of data labels, called its index.obj = pd.Series([4,7,-5,3])objOutput: 0 4 1 7 2 -5 3 3 dtype: int64The string representation of a Series displayed interactively shows the index on the left and the values on the right. Since, we did not specify an index for the data, a default one consisting of the integers 0 through N-1 (where N is the length of the data) is created.We can get the array representation and index objects of the Sereis via its ‘values’ and ‘index’ attributes respectively.obj.valuesOutput: array([ 4, 7, -5, 3], dtype=int64)obj.indexOutput: RangeIndex(start=0, stop=4, step=1)Often it will be desirable to create a Series with an index identifying each data pointobj2 = pd.Series([4,7,-5,3], index=[&#39;d&#39;,&#39;b&#39;,&#39;a&#39;,&#39;c&#39;])obj2Output: d 4 b 7 a -5 c 3 dtype: int64obj2.indexOutput:Index([&#39;d&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;], dtype=&#39;object&#39;)We can use labels in the index when selecting single values or set of values.obj2[&#39;a&#39;]Output: -5obj2[&#39;d&#39;]Output: 4obj2[[&#39;c&#39;, &#39;a&#39;, &#39;d&#39;]]Output: c 3 a -5 d 4 dtype: int64obj2[obj2 &amp;gt; 0]Output: d 4 b 7 c 3 dtype: int64obj2 * 2Output: d 8 b 14 a -10 c 6 dtype: int64np.exp(obj2)Output: d 54.598150 b 1096.633158 a 0.006738 c 20.085537 dtype: float64Note: Series can also be thought of as a fixed-length, ordered dict, as it is a mapping of index values to data values&amp;lt;/p&amp;gt;&#39;b&#39; in obj2Output: True&#39;e&#39; in obj2Output: FalseConverting a Python dict obj to a Series obj in pandas.sdata = {&#39;Ohio&#39;: 35000, &#39;Texas&#39;: 71000, &#39;Oregon&#39;: 16000, &#39;Utah&#39; : 5000}obj3 = pd.Series(sdata)obj3Output: Ohio 35000 Texas 71000 Oregon 16000 Utah 5000 dtype: int64When we pass only a dict, the index in the resulting Series will have the dict’s keys in sorted order. We can override this by passing the dict keys in the order we want them to appear in the resulting Series.type(sdata)Output: dicttype(obj3)Output: pandas.core.series.Seriesstates = [&#39;California&#39;, &#39;Ohio&#39;, &#39;Oregon&#39;, &#39;Texas&#39;]obj4 = pd.Series(sdata, index = states)obj4Output: California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 dtype: float64Here, 3 values found in sdata were placed in the appropriate locations, but since no value for ‘California’ was found, it appears as NaN(not a number), which is considered in pandas to mark missing or NA values.pd.isnull(obj4)Output: California True Ohio False Oregon False Texas False dtype: boolpd.notnull(obj4) California False Ohio True Oregon True Texas True dtype: boolThe ‘isnull’ and ‘notnull’ functions in pandas should be used to detect missing data.obj4.isnull()Output: California True Ohio False Oregon False Texas False dtype: boolobj3Output: Ohio 35000 Texas 71000 Oregon 16000 Utah 5000 dtype: int64obj4Output: California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 dtype: float64obj3 + obj4 California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64Both the Series object itself and its index have a name attribute, which integrates with other key areas of pandas funcitonality:obj4.name = &#39;population&#39;obj4.index.name = &#39;state&#39;obj4Output: state California NaN Ohio 35000.0 Oregon 16000.0 Texas 71000.0 Name: population, dtype: float64A series index can be altered in-place by assignment:objOutput: 0 4 1 7 2 -5 3 3 dtype: int64obj.index = [&#39;Bob&#39;, &#39;Seteve&#39; ,&#39;Jeff&#39;, &#39;Ryan&#39;]objOutput: Bob 4 Seteve 7 Jeff -5 Ryan 3 dtype: int64DataFrame: A DataFrame represents a rectangular table of data and condains an ordered collection of columns, each of which can be a different value type (numeric, string, boolean, etc.). The DataFrame has both a row and column index; it can be thought of as a dict of Series all sharing the same index. Note: While a DataFrame is physically two-dimensional, we can use it to represent higher dimensional data in a tabular formati using heirarchial indexing.There are many ways to create a DataFrame, though one of the most common is from a dict of equal-length lists or NumPy array:data = {&#39;state&#39;: [&#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Ohio&#39;, &#39;Nevada&#39;, &#39;Nevada&#39;, &#39;Nevada&#39;], &#39;year&#39;: [2000,2001,2002,2001,2002,2003], &#39;pop&#39;: [1.5,1.7,3.6,2.4,2.9,3.2]}frame = pd.DataFrame(data)frameOutput:   pop state year 0 1.5 Ohio 2000 1 1.7 Ohio 2001 2 3.6 Ohio 2002 3 2.4 Nevada 2001 4 2.9 Nevada 2002 5 3.2 Nevada 2003 For Large DataFrames, the head method selects only the first five rows&amp;lt;frame.head()Output:   pop state year 0 1.5 Ohio 2000 1 1.7 Ohio 2001 2 3.6 Ohio 2002 3 2.4 Nevada 2001 4 2.9 Nevada 2002 If we specify a sequence of columns, the DataFrame’s columns will be arranged in that order:pd.DataFrame(data,columns=[&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;])Output:   year state pop 0 2000 Ohio 1.5 1 2001 Ohio 1.7 2 2002 Ohio 3.6 3 2001 Nevada 2.4 4 2002 Nevada 2.9 5 2003 Nevada 3.2 If passed a column that isn’t contained in the dict, it will appear with the missing values in the result:frame2 = pd.DataFrame(data, columns = [&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;, &#39;debt&#39;], index=[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;six&#39;])frame2Output:   year state pop debt one 2000 Ohio 1.5 NaN two 2001 Ohio 1.7 NaN three 2002 Ohio 3.6 NaN four 2001 Nevada 2.4 NaN five 2002 Nevada 2.9 NaN six 2003 Nevada 3.2 NaN frame2.columnsOutput: Index([&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;, &#39;debt&#39;], dtype=&#39;object&#39;)We can retrive a column in a DataFrame as a Series either by dict-like notation or by attribute:frame2[&#39;state&#39;]Output: one Ohio two Ohio three Ohio four Nevada five Nevada six Nevada Name: state, dtype: objectframe2.yearOutput: one 2000 two 2001 three 2002 four 2001 five 2002 six 2003 Name: year, dtype: int64 Rows can also be retrieved by position or name with the special loc attribute .frame2Output:   year state pop debt one 2000 Ohio 1.5 NaN two 2001 Ohio 1.7 NaN three 2002 Ohio 3.6 NaN four 2001 Nevada 2.4 NaN five 2002 Nevada 2.9 NaN six 2003 Nevada 3.2 NaN frame2.loc[&#39;three&#39;]Output: year 2002 state Ohio pop 3.6 debt NaN Name: three, dtype: objectColumns can be modified by assignment. For example, the empty debt column could be assigned a scalar value or an array of values:frame2[&#39;debt&#39;] = 16.5frame2Output:   year state pop debt one 2000 Ohio 1.5 16.5 two 2001 Ohio 1.7 16.5 three 2002 Ohio 3.6 16.5 four 2001 Nevada 2.4 16.5 five 2002 Nevada 2.9 16.5 six 2003 Nevada 3.2 16.5 frame2[&#39;debt&#39;] = np.arange(6.)frame2Output:   year state pop debt one 2000 Ohio 1.5 0.0 two 2001 Ohio 1.7 1.0 three 2002 Ohio 3.6 2.0 four 2001 Nevada 2.4 3.0 five 2002 Nevada 2.9 4.0 six 2003 Nevada 3.2 5.0 Note: When assigning lists or arrays to a column, we must make sure that the value’s length must match the length of the DataFrame. If we assign a Series, its labels will be realigned exactly to the DataFrame’s index, inserting missing values in any holesval = pd.Series([-1.2,-1.5,-1.7], index = [&#39;two&#39;, &#39;four&#39;, &#39;five&#39;])frame2[&#39;debt&#39;] = valframe2Output:   year state pop debt one 2000 Ohio 1.5 NaN two 2001 Ohio 1.7 -1.2 three 2002 Ohio 3.6 NaN four 2001 Nevada 2.4 -1.5 five 2002 Nevada 2.9 -1.7 six 2003 Nevada 3.2 NaN Assinging a column that doesn’t exist will create a new column. And the ‘del’ keyword wil delete columns as with a dict.frame2[&#39;eastern&#39;] = (frame2.state == &#39;Ohio&#39;)frame2Output:   year state pop debt eastern one 2000 Ohio 1.5 NaN True two 2001 Ohio 1.7 -1.2 True three 2002 Ohio 3.6 NaN True four 2001 Nevada 2.4 -1.5 False five 2002 Nevada 2.9 -1.7 False six 2003 Nevada 3.2 NaN False del frame2[&#39;eastern&#39;]frame2Output:   year state pop debt one 2000 Ohio 1.5 NaN two 2001 Ohio 1.7 -1.2 three 2002 Ohio 3.6 NaN four 2001 Nevada 2.4 -1.5 five 2002 Nevada 2.9 -1.7 six 2003 Nevada 3.2 NaN frame2.columnsOutput: Index([&#39;year&#39;, &#39;state&#39;, &#39;pop&#39;, &#39;debt&#39;], dtype=&#39;object&#39;) Note: The column returned from indexing a DataFrame is a view on the underlying data, not a copy. Thus, any in-place modifications to the Series will be reflected in the DataFrame. The column can be explicitely copied with the Series’s copy method.pop = {&#39;Nevada&#39;: {2001: 2.4, 2002: 2.9}, &#39;Ohio&#39;:{2000:1.5, 2001: 1.7, 2002: 3.6}}If the nested dict is passed to the DataFrame, pandas will intrepret the outer dict keys as the columns and the inner dict keys as the rows indicesframe3 = pd.DataFrame(pop)frame3Output:   Nevada Ohio 2001 2.4 1.7 2002 2.9 3.6 2000 NaN 1.5 We can transpose the DataFrame with similar syntax as in Numpy array. Here, the tranposed dataframe will just be a copy and not a view. frame3.TOutput:   2000 2001 2002 Nevada NaN 2.4 2.9 Ohio 1.5 1.7 3.6 frame3Output:   Nevada Ohio 2001 2.4 1.7 2002 2.9 3.6 2000 NaN 1.5 The keys in the inner dicts are combined and sorted to form the index in the result. This isn’t true if an explicit index is specified:pd.DataFrame(pop, index = [2001,2002,2003])Output:   Nevada Ohio 2001 2.4 1.7 2002 2.9 3.6 2003 NaN NaN The Dict of Sereis are treated in much the same waypdata = {&#39;Ohio&#39;: frame3[&#39;Ohio&#39;][:-1], &#39;Nevada&#39;:frame3[&#39;Nevada&#39;][:2]}pd.DataFrame(pdata)Output:   Nevada Ohio 2000 NaN 1.5 2001 2.4 1.7 frame3Output:   Nevada Ohio 2001 2.4 1.7 2002 2.9 3.6 2000 NaN 1.5 If a DataFrame’s index and columns have their name attribtues set, these will also be displayed along with the frameframe3.index.name = &#39;year&#39;; frame3.columns.name = &#39;state&#39;frame3Output: state Nevada Ohio year     2000 NaN 1.5 2001 2.4 1.7 2002 2.9 3.6 As with Series, the values attribute returns the data contained in the DataFrame as a two-dimensional ndarrayframe3.valuesOutput: array([[2.4, 1.7], [2.9, 3.6], [nan, 1.5]])frame2.valuesOutput: array([[2000, &#39;Ohio&#39;, 1.5, nan], [2001, &#39;Ohio&#39;, 1.7, -1.2], [2002, &#39;Ohio&#39;, 3.6, nan], [2001, &#39;Nevada&#39;, 2.4, -1.5], [2002, &#39;Nevada&#39;, 2.9, -1.7], [2003, &#39;Nevada&#39;, 3.2, nan]], dtype=object)Index ObjectsPandas’s Index objects are responsible for holding the axis lables and other metadata. Any array or other sequence of labels we use when constructing a Series or DataFrame is internally convertedto an Index:obj = pd.Series(range(3), index = [&#39;a&#39;,&#39;b&#39;, &#39;c&#39;])index = obj.indexindexOutput: Index([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], dtype=&#39;object&#39;)index[1:]Output: Index([&#39;b&#39;, &#39;c&#39;], dtype=&#39;object&#39;)Index objects are IMMUTABLE and thus can’t be modified by the user. Thus makes it safer to share Index objects among data structures.labels = pd.Index(np.arange(3))labelsOutput: Int64Index([0, 1, 2], dtype=&#39;int64&#39;)obj2 = pd.Series([1.5, 2.5, 0],index = labels)obj2Output: 0 1.5 1 2.5 2 0.0 dtype: float64obj2.index is labelsOutput: True Some users will not often take advantage of the capabilities pro‐vided by indexes, but because some operations will yield resultscontaining indexed data, it’s important to understand how theywork.frame3Output: state Nevada Ohio year     2001 2.4 1.7 2002 2.9 3.6 2000 NaN 1.5 Note: In addition to being array-like, an Index also behaves like a fixed size setframe3.columnsOutput: Index([&#39;Nevada&#39;, &#39;Ohio&#39;], dtype=&#39;object&#39;, name=&#39;state&#39;)&#39;Ohio&#39; in frame3.columnsOutput: True2003 in frame3.indexOutput: False Note: Unlike python sets, a pandas Index can contain duplicate labelsdup_labels = pd.Index([&#39;foo&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;bar&#39;])dup_labelsOutput: Index([&#39;foo&#39;, &#39;foo&#39;, &#39;bar&#39;, &#39;bar&#39;], dtype=&#39;object&#39;) But selections with duplicate labels will select all occurences of that label.Each Index has a numer of methods and properteis for set logic, which answer other common questions about the data it contains.Essential FunctionalityReindexing - An important method on pandas objects is reindex, which means to create a new object with the data conformed to a new index.obj = pd.Series([4.5, 7.2, -5.3, 3.6], index = [&#39;d&#39;, &#39;b&#39;, &#39;a&#39;, &#39;c&#39;])objOutput: d 4.5 b 7.2 a -5.3 c 3.6 dtype: float64Calling reindex on the above Series rearranges the data according to the new index, introducing missing values if any index values were not already present.obj2 = obj.reindex([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;])obj2Output: a -5.3 b 7.2 c 3.6 d 4.5 e NaN dtype: float64 Note: For ordered data like time series, it may be desirable to do some interpolation or filling of values when reindexing. The method option allows us to do this, using a method such as ffill, which forward-fills the values:obj3 = pd.Series([&#39;blue&#39;, &#39;purple&#39;, &#39;yellow&#39;], index = [0,2,4])Output: 0 blue 2 purple 4 yellow dtype: objectobj3.reindex(range(6), method = &#39;ffill&#39;)Output: 0 blue 1 blue 2 purple 3 purple 4 yellow 5 yellow dtype: object Note: With DataFrame, reindex can alter either the (row) index, columns or both. When passed only a sequence, it reindexed the rows in the result:frame = pd.DataFrame(np.arange(9).reshape((3,3)), index = [&#39;a&#39;, &#39;c&#39;, &#39;d&#39;], columns = [&#39;Ohio&#39;, &#39;Texas&#39;, &#39;California&#39;])frameOutput:   Ohio Texas California a 0 1 2 c 3 4 5 d 6 7 8 frame2 = frame.reindex([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])frame2Output:   Ohio Texas California a 0 1 2 b NaN NaN NaN c 3 4 5 d 6 7 8 The columns can be reindexed with the columns keywordstates = [&#39;Texas&#39;, &#39;Utah&#39;, &#39;California&#39;]frame.reindex(columns=states)Output:   Texas Utah California a 1 NaN 2 c 4 NaN 5 d 7 NaN 8 Note: We can reindex more succinctly by lable-indexing withloc, and this way is more preferable by many users.Dropping Entries from an AxisDropping one or more entries from an axis is easy if you already have an index array or list without those entries. As that can require a bit of munging and set logic, the drop method will return a new object with the indicated value or values deleted from an axis:obj = pd.Series(np.arange(5.), index = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;,&#39;d&#39;, &#39;e&#39;])objOutput: a 0.0 b 1.0 c 2.0 d 3.0 e 4.0 dtype: float64new_obj = obj.drop(&#39;c&#39;)new_objOutput: a 0.0 b 1.0 d 3.0 e 4.0 dtype: float64obj.drop([&#39;d&#39;, &#39;c&#39;])Output: a 0.0 b 1.0 e 4.0 dtype: float64With DataFrame, index values can be deleted from either axis.data = pd.DataFrame(np.arange(16).reshape((4,4)), index = [&#39;Ohio&#39;, &#39;Colorado&#39;, &#39;Utah&#39;, &#39;New York&#39;], columns = [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;])dataOutput:   one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 Calling drop with a sequence of lables will drop values from the row labels(axis = 0)data.drop([&#39;Colorado&#39;, &#39;Ohio&#39;])Output:   one two three four Utah 8 9 10 11 New York 12 13 14 15 We can drop values from the columns by passing axis = 1 or axis = &#39;columns&#39;data.drop(&#39;two&#39;, axis = 1)Output:   one three four Ohio 0 2 3 Colorado 4 6 7 Utah 8 10 11 New York 12 14 15 data.drop(&#39;three&#39;, axis = &#39;columns&#39;)Output:   one two four Ohio 0 1 3 Colorado 4 5 7 Utah 8 9 11 New York 12 13 15 Many functions like drop, which modify the size or shape of a Series or DataFrame, can manipulate an object in-place without returning a new object.obj.drop(&#39;c&#39;, inplace=True)objOutput: a 0.0 b 1.0 d 3.0 e 4.0 dtype: float64 Note: Be careful with the inplace, as it destroys any data that is dropped.Indexing, Selection, and FilteringSeries indexing (obj[…]) works analogously to NumPy array indexing, except you can use the Series’s index values instead of only integers.obj = pd.Series(np.arange(4.), index=[&#39;a&#39;,&#39;b&#39;, &#39;c&#39;,&#39;d&#39;])objOutput: a 0.0 b 1.0 c 2.0 d 3.0 dtype: float64obj[&#39;b&#39;]Output: 1.0obj[2]Output: 2.0obj[2:4]Output: c 2.0 d 3.0 dtype: float64obj[[&#39;b&#39;, &#39;a&#39;, &#39;d&#39;]]Output: b 1.0 a 0.0 d 3.0 dtype: float64obj[[1,3]]Output: b 1.0 d 3.0 dtype: float64obj[obj&amp;lt;2]Output: a 0.0 b 1.0 dtype: float64 Slicing with lables behaves differently than normal Python slicing in that the end-points are inclusive.obj[&#39;b&#39; : &#39;c&#39;]Output: b 1.0 c 2.0 dtype: float64Setting using these methods modifies the corresponding section of the Seriesobj[&#39;b&#39;:&#39;c&#39;] = 5objOutput: a 0.0 b 5.0 c 5.0 d 3.0 dtype: float64Indexing into a DataFrame is for retrieving one or more columns either with a single vlaue or sequencedata = pd.DataFrame(np.arange(16).reshape((4,4)), index = [&#39;Ohio&#39;, &#39;Colorado&#39;, &#39;Utah&#39;, &#39;New York&#39;], columns = [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;, &#39;four&#39;])dataOutput:   one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 data[&#39;two&#39;]Output: Ohio 1 Colorado 5 Utah 9 New York 13 Name: two, dtype: int32data [[&#39;three&#39;, &#39;one&#39;]]Output: three one Ohio 1 0 Colorado 5 4 Utah 9 8 New York 13 12 Name: two, dtype: int32Indexing like this has a few special cases. First, slicing or selecting data with a boolean array:data[:2]Output:   one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 data[data[&#39;three&#39;]&amp;gt;5]Output:   one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 New York 12 13 14 15 The row selection syntax data[:2] is provided as a convenience. Passing a single element or a list to the [] operator selects columns.Another use case is in indexing with a boolean DataFrame, such as one produced by a scalar comparisiondata Output:   one two three four Ohio 0 1 2 3 Colorado 4 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 data &amp;lt; 5Output:   one two three four Ohio True True True True Colorado True False False False Utah False False False False New York False False False False data[data&amp;lt;5] = 0dataOutput:   one two three four Ohio 0 0 0 0 Colorado 0 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 This makes DataFrame syntactically more like a two-dimensional NumPy array in this particular case.Selection with loc and ilocdataOutput:   one two three four Ohio 0 0 0 0 Colorado 0 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 The special indexing operators loc and iloc enables us to select a subset of rows and columns from a DataFrame with NumPy-like notation using either axis labels(loc) or integers (iloc)data.loc[&#39;Colorado&#39;, [&#39;two&#39;, &#39;three&#39;]]Output: two 5 three 6 Name: Colorado, dtype: int32data.iloc[2,[3,0,1]]Output: four 11 one 8 two 9 Name: Utah, dtype: int32data.iloc[2]Output: one 8 two 9 three 10 four 11 Name: Utah, dtype: int32data.iloc[[1,2],[3,0,1]]Output:   four one tow Colorado 7 0 5 Utah 11 8 9 Both indexing functions work with slices in addition to single labels or list of labelsdataOutput:   one two three four Ohio 0 0 0 0 Colorado 0 5 6 7 Utah 8 9 10 11 New York 12 13 14 15 data.loc[:&#39;Utah&#39;, &#39;two&#39;]Output: Ohio 0 Colorado 5 Utah 9 Name: two, dtype: int32data.iloc[:, :3][data[&#39;three&#39;] &amp;gt; 5]Output:   one two three Colorado 0 5 6 Utah 8 9 10 New York 12 13 14 Integer Indexesser = pd.Series(np.arange(3.))serOutput: 0 0.0 1 1.0 2 2.0 dtype: float64ser2 = pd.Series(np.arange(3.), index = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])ser2Output: a 0.0 b 1.0 c 2.0 dtype: float64ser2[-1]Output: 2.0ser[:1]Output: 0 0.0 dtype: float64ser.loc[:1]Output: 0 0.0 1 1.0 dtype: float64ser.iloc[:1]Output: 0 0.0 dtype: float64Arithmetic and Data AlignmentAn important pandas feature for some applications is the behavior of arithmetic between objects with different indexes. When you are adding together objects, if any index pairs are not the same, the respective index in the result will be the union of the index paris. For users with database experience, this is similar to an automatic Outer Join on the index labels.s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index = [&#39;a&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;])s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index = [&#39;a&#39;, &#39;c&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;])s1Output: a 7.3 c -2.5 d 3.4 e 1.5 dtype: float64s2Output: a -2.1 c 3.6 e -1.5 f 4.0 g 3.1 dtype: float64s1 + s2Output: a 5.2 c 1.1 d NaN e 0.0 f NaN g NaN dtype: float64Here, the internal data alignment introduces missing values in the label locations that don’t overlap. Missing values will then propagate in further arithmetic computations.In case of DataFrame, alignment is performed on both the rows and the columnsdf1 = pd.DataFrame(np.arange(9.).reshape((3,3)), columns = list(&#39;bcd&#39;), index = [&#39;Ohio&#39;, &#39;Texas&#39;, &#39;Colorado&#39;])df2 = pd.DataFrame(np.arange(12.).reshape((4,3)), columns = list(&#39;bde&#39;), index = [&#39;Utah&#39;, &#39;Ohio&#39;, &#39;Texas&#39;, &#39;Oregon&#39;])df1Output:   b c d Ohio 0.0 1.0 2.0 Texas 3.0 4.0 5.0 Colorado 6.0 7.0 8.0 df2Output:   b d e Ohio 0.0 1.0 2.0 Texas 3.0 4.0 5.0 Colorado 6.0 7.0 8.0 Oregon 9.0 10.0 11.0 Adding these together returns a DataFrame whose index and columns are the unionis of the ones in each DataFramedf1 + df2Output:   b c d e Colorado NaN NaN NaN NaN Ohio 3.0 NaN 6.0 NaN Oregon NaN NaN NaN NaN Texas 9.0 NaN 12.0 NaN Utah NaN NaN NaN NaN Since, the ‘c’ and ‘e’ columns are note present in bothe the DataFrame objects, they appears as all missing in the result. The same holds for the ‘Utah’, ‘Colorado’, and ‘Oregon’ whose labels are not common to both objects. Note: If you add DataFrame objects with no column or row labels in common, then the result will contain all nulls:df1 = pd.DataFrame({&#39;A&#39;: [1,2]})df2 = pd.DataFrame({&#39;B&#39; : [3,4]})df1Output: A 0 1 1 2df2Output: B 0 3 1 4df1 - df2 A B 0 NaN NaN 1 NaN NaNArithmetic methods with fill valuesIn arithmetic operations between differently indexed objects, you might want to fill with a special value, like 0, when an axis lable is found in one object but not the otherdf1 = pd.DataFrame(np.arange(12.).reshape((3,4)), columns = list(&#39;abcd&#39;))df2 = pd.DataFrame(np.arange(20.).reshape((4,5)), columns=list(&#39;abcde&#39;))df2.loc[1, &#39;b&#39;] = np.nandf1Output:   a b c d 0 0.0 1.0 2.0 3.0 1 4.0 5.0 6.0 7.0 2 8.0 9.0 10.0 11.0 df2Output:   a b c d e 0 0.0 1.0 2.0 3.0 4.0 1 5.0 NaN 7.0 8.0 9.0 2 10.0 11.0 12.0 13.0 14.0 3 15.0 16.0 17.0 18.0 19.0 Adding these together results in NA values in the locations that don’t overlap:df1 + df2Output:   a b c d e 0 0.0 2.0 4.0 6.0 NaN 1 9.0 NaN 13.0 15.0 NaN 2 18.0 20.0 22.0 24.0 NaN 3 NaN NaN NaN NaN NaN Using the add method on df1,we can pass df2 and an arguemnt to fill_valuedf1.add(df2, fill_value = 0)Output:   a b c d e 0 0.0 2.0 4.0 6.0 4.0 1 9.0 5..0 13.0 15.0 9.0 2 18.0 20.0 22.0 24.0 14.0 3 15.0 16.0 17.0 18.0 19.0 df1Output:   a b c d 0 0.0 1.0 2.0 3.0 1 4.0 5.0 6.0 7.0 2 8.0 9.0 10.0 11.0 1/df1Output:   a b c d 0 inf 1.000000 0.500000 0.333333 1 0.250000 0.200000 0.166667 0.142857 2 0.125000 0.111111 0.100000 0.090909 df1.rdiv(1)Output:   a b c d 0 inf 1.000000 0.500000 0.333333 1 0.250000 0.200000 0.166667 0.142857 2 0.125000 0.111111 0.100000 0.090909 When reindxing a Series or DataFrame, we can speciy a different fill valuedf1.reindex(columns = df2.columns, fill_value=0)Output:   a b c d e 0 0.0 1.0 2.0 3.0 0 1 4.0 5.0 6.0 7.0 0 2 8.0 9.0 10.0 11.0 0 Operations between DataFrame and Seriesarr = np.arange(12.).reshape((3,4))arrOutput: array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]])arr[0]Output: array([0., 1., 2., 3.])arr - arr[0]Output: array([[0., 0., 0., 0.], [4., 4., 4., 4.], [8., 8., 8., 8.]])Here, when we subtract arr[0] from arr, the subtraction is performed once for each row. This is referred to as broadcasting.frame = pd.DataFrame(np.arange(12.).reshape((4,3)), columns = list(&#39;bde&#39;), index = [&#39;Utah&#39;, &#39;Ohio&#39;, &#39;Teas&#39;, &#39;Oregon&#39;])series = frame.iloc[0]frameOutput:   b d e Utah 0.0 1.0 2.0 Ohio 3.0 4.0 5.0 Texas 6.0 7.0 8.0 Oregon 9.0 10.0 11.0 seriesOutput: b 0.0 d 1.0 e 2.0 Name: Utah, dtype: float64By default, arithmetic between a DataFrame and Series matches the index of the Series on the DataFrame’s columns, broadcasting down the rowsframe - seriesOutput:   b d e Utah 0.0 0.0 0.0 Ohio 3.0 3.0 3.0 Texas 6.0 6.0 6.0 Oregon 9.0 9.0 9.0 If an index value is not found in either the DataFrame’s columns or the Series’s index, then the objects will be reindexed to form the unionseries2 = pd.Series(range(3), index = [&#39;b&#39;, &#39;e&#39;, &#39;f&#39;])frame + series2Output:   b d e f Utah 0.0 NaN 3.0 NaN Ohio 3.0 NaN 6.0 NaN Texas 6.0 NaN 9.0 NaN Oregon 9.0 NaN 12.0 NaN If we want to instead broadcast over the columns, matching on the rows, we have to use one of the arithmetic methodsframeOutput:   b d e Utah 0.0 1.0 2.0 Ohio 3.0 4.0 5.0 Texas 6.0 7.0 8.0 Oregon 9.0 10.0 11.0 series3 = frame[&#39;d&#39;]series3Output: Utah 1.0 Ohio 4.0 Teas 7.0 Oregon 10.0 Name: d, dtype: float64frame.sub(series3, axis = &#39;index&#39;)Output:   b d e Utah -1.0 0.0 1.0 Ohio -1.0 0.0 1.0 Texas -1.0 0.0 1.0 Oregon -1.0 0.0 1.0 The axis number that we pass is the axis to mathc on. In this case we mean to match on the DataFrame’s row index (axis=’index’ or axis= 0) and broadcast across the columns.Function Application and MappingNumpy ufuncs (element-wise array methods) also work with pandas objectsframe = pd.DataFrame(np.random.randn(4,3), columns = list(&#39;bde&#39;), index = [&#39;Utah&#39;, &#39;Ohio&#39;, &#39;Texas&#39;, &#39;Oregon&#39;])frameOutput: b d eUtah -0.059728 -1.671352 -2.322987Ohio -1.072084 -1.265158 -1.452127Texas -1.487410 -2.289852 -1.427222Oregon -0.852068 -0.911926 0.486711np.abs(frame)Output: b d eUtah 0.059728 1.671352 2.322987Ohio 1.072084 1.265158 1.452127Texas 1.487410 2.289852 1.427222Oregon 0.852068 0.911926 0.486711Another frequent operation is applying a function on one-dimensional arrays to each column or row. DataFrame’s apply method does exactly this.f = lambda x: x.max() - x.min()frame.apply(f)Output: b 1.427681 d 1.377926 e 2.809697 dtype: float64Here, the function f, which computes the difference between the maximum and minimum of a Series, is invoked once on each column in frame. The result is a Series having the columns of frame as its index.If we pass axis = &#39;columns&#39; to apply, the function will be invoked once per row insteadframe.apply(f, axis = &#39;columns&#39;)Output: Utah 2.263259 Ohio 0.380044 Texas 0.862630 Oregon 1.398637 dtype: float64Many of the most common array statistics (like sum and mean) are DataFrame methods so using apply is not necessary.The function passed to apply need not return a scalar value, it can also return a Series with multiple values.For example:def f(x): return pd.Series([x.min(),x.max()], index = [&#39;min&#39;, &#39;max&#39;])frame.apply(f)Output: b d emin -1.487410 -2.289852 -2.322987max -0.059728 -0.911926 0.486711frame.apply(f, axis = &#39;columns&#39;)Output: min maxUtah -2.322987 -0.059728Ohio -1.452127 -1.072084Texas -2.289852 -1.427222Oregon -0.911926 0.486711Element wise python functions can be used, too. Suppose, you wanted to compute a formatted string from each floating-point value in frame. You can do this with applymapformat = lambda x: &#39;%2f&#39; % xframe.applymap(format)Output: b d eUtah -0.059728 -1.671352 -2.322987Ohio -1.072084 -1.265158 -1.452127Texas -1.487410 -2.289852 -1.427222Oregon -0.852068 -0.911926 0.486711The reason for the name applymap is that Series has a map method for applying an element-wise functionframe[&#39;e&#39;].map(format)Output: Utah -2.322987 Ohio -1.452127 Texas -1.427222 Oregon 0.486711 Name: e, dtype: objectSorting and RankingSorting a dataset by some criterion is another important built-in operation. To sort lexicographically by row or column index, use the sort_index method, which returns a new, sorted objectobj = pd.Series(range(4), index = [&#39;d&#39;, &#39;a&#39;,&#39;b&#39;,&#39;c&#39;])objOutput: d 0 a 1 b 2 c 3 dtype: int64obj.sort_index()Output: a 1 b 2 c 3 d 0 dtype: int64With a DataFrame, we can sort by index on either axisframe = pd.DataFrame(np.arange(8).reshape((2,4)), index = [&#39;three&#39;, &#39;one&#39;], columns = [&#39;d&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;])frameOutput: d a b cthree 0 1 2 3one 4 5 6 7frame.sort_index()Output: d a b cone 4 5 6 7three 0 1 2 3frame.sort_index(axis = 1)Output: a b c dthree 1 2 3 0one 5 6 7 4The data is sorted in ascending order by default, but can be sorted in descending order too using the ascending attributeframe.sort_index(axis = 1, ascending=False)Output: d c b athree 0 3 2 1one 4 7 6 5To sort a Series by its values, use it’s sort_values methodobj = pd.Series([4,7,-3,2])obj.sort_values()Output: 2 -3 3 2 0 4 1 7 dtype: int64Any missing values are sorted to the end of the Series by defaultobj = pd.Series([4, np.nan, 7, np.nan, -3, 2])obj.sort_values()Output: 4 -3.0 5 2.0 0 4.0 2 7.0 1 NaN 3 NaN dtype: float64When sorting a DataFrame, we can use the data in one or more columns as the sort keys.To do so, pass one or more columns names to the by option of sort_valuesframe = pd.DataFrame({&#39;b&#39;: [4,7,-3,2,], &#39;a&#39;: [0,1,0,1]})frameOutput: b a0 4 01 7 12 -3 03 2 1frame.sort_values(by=&#39;a&#39;)Output: b a0 4 02 -3 01 7 13 2 1To sort by multiple columns, pass a list of nameframe.sort_values(by=[&#39;a&#39;, &#39;b&#39;])Output: b a2 -3 00 4 03 2 11 7 1Ranking assigns ranks from one through the number of valid data points in an array. The rank methods for Series and DataFrame are the place to look; by default rank breaks ties by assinging each group the mean rankobj = pd.Series([7,-5,7,4,2,0,4])obj.rank()Output: 0 6.5 1 1.0 2 6.5 3 4.5 4 3.0 5 2.0 6 4.5 dtype: float64Ranks can also be assigned according to the order in which they’re observed in the dataobj.rank(method=&#39;first&#39;)Output: 0 6.0 1 1.0 2 7.0 3 4.0 4 3.0 5 2.0 6 5.0 dtype: float64Here, instead of using the average rank 6.5 for the entries 0 and 2, they instead have been set ot 6 and 7 because label 0 precedes label 2 in the data.We can rank in descending order tooobjOutput: 0 7 1 -5 2 7 3 4 4 2 5 0 6 4 dtype: int64obj.rank(ascending=False, method = &#39;max&#39;)Output: 0 2.0 1 7.0 2 2.0 3 4.0 4 5.0 5 6.0 6 4.0 dtype: float64DataFrame can compute ranks over the rows or the columnsframe = pd.DataFrame({&#39;b&#39;: [4.3, 7, -3,2], &#39;a&#39;: [0,1,0,1], &#39;c&#39;: [-2,5,8,-2.5]})frameOutput: b a c0 4.3 0 -2.01 7.0 1 5.02 -3.0 0 8.03 2.0 1 -2.5frame.rank(axis=&#39;columns&#39;)Output: b a c0 3.0 2.0 1.01 3.0 1.0 2.02 1.0 2.0 3.03 3.0 2.0 1.0Axis Indexes with Duplicate Labelsobj = pd.Series(range(5), index = [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;,&#39;b&#39;, &#39;c&#39;])objOutput: a 0 a 1 b 2 b 3 c 4 dtype: int64The index’s is_unique property can tell you whether its labels are unique or notobj.index.is_uniqueOutput: FalseData Selection is one of the main things that behaves differently with duplicates. Indexing a label with multiple entries returns a Series, while single entries return a scalar value.obj[&#39;a&#39;]Output: a 0 a 1 dtype: int64obj[&#39;c&#39;]Output: 4This can make our code more complicated as the output type from indexing can vary based on whether a label is repeated or not.df = pd.DataFrame(np.random.randn(4,3), index = [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;])dfOutput: 0 1 2a -0.088814 -0.602398 0.402683a 1.195694 -0.383322 0.330696b -3.542210 0.460190 0.339993b -0.718968 -0.049578 0.127387df.index.is_uniqueOutput: Falsedf.loc[&#39;b&#39;]Output: 0 1 2b -3.542210 0.460190 0.339993b -0.718968 -0.049578 0.127387Summarizing and Computing Descriptive Statisticspandas objects are equipped with a set of common mathematical and statistical meth‐ods. Most of these fall into the category of reductions or summary statistics, methods that extract a single value (like the sum or mean) from a Series or a Series of values from the rows or columns of a DataFrame.df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5], [np.nan, np.nan], [0.75, -1.3]], index = [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;], columns = [&#39;one&#39;, &#39;two&#39;])dfOutput: one twoa 1.40 NaNb 7.10 -4.5c NaN NaNd 0.75 -1.3Calling DataFrame’s sum method returns a Series containing Column Sumsdf.sum()Output: one 9.25 two -5.80 dtype: float64Passing axis = &#39;columns&#39; or axis = 1 sums across the columsn insteaddf.sum(axis=&#39;columns&#39;)Output: a 1.40 b 2.60 c 0.00 d -0.55 dtype: float64df.sum(axis=&#39;columns&#39;,skipna=False)Output: a NaN b 2.60 c NaN d -0.55 dtype: float64NA values are excluded unless the entire slice (row or column in this case) is NA. This can be disabled with the skipna optiondf.mean(axis=&#39;columns&#39;, skipna= False)Output: a NaN b 1.300 c NaN d -0.275 dtype: float64Some methods like idxmin and idxmax return indirect statistics like the index value where the minimum or maximum values are attained:df.idxmax()Output: one b two d dtype: objectOther methods are accumulationsdf.cumsum()Output: one twoa 1.40 NaNb 8.50 -4.5c NaN NaNd 9.25 -5.8Another type of method is neither a reduction nor an accumulatoin. describeis one such example, producing multiple summary statistics in one shot.df.describe()Output: one twocount 3.000000 2.000000mean 3.083333 -2.900000std 3.493685 2.262742min 0.750000 -4.50000025% 1.075000 -3.70000050% 1.400000 -2.90000075% 4.250000 -2.100000max 7.100000 -1.300000On non-numeric data, describe produces alternative summary statisticsobj = pd.Series([&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;c&#39;] * 4)obj.describe()Output: count 16 unique 3 top a freq 8 dtype: objectUnique Values, Value Counts, and Membershipobj = pd.Series([&#39;c&#39;, &#39;a&#39;, &#39;d&#39;, &#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;c&#39;, &#39;c&#39;])uniques = obj.unique()uniquesOutput: array([&#39;c&#39;, &#39;a&#39;, &#39;d&#39;, &#39;b&#39;], dtype=object)The unique values are not necessarily returned in sorted order, but could be sorted after the fact if needed (uniques.sort()). Relatedly, value_counts computes a Series containing value frequenciesobj.value_counts()Output: a 3 c 3 b 2 d 1 dtype: int64The Series is sorted by value in descending order as a convenience. But it can be sorted in ascending order as well by setting the attribute of sort value to Falsepd.value_counts(obj.values, sort = False)Output: a 3 d 1 c 3 b 2 dtype: int64isin performs a vectorized set memebership check and can be useful in filtering a dataset down to a subset of values in a Series or column in a DataframeobjOutput: 0 c 1 a 2 d 3 a 4 a 5 b 6 b 7 c 8 c dtype: objectmask = obj.isin([&#39;b&#39;, &#39;c&#39;])maskOutput: 0 True 1 False 2 False 3 False 4 False 5 True 6 True 7 True 8 True dtype: bool#equivalent numpy expressionnp.in1d(obj,[&#39;b&#39;,&#39;c&#39;])Output array([ True, False, False, False, False, True, True, True, True])obj[mask]Output: 0 c 5 b 6 b 7 c 8 c dtype: objectRelated to isin is the Index.get_indexer method, which gives us an index array from an array of possibly non-distinct values into another array of distinct valuesto_match = pd.Series([&#39;c&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, &#39;c&#39;, &#39;a&#39;])unique_vals = pd.Series([&#39;c&#39;, &#39;b&#39;, &#39;a&#39;])pd.Index(unique_vals).get_indexer(to_match)Output: array([0, 2, 1, 1, 0, 2], dtype=int64)In some cases, we may want to compute a histogram on multiple related columns in a DataFrame.data = pd.DataFrame({&#39;Qu1&#39;: [1,3,4,3,4], &#39;Qu2&#39;: [2,3,1,2,3], &#39;Qu3&#39;: [1,5,2,4,4]})dataOutput: Qu1 Qu2 Qu30 1 2 11 3 3 52 4 1 23 3 2 44 4 3 4data.apply(pd.value_counts)Output: Qu1 Qu2 Qu31 1.0 1.0 1.02 NaN 2.0 1.03 2.0 2.0 NaN4 2.0 NaN 2.05 NaN NaN 1.0result = data.apply(pd.value_counts).fillna(0)resultOutput: Qu1 Qu2 Qu31 1.0 1.0 1.02 0.0 2.0 1.03 2.0 2.0 0.04 2.0 0.0 2.05 0.0 0.0 1.0" }, { "title": "4. Patterns 2", "url": "/posts/mongodb-dm4/", "categories": "MongoDB, MongoDB Data Modeling", "tags": "mongodb", "date": "2022-06-17 00:00:00 +0545", "snippet": "Computed PattternSome computations are complex and if we store the operands of the computation separately, we can end up with poor peformance. Math operations: sum, average, median,… usually calling a built-in function on the server.E.g. Keeping track of ticket sales. Instead of summing up the screenings docs every time a movie is displayed, we save the sum and update it whenever a new screening document comes in. Fan-out operations: do many tasks to represent a logical task. Two schemes: fan-out reads and read from different locations when reading, of fan-out writes that involves multiple write operations but not fanning-out when reading. Roll-up operations: is a drill-down operation, data is meged together. Grouping time-series data from small intervals into larger ones is a good example of rolling-up.E.g. reporting hourly, daily, monthly or yearly summaries.All math operations are roll-ups, but we can think of roll-ups as a grouping operation. Doing the roll-up at the right time makes more sense than always doing it when reading the aggregation.Problems Costly computational manipulation of data Executed frequently on the same data with same resultSolution Perform the operation and store the result in the appropiate document or collection If need to redo calculation keep source of our computationsUse Cases IoT, event sourcing, time-series data, frequent aggregationsBenefits/Trade-offs ✔ Faster read queries ✔ Saving resources (CPU/disk) ✘ Difficult to indentify the need ✘ Do not overuse as complexity increasesBucket PatternsIn the IoT world, having 10 million temperature sensors, each sending temp data every minute means 36 billion pieces of information oer hour. Storing each readout as a document, means too many docs to manage.Keeping one document per device means the 16 MB limit per document will be reached sooner or later. Or the big documents are not manageable.A suggestion is one doc per, device per day. Every new day a new document will be created. The doc contains an array per hour.This easy-to-understand-for-human structure makes it difficult to average the temp for a given period =&amp;gt; get the array or section for each hour of the period and do the calculation. Maybe a single array for a day makes more sense.Besides, if data is not coming, the position needs to be inserted with missing (null) entry. { _id: &quot;device_id&quot;, date: ISODate(&quot;2018-03-01&quot;), temp: [ [20, 20, 20.1, ...], [21.1, 20.9,...], ... ] }Another suggestion is have a document per device, per hour. There are 24 times more documents, but they are smaller. Beside, the temperatures are in an object with a minute key.For missing data, the value for the minute key will simply not be there. { _id: &quot;device_id&quot;, date: ISODate(&quot;2018-03-01T13&quot;), temp: {1: 20, 2: 20, 3: 20.1, ...} ] } { _id: &quot;device_id&quot;, date: ISODate(&quot;2018-03-01T14&quot;), temp: {1: 20, 2: 20, 3: 20.1, ...} ] }There are multiple ways of a bucketing and each one is fit for a set of operations.Another example when bucketing comes handy is storing message in a chat app with many channels.We can create a documetn per channel per day, for instance. It makes easy to archive or shard to a cheaper storage based on date.Another usage is mimicking column-oriented databases to bring only the data that is needed for each document, instead of the whole document. In the IoT world, imagine that a device pushes multiple fields. We can create a bucket with the values for each field and a different document contains only one field. It becomes very efficient to calculate on a single field.Gotchas Random insertion and deletion in buckets. Difficult sorting across buckets Ad-hoc queries are more complex across buckets Works best when complexity is hidden behind application.Problem Avoiding too many documents or too big documents 1-* that can’t be embedded.SolutionDefine optimal amount of info and store it in an array in the main object.It is a embedded 1-* with N docs, each having an average sub-docsUse CasesIoT, data-warehousing, lots of information associated to a documentBenefits/Trade-offs ✔ Good-balance between data access and size of data returned ✔ Makes data more manageable ✔ Easy to prune data ✘ Can lead to poor query results if not designed correctly ✘ Less friendly to BI tools as bucketing needs to be understood by queriesSchema Versioning PatternSchema is going to change, but mongo allows those changes without downtime because the schema is flexible.We can have documents that have a property and other documents with another array property. Each will be annotated with the version of the schema that it conforms to.In a relational database there is only one schema per table.The application will have to handle all different versions of the document. Additionally, the document can be brought forward to the latest version whenever is retrieved. When all docs are migrated to the latest version, it is up to us to remove the previous version handlers.Optionally, docs can be migrated via background jobs.Problem Avoid downtime doing schema updates Upgrade docs can take way too long Not all documents need to be updatedSolution Each doc has a schema version. The app can handle all version Pick a strategy to migrate the docs.Use Cases Every app the has a production-deployed db Systems with a lot of legacy dataBenefits/Trade-offs ✔ No downtime ✔ Full control of migrations process ✔ Less technical debt ✘ May need multiple indexes fot the same field during migration periodTree PatternsModel hierarchical information.There are several use cases for hirarchies between objects/nodes: organization charts, books in bookstore or product categorization.There are a set of common operations that are useful for these hierarchical data: find ancestors of node X (faX) find reports to Y (frY) Find all nodes under Z (fuZ)move children from parent N to parent P (mNP)…Documents are hierarchical by nature.StructuresThere are several structures that can be combined. For example, using ancestors array and parent reference:{ _id: 8, name: &quot;Umbrellas&quot;, parent: &quot;Fashion&quot;, ancestors: [&quot;Swag&quot;, &quot;Fashion&quot;]}to get the benefits of optimal operations from each model: frY, fuZParent referencesDoc has a ref to parent. { name: &quot;&amp;lt;string&amp;gt;&quot;, parent: &quot;name_of_parent&quot; } faX is an aggregation with $graphlookup of the inmediate parents =&amp;gt; ! frY .find({parent: ‘Y’}) =&amp;gt; y fuZ is straighforward, but iteration over several docs is needed =&amp;gt; ! mNP is single update where parent = N =&amp;gt; yChild referencesDoc has an array property with inmediate children{ name: &quot;&amp;lt;string&amp;gt;&quot;, children: [&quot;child_1&quot;, &quot;child_2&quot;, ...]} faX =&amp;gt; ! frY =&amp;gt; ! fuZ is really simple and a single query .find(name: Z, {children: 1}) =&amp;gt; y mNP =&amp;gt; !Array of ancestorsDoc has a ancestors array that represent the ancestors path.{ name: &quot;&amp;lt;string&amp;gt;&quot;, ancestors: [&quot;parent&quot;, &quot;grandparent&quot;, &quot;great-grandparent&quot;...]} faX .find({name: X}, {ancestors: 1}) =&amp;gt; y frY =&amp;gt; y fuZ =&amp;gt; y mNP =&amp;gt; !Materialized pathsDoc as a ancestors string that represents ancestors path with a delimiter.{ name: “&quot;, ancestors: &quot;parent.grandparent.great-grandparent&quot;} faX .find({ancestors: /.Y$/} ) =&amp;gt; y frY =&amp;gt; ! fuZ =&amp;gt; ! mNP =&amp;gt; !Problem Represent hierarchical structured data different access patterns to navigate the tree Optimize for common operationsSolutionUse one or more: child reference parent reference array of ancestors materialized pathsUse Cases Org charts CategorizationBenefits/Trade-offs ✔ child reference: easy to navigate to children or tree descending access ✔ parent reference: immediate parent discovery and tree updates ✔ array of ancestor: navigate upward the ancestor path ✔ materialized path: use regex to find nodes in treePolymorphic PatternOrganizing documents by either what is common or by what is different: same or different collection.Different collections involves more difficulties to find query results that come from both collections.=&amp;gt; Group things together because we want to query them together.E.g. vehicles collection that have cars (owner, taxes, #wheels), trucks (owner, taxes, #wheels, #axels) and boats (owner, taxes). Some doc types have properties that others have not. Having a vehicle discriminator (vehicle_type) is the canonic implementation of a polymorphic collection, because determines the expected shape of the document.Another case of polymorphism occurs in embedded sub-documents. For example a customer with several addresses in different countries. Addresses are different by country, containing different properties.Based on the country we can infer the shape of the sub-document.This pattern is commonly applied when we need a single (unified) view of the different types of documents. The relational solution to this problem is sub-optimal. When using document databases, the solution is much simpler.The schema versioning pattern is a case of polymorphic pattern. With the version being the discriminator.Problem Store in a single view (collection) objects that are more similar than different.Solution Use a discriminator field that tracks the type of document or sub-document.The application is responsible to treat each type accordinglyUse Cases single-view systems product catalogs content management systemsBenefits/Trade-offs ✔ Easy to implement ✔ Allows queries across a single collectionApproximation PatternReduce the resources needed to perform some write operations.For example, tracking page views can be achieved writing to the db each time a page is viewed, but it can be too many writes whenever there are hundreds of thousands visits a day.Instead, we increment +10 or +100 once in a while.Problem Data is expensive to calculate Precission is not criticalSolution Fewer writes, but with higher payloadThe application is responsible to treat each type accordinglyUse Cases visit counters Counters tolerant to precision Metric statisticsBenefits/Trade-offs ✔ Less writes ✔ Less contention on documents ✔ Statistically valid numbers ✘ Not exact numbers ✘ Application must implement the correct write logicOutlier PatternHandling documents that stand out from the rest.In big data scenarios, means you have few orders of magnitude higher than the norm.For example, famous singers have hundreths of millions followers, while majority have less than 1000.Outlier can drive developers to develop for the corner case, making it sub-optimal for 99% of the cases and hurting majority of queries.For example, tracking the names of the extras in a movie. For most movies, can embed the less than 1000 names in an array. For those movies that many more extras we can overflow that array into another collection and flag the originating document as an outlier. The application will know that the list of extras overflows into another collection.Problem Few document could drive the overall solution Managing those few documents would impact negatively majority of the queriesSolution Implement a solution that works for majority Identify the exceptional docs with a field Handle outliers differentlyUse Cases Social networks Popularity Metric statisticsBenefits/Trade-offs ✔ Optimized solution for most use cases ✘ Difference handle in the application ✘ Difficult aggregated or ad-hoc queries (bc exception is handled application-side)" }, { "title": "3. Patterns 1", "url": "/posts/mongodb-dm3/", "categories": "MongoDB, MongoDB Data Modeling", "tags": "mongodb", "date": "2022-06-17 00:00:00 +0545", "snippet": "PatternsPatterns are very exciting because they are the most powerful tool for designing schemas for MongoDB and NoSQL. Patterns are not full solution to problems. Those are called full solutions. Patterns are a smaller section of those solutions. They are reusable units of knowledge.We will see how to optimize when faced with large documents with a subset pattern, use the computed pattern to avoid repeated computations, structure similar fields with the attribute pattern, handle changes to your deployment without downtime with the schema versioning pattern, and much more.Those patterns will also serve as a common language for your teams working on schema designs. Finally, having well-defined patterns and understanding when and how to use them will remove a little bit of the are in data modeling for MongoDB and make the process more predictable.Handling Duplication Staleness and Referential IntegrityPatterns are a way to get the best out of your data model. Often, the main goal is to optimize your schema to respond to some performance operation or optimize it for a given use case or access pattern.However, many patterns lead to some situation that would require some additional actions. For example: - Duplication: Duplicating data across documents Data Staleness: Accepting staleness in some pieces of data Data Integrity Issues: Writing extra applicaiton side logic to ensure referential integrityChoosing a pattern to be applied to your schema requires taking into account these three concerns. If these concerns are more important than the potential simplicity of performance gains provided by the pattern, you should not use the pattern.DuplicationWhy do we have duplication?It is usually the result of embedding information in a given document for faster access.The concern is that it makes handling changes to duplicated information a challenge for correctness and consistency, where multiple documents across different collections may need to be updated. There is this general misconception that duplication should not exist. In some cases, duplication is better than no duplication. However, not all pieces of information are affected in the same way by duplication.Let’s start with a situation where duplicating information is better than not doing it.Let’s link orders of products to the address of the customer that placed the order by using a reference to a customer document. Updating the address for this customer updates information for the already fulfilled shipments, order that have been already delivered to the customer. This is not the desired behavior. The shipments were made to the customer’s address at that point in time, either when the order was made or before the customer changed their address. So the address reference in a given order is unlikely to be changed.Embedding a copy of the address within the shipment document will ensure we keep the correct value. When the customer moves, we add another shipping address on file. Using this new address for new orders, does not affect the already shipped orders.The next duplication situation to consider is when the copy data does not ever change.Let’s say we want to model movies and actors. Movies have many actors and actors play in many movies. So this is a typical many-to-many relationship. Avoiding duplication in a many-to-many relationship requires us to keep two collections and create references between the documents in the two collections. If we list the actors in a given movie document, we are creating duplication. However, once the movie is released, the list of actors does not change. So duplication on this unchanging information is also perfectly acceptable.This leaves us with the last duplication situation, the duplication of a piece of information that needs to or may change with time.For this example, let’s use the revenues for a given movie, which is stored within the movie, and the revenues earned per screening. Oh, yeah, with said duplication add to be a single value in two locations. In this case, we have duplication between the sum store in the movie document and the revenue store in the screening documents used to compute the total sum. This type of situation, where we must keep multiple values in sync over time, makes us ask the question is the benefit of having this sum precomputed surpassing the cost and trouble of keeping it in sync? If yes, then use this computed pattern. If not, don’t use it.Here, if we want the sum to be synchronized, it may be the responsibility of the application to keep it in sync. Meaning, whenever the application writes a new document to the collection or updates the value of an existing document, it must update the sum. Alternatively, we could add another application or job to do it.StalenessStaleness is about facing a piece of data to a user that may have been out of date. We now live in a world that has more staleness than a few years ago. Due to globalization and the world being flatter, systems are now accessed by millions of concurrent users, impacting the ability to display up-to-the-second data to all these users more challenging.For example, the availability of a product that is shown to a user may still have to be confirmed at checkout time. The same goes for prices of plane tickets or hotel rooms that change right before you book them.Why do you get staleness?New events come along at such a fast rate that updating data constantly can cause performance issues. The main concern when solving this issue is data quality and reliability. We want to be able to trust the data that is stored in the database.The right question is, for how long can the user tolerate not seeing the most up-to-date value for a specific field. For example, the user’s threshold for seeing if something is still available to buy is lower than knowing how many people view or purchase a given item. When performing analytic the queries it is often understood that the data may be stale and that the data being analyzed is based on some past snapshot. Analytic queries are often run on the secondary node, which often may have stale data. It may be a fraction of a second or a few seconds out of date. However, it is enough to break any guarantee that we’re looking at the latest data recorded by the system.The solution to resolve staleness in the world of big data is to batch updates.As long as the updates are run within the acceptable thresholds, staleness is not a significant problem. So, yes, every piece of data has a threshold for acceptable staleness that goes from 0 to whatever makes sense for given piece of information. A common a way to refresh stale data is to use a Change Stream to see what has changed in some documents and derive a list of dependent piece of data to be updated.Referential IntegrityOur third concern, when using patterns, is referential integrity. Referential integrity has some similarities to staleness. It may be OK for the system to have some extra or missing links, as long as they get corrected within the given period of time.Why do we get refferential integrity issues?Frequently, it may be the result of deleting a piece of information [INAUDIBLE] document– for example, without deleting the references to it. In the big data world, we can also associate referential integrity issues to adding distributed system, where a related piece of information live on different machines.At this time, the MongoDB server does not support foreign keys and associated cascading deletes and updates responsible for keeping referential integrity. It is the responsibility of the application to do so. Here again, the main concern is data quality and reliability.Attribute PatternPolymorphic, one of the most frequent schema design patterns used in MongoDB.Polymorphic is when you put different products, like these three examples, in one collection without going through relational acrobatics.Our products should have an identification like manufacturer, brand, sub-brand, enterprise that are common across the majority of products. Products’ additional fields that are common across many products, like color and size– either these values may have different units and means different things for the different products. For example the size of a beverage made in the US maybe measured as ounces, while the same drink in Europe will be measured in milliliters. As for MongoDB charger, well, the size is measured according to its three dimensions.For the size of a Cherry Coke six-pack, we would say 12 ounces for a single can, six times 12 ounces, or 72 ounces to count the full six-pack. Ultimately we could list the physical dimension and report the amount of the liquid in that field.Note that physical dimensions for a beverage make sense if your main concern is the storage or transportation of the product, not the drinking of it. Then there is the third list of fields, the set of fields that are not going to exist in all the products. You may not even know where they are in advance.They may exist in the new description that your supplier is providing you. For a sugary drink, you may want to know the type of sweetener, while for a battery, you are more interested in its specifications, like the amount of electricity provides. For the characteristics that are almost always present, we keep them as fields those qualify as the common schema part.Schema and indexing may appear in the third list of fields. To search effectively on one of those fields, you need an index. For example, searching on the capacity for my battery would require an index. Searching on the voltage output of my battery would also require an index. If you have tons of fields, you may have a lot of indexes.Remember some of the characteristics may be very specific to a few products and the list of fields may be unpredictable. Each addition or discovery of a new characteristic may require you to add an index, modify your schema validators, and modify your user interface to show the new information. For this case you want to use the attribute pattern.We might want to use the attribute pattern. identify the fields to tranpose for each field and its value a named tuple is created: {k: &quot;name_of_the_field&quot;, v: &quot;value_of_the_field&quot;} and place them all inside a new array property (props, for instance). There could be a third field in the tuple with the unit, for example. Create an index on {props.k: 1, props.v: 1}Another example: a movie has differnet dates of release in different countries: { title: &quot;Dunkirk&quot;, release_USA: &quot;2017/07/23&quot;, release_Mexico: &quot;2017/08/01&quot;, release_France: &quot;2017/08/01&quot;, ... }What if we wanted to find all the movies released between two dates across all countries?Moving to { title: &quot;Dunkirk&quot;, releases: [ {k: &quot;USA&quot;, v: &quot;2017/07/23&quot; }, {k: &quot;Mexico&quot;, v: &quot;2017/08/01&quot;}, {k: &quot;France&quot;:, v: &quot;2017/08/01&quot; } ] ... }makes the query so much simpleProblem Lot’s of similar fields with similar types and need to search across those fields at once. Another case is when only a subset of documetns have many similar fields.SolutionTransform fields into a new array property of key, value pairs with the key being the name of the field and the value, its value. Then create an index containing both key and valueUse Cases Searchable characteristics of a product Set of fields all having the same value type (list of dates)Benefits/Tradeoffs Easier to index Allow variety of field names Allows qualifying the relationship between key and value with a third tuple field.Extended Reference PatternIf you find yourself “joining” data between collections, event if the query is not so horrid, with a high volume of data, performance is a liability.Before lookup, all joining had to be done in the application with multiple queries involved.graphLookup allows recursive queries on the same collection, like on graph databases.Another way would be embedding in the one-side of the 1-* relationship. But… what if the joins come from the other side?Imagine a 1-* between a customer and its orders. But we usually query orders, not customers.Embedding the most used information (duplication) in the many-side while maintaining the reference field in the many, allows us to not having to join most of the time, but joining if we must at the expense of duplication.Duplication management: minimize it: duplicate fields that change rarely and only the fields needed to avoid the join After change in “master” information: identify what needs to be chanaged and do it straight away if we must or wait for a batch update to do it at a later stageIn the example, duplicating is the right thing to do.Problem too many repetitive joinsSolution identify the field on the lookup side bring in those fields into the main objectUse Cases-Catalog, mobile applications, real-time analytics.That is: optimize read operations by avoiding round-trips or touching too many pieces of data.Benefits/Trade-offs ✔ Faster reads ✔ Reduced number of joins and lookups ✘ Duplication if the extended reference contains data that changes a lotSubset PatternMongoDB tries to put in memory the working set (docs and portion of indexes that are accessed).If the working set does not fit in memory, MongoDB will start trying to occupy RAM and there will be a constant thrashing of data trying to make it to RAM just to be evicted shortly after for other data to get there.Solutions: add more RAM (or more nodes to the cluster) scale with sharding (or are more shards) reduce the size of the working setWill focus on third. Get rid of part of huge documents that is not used so often.For example, for a movie, people might want to access only the main actors, top reviews or quotes. The rest can go into a separate collection.ProblemWorking set does not fit in memoryOr only some of the data of the working set is frequently used.Lots of pages evicted from memorySolutionDivide the document in two: fields that are often required and fields that are rarely required.The resulting relationship will be a 1-1 or 1-*.Part of the information is duplicated in the most used side.Use Cases List of review of a product. List of comments on an article List of actors in a movieBenefits/Trade-offs ✔ Smaller working set as most used docs are smaller ✔ Short disk access from most used collection ✘ More round trips to the server when accessing less frequent information ✘ Mor disk usage as information is duplicated" }, { "title": "2. Introduction to Relationships", "url": "/posts/mongodb-dm2/", "categories": "MongoDB, MongoDB Data Modeling", "tags": "mongodb", "date": "2022-06-16 00:00:00 +0545", "snippet": "Relationships with MongoDBEven if MongoDB is classified as a document database, the pieces of information stored in the database still have relationships between them. Understanding how to represent relationships, and deciding between embedding and linking relationship information, is crucial.Having a good model is the single most important thing you can do to ensure you get good performance. The face of identifying and modeling relationships correctly is a step that is not optional in the methodology.What are relationships in the data model??If you look at any schema implementation in MongoDB, or any other database, you can observe objects, referred to as entities.The relationships represent all the entities and the other piece of information are related to each other.For example, a customer name and its customer ID have a one to one relationship. We often group this type of relationship in a single entity. The relationship between a customer and the invoice sent to them is a one to many relationship, as one customer has many invoice, but each invoice only belong to one customer.And the invoices and their products have a many to many relationship, as the invoice referred to many products, and a product is likely listed in many invoices.Types and Cardinality of RelationshipsMost of the relationships between units of information can be classified as: one-to-one: A customer has one name, which is associated with only one customer_id, and this customer_id can only be used to identify one customer’s name. The one-to-one relations are often represented by grouping the two pieces of data in the same entity or document. one-to-many: Invoices associated with the customer are an example of a one-to-many relationship. A customer has many invoices, and each of these invoices is only associated with one customer. many-to-many: Finally, an invoice may contain many products, and each of these products is likely present in more invoices than just the one that we were looking at. This is called a many-to-many relationship. However, is this the best and complete way to describe data relationships, especially when dealing with Big Data?Let’s say we look at the relationship between a mother and the children she gave birth to. Well, she may not have children, have one, have two, ten, however, the maximum is pretty limited. Very often there are two children per family.A different example is Twitter users. Some people just started their account and may have zero or one follower, while others may have 20, 100, or up to 100 million if they are a celebrity. In this case, many-to-many is a very poor way to characterize a relationship. And this might be true for an increasing number of examples in the world of Big Data.We could embed the information about the children in the document representing the mother, but it would not make sense to embed 100 million followers into one document.What we need is a more expressive way to represent the one-to-many relationship so that we know that we are dealing with large numbers and avoid mistakes associated with that distinction.Looking at earlier examples, we are missing some information. The fact that the relationship can be a large number isn’t reflected clearly with the one-to-many description, the value for the maximum of many is not clear.The most likely many value for a given one-to-many relationship is also missing.Let’s introduce this additional symbol for the crow foot notation, and call it zillions. It is based on the many symbol, however, with additional lines. This relationship would read as from one to zero to zillion. Or in short, one-to-zillions. This new symbol addresses the identification of large numbers.And if we go to the trouble of identifying the maximum number, why not preserve this information in the model? For this we use a tuple of one to three values, with the following meaning: Minimum, usually zero or one Most likely value, or the median Maximum.If you have two values they represent the minimum and the maximum.When a single value is used it means the relationship is fixed to that number.One-to-Many RelationshipThe most interesting type of relationship is the one-to-many relationship. First, because if all our data is only composed of one-to-one relationships, a spreadsheet application like Excel could do the job, at least for a small data set. As for the many-to-many relationships, most of them can be expressed as two one-to-many relationships.A one-to-many relationship means that an object of a given type is associated with n objects of a second type, while the relationship in the opposite direction means that each of the objects of the second type can only be associated with one object on the one side. Example: - As an example of this relationship, we use a person and their credit cards, or a blog entry and its comments. A person has n credit cards, but each of these credit cards belongs to one and only one person.Using MongoDB and its document model, give us a few ways to represent this kind of relationship. We can embed each document from the many side into the document on the one side. Or vise versa, we can embed the document from the one side into each document on the many side.Instead of using a single collection and embedding the information in it, we keep the documents in two separate collections and reference documents from one collection in documents of the other collection.Many-to-Many RelationshipThe many-to-many relationship is identified by documents on the first side being associated with many documents on the second side, and documents on the second side being associated with many documents on the first side.One-to-One RelationshipCommonly, a one-to-one relationship is represented by a single table in a tabular database. In general, the same applies to MongoDB. For example a person’s name, date of birth, and email address would be kept together in the same document. All these fields have a one-to-one relationship with each other. A user in the system has one and only one name, has one and only one date of birth, and is associated with one and only one email address.When we group information together, that is in two different entities, we refer to this action as embedding. This is in contrast to grouping fields together in a given entity. We refer to those fields as attributes of the entities.One-to-Zillions RelationshipThe one-to-zillions relationship is not that different relationship, but it is a subcase of the one-to-many relationship. If we have a one to many relationship and the many’s identified as 10,000 or more, we call that relationship one to zillion. This means we need to be mindful of this relationship every place we use it in the code. The last thing you want the application to do is to retrieve a document and its zillions associated documents, then process the complete results set." }, { "title": "1. Introduction to Data Modeling", "url": "/posts/mongodb-dm1/", "categories": "MongoDB, MongoDB Data Modeling", "tags": "mongodb", "date": "2022-06-16 00:00:00 +0545", "snippet": "One of the most common misconceptions about data modeling in MongoDB is that modeling is schemaless, meaning that it doesn’t really matter which field documents have or how different the documents can be from one another or even how many collections we might have per database.What MongoDB unquestionably has is a very flexible data model. There are no default rules for what a document should be looking like apart from having correctly been defined in BSON and containing a primary key. But most importantly all data as some sort of structure, and therefore a schema. MongoDB just happens to make it easier for us to deal with that later rather than sooner.Before you jump into an ERD and UML tooling, in order to determine the full scope of our data structures, it tends to be preferable to start building our application and finding out from that particular experience what the data structure should look like.However, if we do know our our usage pattern how our data is accessed which queries are critical to our application ratios between reads and wrtieswe will be able to extract a good model even before writing the full application to make it scale with MongoDB.Being flexible means that your application changes.When we start start having a pretty good idea of how documents should be looking like and should be shaped out and which data types those fields we have, we’ll be able to enforce those rules in MongoDB using document validation.Another misconception is that all information, regardless of how data should be manipulated, can be stored in one single document. There are some usecases where this approach is actually correct. But in reality this is not the way application generally uses data.Keeping the amount of information stored per individual documents to the data that your applicaiton uses and having different models to deal with historical data or other types of data that are not always accessed is something that we’ll be looking into Data Modeling.And there is also this perception that there is no way to perform a join between documents in MongoDB. While MongoDB does not call $lookup a join, for many good reasons, you can still perform all sorts of join in MongoDB.MongoDB Document ModelData in MongoDB stored in a hierarchical structure where the database are at the top level where each MongoDB deployment can have many databases. Then there are one or more collections in the database. And finally, there are documents which are kept at the collection level.In MongoDB, data is stored as BSON documents that are composed of field value pairs, where BSON is a binary representation of JSON documents. Since BSON is not human-readable, we will stick with JSON in our examples throughout this course.Keeping with the standard JSON notation, we have an open curly bracket indicating the beginning of a document, followed by several field value pairs separated by commas.Example: - { &#39;firstName&#39;: &#39;Babin&#39;, &#39;lastName&#39;: &#39;Joshi&#39;, &#39;age&#39;: 24, &#39;phone&#39;: [ 123456789,987654321 ], &#39;address&#39;: { &#39;street&#39;: &#39;One&#39;, &#39;building&#39;: 1, &#39;city&#39;: &#39;Kathmandu&#39;, &#39;province&#39;: 3, &#39;country&#39;: &#39;USA&#39; }, &#39;education&#39;: [ { &#39;College&#39;: &#39;Prasadi&#39;, &#39;Degree&#39;: +2 }, { &#39;College&#39;: &#39;Kathmandu University&#39;, &#39;Degree&#39;: &#39;Bsc. Computer Science&#39; } ] }Each value can be of any BSON data type, which in this case are a string, an integer, another string, and an array.With MongoDB, a document is a collection of attributes for an object. If you’re coming from SQL, a document is like a row in the table that has been joined with other relevant rows from other tables. You have a field and its assigned value, just like each column in a row has a value.Instead of having to query multiple tables of related data and assembling it all together, we can keep your related data in a single document and pull it all down using a single query.We can consider a MongoDB document as being similar to a dictionary or a map or an associative array– an object that has a number of key value pairs.Since MongoDB documents support a flexible structure, we can use a document to represent an entire object rather than having to break up the data across multiple records as we would have to do with the relational database.The exact structure of a document– all the fields, values, and embedded documents– represent the schema of a document.Documents in the same collection don’t need to have the exact same list of fields.Furthermore, the data type in any given field can vary across documents.WE do not have to make changes at the cluster level to support this.Another way to view this is that we can have multiple versions of your schema as our application develops over time and all the schema versions can coexist in the same collection.Intro to MethodologyWe will go over a methodology to help you through the old process of data modeling for MongoDB. The methodology we use is composed of three phases. The first phase is to describe the workload. In other terms, it means gathering everything there is to know about how you would be using your data. The second phase is to identify the relationships between the different entities you need to handle and choose how to model those relationships. And the third phase is to apply design patterns or transformation to the current model to address performance requirements. Let’s describe each phase a little bit more.Our goal is to create a data model, what is often referred to as our MongoDB schema.For example, you may have a requirements document listing the scenarios the system needs to support. Alternatively, or in complement, you may have an expert on the domain, who can advise on what needs to be done. You may be migrating from a relational database, or you are evolving an existing MongoDB database. In both cases, logs, stats, et cetera, give you additional information about the current state of the system. If they exist, you want to use them.Finally, someone needs to assemble this information together in the schema. This is done by the data modeling expert.So the first phase is to look at the documents that you have in your input and create some artifacts out of them. You should be able to size the amount of data your system will have initially, in few months, and in few years. The action of recording those numbers will permit you to observe any major deviations in your system once it’s in operation. Those differences will be a good indicator that you may have to iterate again over your schema. The same applies to the operations, the reads and the writes. You should be able to tell how many are run per unit of time and if each query has additional requirements in terms of execution time, the latency from the application, tolerance to staleness, et cetera.For each of these operation requirements, record your thoughts and assumptions.They will also be a good indicator to see whether you need to reevaluate the model again later.In our second phase, we start with a piece of information that were identified. Each piece has a relationship with another one. The ones that have a one-to-one relationship tend to be grouped together in the same table or collection. In modeling for a relational database, you would probably have come up with those three entities– actors, movies, and reviews. And place the piece of information inside the appropriate entity.For example, a movie title has a one-to-many relationship to the reviews for the movie, while the money earned by the movie has a one-to-one relationship with the movie title. So the movie title and its revenues are in the same entity or collection, while the reviews are in a separate collection. With MongoDB, you follow the same process of identifying the relationships between the pieces of information. However, you need to decide if you embed information or keep it separate. At the end of this process, you will have a list of entities with their fields, some of them grouped together inside the common collection.Our last phase is to apply schema design patterns. This is where you will get to make your model more performant or more clear by applying some transformations.If any of the input information on the left changes, you need to assess the impact on the decision you’ve made in their corresponding phase. For example, if you discover another reported query, get more data about the size of your problem, or run benchmarks on your current solution, all that known information, with feedback as the input to the model. Any successful application will undergo modifications at some point in its lifetime, so be ready to get new inputs at some point. If you track why you made some decision and what were the assumptions in the past, it will be much easier to apply the needed changes.Introduction Modeling for Simplicity vs PerformanceModeling for simplicity means we will avoid any complexity that could slow down the development of the system by our engineers. Frequently, for those kind of projects, there are limited expectations and small requirements in term of CPU, disk, I/O, memory. Things are generally small and simple. You should start by identifying the most important operations for the system. And you will need to establish the relationships between the entities and fields. To keep the model simple, you are likely to group a lot of those pieces inside a few collection using sub-documents or arrays to represent the one-to-one, one-to-many, too many-to-many many relationships. By keeping the modeling steps to the minimum, we can remain very agile, with the ability to quickly iterate on our application, reflecting these changes back into the model if needed.If you model for simplicity, as a result, you will likely see fewer collection in your design where each document contains more information and maps very well to the object you have in your application code– the objects being represented in your favorite language as hashes, maps, dictionary, or nested objects. Finally, as a result of having larger documents with embedded documents in them, the application is likely to do less disk accesses to retrieve the information. These three collection embedded into one, a single read will be sufficient to retrieve the information instead of four.At other end of our axis, we have the performance criteria.In this scenario, resources are likely to be used to the maximum. Project that makes use of sharding to scatter horizontally are likely to fall into this category, because you often shard your database because there is not enough resources available with a simple replica set.The system may require very fast read or writes operation, or it may have to support a ton of operations. Although situations are demanding a model for performance. When you model for performance or have more complexity to handle, you want to go over all the steps of the methodology.Again, you start by identifying the important operations, but also quantify those in terms of metrics like operation per second, required latency, and pinning some quality attributes on those queries such as– can the application work with data that’s a little stale, are those operations parts of the large analytic query?If you model for performance you will often see more collection in your design. You will also need to apply a series of schema design patterns to ensure the best usage of resources like CPU, disk, bandwidth. And in between, well, you have project that have more of a balance or trade between those two criteria." }, { "title": "5. Indexing and Aggregation Pipeline", "url": "/posts/mongodb5/", "categories": "MongoDB, MongoDB Basics", "tags": "mongodb", "date": "2022-06-15 00:00:00 +0545", "snippet": "Aggregation FrameworkThe aggregation framework in its simplest form is just another way to query data in MongoDB. Everything we know how to do using the MongoDB query language (MQL) can also be done using the aggregation framework.Example: - Let’s find all documents that have wi-fi as one of the amenities only including the price and address in the resulting cursor.With MQL, we will use the following command:- db.listingsAndReviews.find( { &#39;amenities&#39;: &#39;Wifi&#39; }, { &#39;price&#39;: 1, &#39;address&#39;: 1, &#39;_id&#39;: 0 } ).pretty()With the aggregation framework, we use the following command:- db.listingsAndReviews.aggregate( [ { &quot;$match&quot;: { &quot;amenities&quot;: &quot;Wifi&quot; } }, { &quot;$project&quot;: { &quot;price&quot;: 1, &quot;address&quot;: 1, &quot;_id&quot;: 0 } } ] ).pretty()To use the aggregation framework, we use the aggregate instead of find. The reason for that is because sometimes we might want to aggregate, as in group or modify our data in some way, instead of always just filtering for the right documents. This means that you can perform operations other than finding and projecting data. But you can also claculate using aggregation. The aggregatoin framework works as a pipeline, where the order of actions in the pipeline matters. And each action is executed in the order in which we list it. Meaning that we give our data to the pipeline on our end, then we describe how this pipeline is going to treat our data using aggregation stages. And then the transformed data emerges at the end of the pipeline.The “$group” stageThe $group stage is one of the many stages that differentiates the aggregation framework from MQL. With MQL, we can filter and update data. With the aggregation framework, we can compute and reshape data. The $group is an operator that takes the incoming stream of data and siphons it into multiple distinct reservoiors.Syntax: { &#39;$group&#39;: { &#39;_id&#39;: &amp;lt;expression&amp;gt;, //Group By Expression &amp;lt;field1&amp;gt;: { &amp;lt;acumulator1&amp;gt;: &amp;lt;expression1&amp;gt; }, ..., } } }Example-1: db.listingsAndReviews.aggregate( [ { &#39;$project&#39;: { &#39;address&#39;: 1, &#39;_id&#39;: 0 } }, { &#39;$group&#39;: { &#39;_id&#39;: &#39;$address.country&#39; } } ] ) The above query projects only the address field value for each document, then group all documents into one document per address.country value.Example-2: db.listingsAndReviews.aggregate( [ { &#39;$project&#39;: { &#39;address&#39;: 1, &#39;_id&#39;: 0 } }, { &#39;$group&#39;: { &#39;_id&#39;: &#39;$address.country&#39;, } } ] ) The above query projects only the address field value for each document, then group all documents into one document per address.country value, and count one for each document in each group.sort() and limit()Sometimes, when we’re creating a collection, we are not interested in all the results, but are looking for the top 3 or top 10 results.Suppose we want to find the least popoulated zip code in the zips collection. Then we willl use the following query. db.zips.find().sort( { &#39;pop&#39;: 1 } ).limit(1).pretty() The above query gets all the documents, sorts them by their population in ascending or increasing order, and only returns the first document in the cursor, a.k.a the one with the smallest population value. db.zips.find().sort( { &#39;pop&#39;: -1 } ).limit(10).pretty() The above query gets all the documents, sorts them by their population in descending or decreasing order, and only returns the first 10 document in the cursor, a.k.a the one with the largest population value.The sort() and limit() are cursore methods. A cursor method is not applied to the data that is stored in the database. It is instead applied to the result set that lives in the cursor. After the curosr is populated with the filter data that’s the result of the Find command, we can then apply the sort() method which will sort the data based on the criteria that we provided.We can sort the data by one or more fields in increasing or decreasing direction. For example: - db.zips.find().sort( { &#39;pop&#39;: 1, &#39;city&#39;: -1 } ) The above query gets all the documenets, sorts them in the increasing order by population and decreasing order by the city name.IndexesIndexes are one of the most impactful way to improving query performance. An index in a databse is, by its function, similar to an index in a book, where you have an alphabetical list of names and subjects with references to the places where they occur. Index in database is a special data structure that stores a small portion of the collection’s data set in an easy to traverse form. In simple terms, an index is a data structure that optimizes queries.Given: db.trips.find({&#39;birth year&#39;: 1989}) db.trips.find({&#39;start station id&#39;: 476}).sort(&#39;birth year&#39;: 1)The first query filters the data by the value of the birth year and the second sorts by the value of that field. Both could benefit from that index. Creating an Index: Single field index db.trips.createIndex( { &#39;birth year&#39;: 1 } ) The above query creates an index on the birth year field in increasing order. Compund index: an index on multiple fields. db.trips.createIndex( { &#39;start station id&#39;: 1, &#39;birth year&#39;: 1 } ) Data ModelingMongoDB doesn’t enforce how data is organized by default. So how can we decide what structure to use to store our data? Where should we create subdocuments? And where should we use arrays of values? At which point should data get its own collection?Making these decision about the shape and structure of our data is called Data Modeling. More specifically, Data Modeling is a way to orgranize fields in a document to support our applicaiton performacen and querying capabilities.The most important rule of thumb in data modeling with MongoDB is that data is stored in the way that it is used. This notion determines the decision that we make about the shape of our document and the number of our collections. Note: Data that is used/acccessed together should be stored together. And as our application evolves, our data model should also evolve.UpsertEverything in MOSQL that can be used to locate a docuent in a collection can also be used to modify the document. db.collection.updateOne( { &amp;lt;query to locate&amp;gt; }, { update } )The first part of the update operation is the query to locate the document in question. One of the awesome features of MQL is the upsert option within the update documents.Upsert is a hybrid of update and insert, and it should only be used when it is needed. db.collection.updateOne( { &amp;lt;query&amp;gt; }, { &amp;lt;update&amp;gt; }, { &#39;upsert&#39;: true } )By default upsert is set to false, but if it is set to ture, we can expect it to do either an update or an insert. THe update will happen if there are documents that match the filter criteria of the update operation. The insert will happen if there are no documents that match the filter criteria.Example: db.iot.updateOne( { &#39;sensor&#39;: r.sensor, &#39;date&#39;: r.date, &#39;valcount&#39;: { &#39;$lt&#39;: 48 } }, { &#39;$push&#39;: { &#39;readings&#39;: { &#39;v&#39;: r.value, &#39;t&#39;: r.time } }, &#39;$inc&#39;: { &#39;valuecount&#39;: 1, &#39;total&#39;: r.value } }, { &#39;upsert&#39;: true } )" }, { "title": "4. Advanced CRUD Operations", "url": "/posts/mongodb4/", "categories": "MongoDB, MongoDB Basics", "tags": "mongodb", "date": "2022-06-15 00:00:00 +0545", "snippet": "MQL Operators Update Operators: Example: $inc, $set, $unset These update operators enable us to modify data in the database. Query Operators: Query Operators provides additional ways to locate data within the database. What Query Operators have in common with all kinds of operators is the $ sign that precedes the operator. Note: $ has multiple uses in MongoDB like: It precedes MQL operators It precedes aggregation pipeline stages. Allows to access field values. Comparison Operators: $eq = EQual to : Allows to compare if the two values are equal $ne = Not Equal to : Allows us to compare if the two values are not equal $gt = Greater Than $gte = Greater Than or Equal $lt = Less Than $lte = Less Than or Equal The Syntax for the Comparision Operators is: Syntax: { &amp;lt;field&amp;gt; : { &amp;lt;operator&amp;gt; : &amp;lt;value&amp;gt; } } Example-1: db.trips.find( { &quot;tripduration&quot;: { &quot;$lte&quot; : 70 }, &quot;usertype&quot;: { &quot;$ne&quot;: &quot;Subscriber&quot; } } ).pretty() The above query finds all the documents in the trips collections where the tripduration was less than or equal to 70 seconds and the usertype was not Subscriber Example-2: db.trips.find( { &quot;tripduration&quot;: { &quot;$lte&quot; : 70 }, &quot;usertype&quot;: { &quot;$eq&quot;: &quot;Customer&quot; } } ).pretty() The above query finds all the documents where the tripduration was less than or equal to 70 and the usertype was Customer using a redundant equality operator. Example-3: db.trips.find( { &quot;tripduration&quot;: { &quot;$lte&quot; : 70 }, &quot;usertype&quot;: &quot;Customer&quot; } ).pretty() The above query finds all the documents where the tripduration was less than or equal to 70 seconds and the usertype was Customer using the implicit equality operator. Logic Operators In MQL, we have the standard set of foru logical operators which are given below:- $and: Returns the documents that meets all of the specified query clauses. $or: Returns the documents as long as at least one of the query clauses is matched. $nor: Returns all documents that fails to match both clauses. $not: Negates the query requirements and therefore returns all the documents that do not match the query. The and, or and nor have the similar syntax where the operator precedes an array of clauses that it will operate o. Syntax for and, or and nor operators: { &amp;lt;operator&amp;gt;: [{statement1}, {statement2},...] } Syntax for not operator: { $not: {statement} } Example-1: db.inspections.find( { &#39;$nor&#39;: [{ &#39;result&#39;: &#39;No violation&#39; }, { &#39;result&#39;: &#39;Pass&#39; }, { &#39;result&#39;: &#39;Fail&#39; }] } ) The above query ensures that every document with the result set to No violation or Pass or Fail will not be part of the result. Example-2: db.grades.find({ &#39;$and&#39;: [ { &#39;student_id&#39;: { &#39;$gt&#39;: 25 } }, { &#39;student_id&#39;: { &#39;$lt&#39;: 100 } } ] }) The above query finds all the students where the student_id is greater than 25 and less than 100 in the sample_training.grades ccollections. But we could also simplify it significantly as we’re querying on the same field, we can get rid of the implied $and. Then, we can also combine both conditions in a single statement like: db.grades.find({ &#39;student_it&#39;: { &#39;$gt&#39;: 25, &#39;$lt&#39;:100 } }) The above query does the same as the one above it but only simpler. Example-3: db.routes.find( { &quot;$and&quot;: [ { &quot;$or&quot; :[ { &quot;dst_airport&quot;: &quot;KZN&quot; }, { &quot;src_airport&quot;: &quot;KZN&quot; } ] }, { &quot;$or&quot; :[ { &quot;airplane&quot;: &quot;CR2&quot; }, { &quot;airplane&quot;: &quot;A81&quot; } ] } ] } ).pretty() The above query finds all documents where airplanesCR2 or A81 left or landed in the KZN airport. Expressive Query Operator The $expr is an expressive query operator meaning it can do more than one simple operation. It allows the use of aggregation expressions within the query language and it uses this syntax. It allows for more complex queries and for comparing fields within a document. Syntax: { $expr: { &amp;lt;expression&amp;gt; } } The $expr also allows us to use variables and conditional statements. Example-1: db.trips.find( { &quot;$expr&quot;: { &quot;$eq&quot;: [ &quot;$end station id&quot;, &quot;$start station id&quot; ] } } ).count() The above query allows us to find all the documents where the trip started and ended at the same station. Example-2: db.trips.find( { &#39;$expr&#39;: { &#39;$and&#39;: [ { &#39;$gt&#39;: [ &#39;$tripduration&#39;, 1200 ] }, { &#39;$eq&#39;: [ &#39;$end station id&#39;, &#39;$start station id&#39; ] } ] } } ).count() Array Operators and Projection $push: It allows us to add an element to an array. It also allows us to turn a field into an array field if it was previously a different type. $sall: It returns a cursor with all docuements in which the specified array field contains all the given elements, regardless of their order in the array. $size: It returns all documents whree the specified array field is exactly the given length. Example: db.listingsAndReviews.find( { &quot;amenities&quot;: { &quot;$size&quot;: 20, &quot;$all&quot;: [ &quot;Internet&quot;, &quot;Wifi&quot;, &quot;Kitchen&quot;, &quot;Heating&quot;, &quot;Family/kid friendly&quot;, &quot;Washer&quot;, &quot;Dryer&quot;, &quot;Essentials&quot;, &quot;Shampoo&quot;, &quot;Hangers&quot;, &quot;Hair dryer&quot;, &quot;Iron&quot;, &quot;Laptop friendly workspace&quot; ] } } ).pretty() The above query finds all the documetns with exactly 20 amenities which includes all the amenities listed in the query array. When we look at the sample_airbnb dataset, we see documents with lots of fields that often don’t fit on the screen. To mitigate this, we can add a projection to our Find queries and only look at fiends that we’re interested in at the moment. Example: db.listingsAndReviews.find( { &#39;amenities&#39;: { &#39;$size&#39;: 20, &#39;$all&#39;: [&#39;Internet&#39;, &#39;Wifi&#39;, &#39;Kitchen&#39;, &#39;Heating&#39;] } }, { &#39;price&#39;: 1, &#39;address&#39;: 1 } ).pretty() The first part of the find() query describes the content that we’re looking for. The second is a projection, describing specifically which fields we’re looking for. This way, the cursor doesn’t have to include every single field in the result set. Note: When using projection, you can specify which fields you do or do not want to see in the resulting cursor. Use 1 to specify the fields that you want to see, and 0 to specify the fields that you don’t want to see. You cannot mix zeros and ones in a single projection. $elemMatch: An Array Operator that can be used both in query and projection part of the find command. It matches the documents that contain an array field with at least one element that matches the specified query criteria. Syntax: { &amp;lt;field&amp;gt;: { &#39;$elemMatch&#39;: { &amp;lt;field&amp;gt; : &amp;lt;value&amp;gt; } } } Example-1: db.grades.find( { &#39;class_id&#39;: 431 }, { &#39;scores&#39;: { &#39;$elemMatch&#39;: { &#39;score&#39;: { &#39;$gt&#39;: 85 } } } } ).pretty() Example-2: db.grades.find( { &#39;scores&#39;: { &#39;$elemMatch&#39;: { &#39;type&#39;: &#39;extra credit&#39; } } } ).pretty() " }, { "title": "3. Creating and Manipulating Documents", "url": "/posts/mongodb3/", "categories": "MongoDB, MongoDB Basics", "tags": "mongodb", "date": "2022-06-14 00:00:00 +0545", "snippet": "Inserting New DocumentsEvery MongoDB document must have unique _id value. ANd every _id field in a collection must have a unique value from the rest of the documents in the collection. Likewise, you can have a collection where each document is so distinctly different from the other that they don’t have the same shape or any field names in common.When we insert a new document, MongoDB populates the _id field with a value that is of type ObjectId. The _id doesn’t have to have the type ObjectId. It is just what is created by default to ensure unique values for each document. If you already know of unique value that you can use for each document then you can use those values in the _id field instead.Inserting One Document At A TimeWe can insert a new document using insert.For example:db.inspections.insert({ &quot;_id&quot; : ObjectId(&quot;56d61033a378eccde8a8354f&quot;), &quot;id&quot; : &quot;10021-2015-ENFO&quot;, &quot;certificate_number&quot; : 9278806, &quot;business_name&quot; : &quot;ATLIXCO DELI GROCERY INC.&quot;, &quot;date&quot; : &quot;Feb 20 2015&quot;, &quot;result&quot; : &quot;No Violation Issued&quot;, &quot;sector&quot; : &quot;Cigarette Retail Dealer - 127&quot;, &quot;address&quot; : { &quot;city&quot; : &quot;RIDGEWOOD&quot;, &quot;zip&quot; : 11385, &quot;street&quot; : &quot;MENAHAN ST&quot;, &quot;number&quot; : 1712 }}) The above statemtnt will insert a new document in the inspections collections if the _id is a unique value and there are no errors.Inserting Multiple Documents At A Timedb.inspections.insert([ { &quot;test&quot;: 1 }, { &quot;test&quot;: 2 }, { &quot;test&quot;: 3 } ])Updating Documents updateOne(): Used to update only one document at one time. If there are multiple documents that match a given criteria, then only one of them will be updated, whichever one the operation finds first. For example: - db.zips.updateOne( { &#39;zip&#39;: &#39;12534&#39; }, { &#39;$set&#39;: { &#39;pop&#39;: 17630 } }) Here, $set is the update operator. In the above query, we are looking to set the pop field to 17630 in the document where zip = 12534. The $set update operator sets the field value to a new specified value. $set syntax also allows us to set multiple fields at the same time by listing the fields and their specified value separated by a comma. Syntax: {&#39;$set&#39;: { &#39;pop&#39;: 17630, &#39;&amp;lt;field2&amp;gt;&#39;: &amp;lt;new value&amp;gt;, ... }} updateMany(): Used to update multiple documents at one time. It updates all the documents that match the given criteria. For example: - db.zips.updateMany( {&#39;city&#39;: &#39;HUDSON&#39;}, {&#39;$inc&#39;: {&#39;pop&#39;: 10} } ) Here, $inc is the update operator. In the above query, we are looking to increment the pop field by 10 in every document which lists Hudson as the city. $inc syntax also allows us to update multiple fields at the same time by listing the fields and their increment value separated by a comma. Syntax: {&#39;$inc&#39;: { &#39;pop&#39;: 10, &#39;&amp;lt;field2&amp;gt;&#39;: &amp;lt;increment value&amp;gt;, ... }} Just like the $set and $incoperator, the $push is also an Update operator. To add an element to an array field, one of the options is to use the $push oeprator which has the following syntax: Syntax: {&#39;$push&#39;: { &amp;lt;field1&amp;gt;: &amp;lt;value1&amp;gt;, .... }} Just like with the set operator, if the field that you specify doesn’t exist in the document then $push will add an array field to the document with a specified value.Example:db.grades.updateOne( { &quot;student_id&quot;: 250, &quot;class_id&quot;: 339 }, { &quot;$push&quot;: { &quot;scores&quot;: { &quot;type&quot;: &quot;extra credit&quot;, &quot;score&quot;: 100 } } }) The above query modifies the scores array of the student with student_id=250 by adding another element to it. In this case, the added element is a document with two field value pairs, type-extra credit and score-100.Deleting Documents and Collections deleteOne(): Deletes one document at a time. deleteMany(): Deletes many documents at a time. To delete a collection use db.&amp;lt;collection&amp;gt;.drop.{. prompt-info}" }, { "title": "2. Importing, Exporting, and Querying Data", "url": "/posts/mongodb2/", "categories": "MongoDB, MongoDB Basics", "tags": "mongodb", "date": "2022-06-14 00:00:00 +0545", "snippet": "How Does MongoDB Store Data?MongoDB uses documents to store data. When we view or update documents in the MongoDB shell, you’re working in JSON which stands for ‘JavaScript Standard Object Notation’. For a document to comply with the JSON format, you need to: start and edn with curly brace {} separate each key and value with a colon : separate each key:value pair with a comma , &quot;keys&quot; must be surrounded by quotation marks &quot;&quot; Note: Keys are also known as fields in MongoDBFor example:{ &#39;_id&#39;: &#39;1&#39;, &#39;company_name&#39;: &#39;Fusemachines&#39;, &#39;date&#39;: &#39;2022-06-14&#39;, &#39;position&#39;: &#39;Software Engineer Associate Trainee&#39;, &#39;team&#39;: &#39;Data Engineering Team&#39;, &#39;address&#39;: { &#39;city&#39;: &#39;Kathmandu&#39;, &#39;zip&#39;: 44600, &#39;street&#39;: &#39;baneshword&#39; }} The above example is a valid JSON. Note: You may also have noticed the ‘address’ field which in itself contains a document as a value.Pros and Cons of JSONPros of JSON Friendly Readbale FamiliarCons of JSON JSON is a text-based format, and text parsing is very slow. JSON’s redable format is far from space-efficient, another database concern. JSON only supports a limited number of basic data types.Therefore, MongoDB Decided to address these drawbacks. If you look at your data the way it is stored in memory inside MongoDB, then you’ll what is called BSON format-Binary JSON.What is BSON?BSON simply stands for “Binary JSON,” and that’s exactly what it was invented to be. BSON’s binary structure encodes type and length information, which allows it to be parsed much more quickly.Since its initial formulation, BSON has been extended to add some optional non-JSON-native data types, like dates and binary data, without which MongoDB would have been missing some valuable support.Languages that support any kind of complex mathematics typically have different sized integers (ints vs longs) or various levels of decimal precision (float, double, decimal128, etc.).Not only is it helpful to be able to represent those distinctions in data stored in MongoDB, it also allows for comparisons and calculations to happen directly on data in ways that simplify consuming application code.Why BSON?In order to make MongoDB JSON-first, but still high-performance and general-purpose, the Binary JSON (BSON) was invented to bridge the gap between binary representation and JSON format. IT is optimized for speed, space and flexibility. The goal was to achieve high performance and general purpose focus. Note: MongoDB stores data in BSON both internally and over the network. But that doesn’t mean you can’t think of MongoDB as a JSON Databse. Anything that can be natively stored in MongoDB and retrieved just as easily in JSON. BSON provides additional speed and flexibility, which is important to keep in mind when working with MongoDB.Unlike systems that simply store JSON as string-encoded values, or binary-encoded blobs, MongoDB uses BSON to offer the industry’s most powerful indexing and querying features on top of the web’s most usable data format.For example, MongoDB allows developers to query and manipulate objects by specific keys inside the JSON/BSON document, even in nested documents many layers deep into a record, and create high performance indexes on those same keys and values.Importing and Exporting DataWe know, Data in MongoDB is stored in BSON format but viewed in JSON format. BSON is great but isn’t really human readable. If you’re just looking to store the data and then may be transfer it to a different system or cluster then your best bet would be to export in BSON. It’s lighter and faster. However, If I plan on viewing this data and reading through it locally after I export it, then a human redable JSON is a better choice.Here,are four commands, two of which is used for importing and exporting data in JSON and two that is used for importing and exporting data in BSON. JSON BSON mongoimport mongorestore mongoexport mongodump Exportmongodump --uri &quot;&amp;lt;Atlas Cluster URI&amp;gt;&quot;It allows to export the data in BSON format.mongoexport --uri &quot;&amp;lt;Atlas Cluster URI&amp;gt;&quot; --collection=&amp;lt;collection name&amp;gt; --out=&amp;lt;filename&amp;gt;.jsonIt allows to export the data in JSON format.Importmongorestore --uri &quot;&amp;lt;Atlas Cluster URI&amp;gt;&quot; -- drop dumpIt allows to import the data in BSON format.mongoimport --uri &quot;&amp;lt;Atlas Cluster URI&amp;gt;&quot; --drop=&amp;lt;filename&amp;gt;.jsonIt allows to import the data in JSON format.Querying in Mongo Shell Use show dbs and show collections for viewing available namespaces Use use &amp;lt;database_name&amp;gt; for connecting to a database find() returns a cursor with documents that match the find query count() returns the number of documents that match the find query pretty() formats the documents in the cursor" }, { "title": "1. What is MongoDB?", "url": "/posts/mongodb1/", "categories": "MongoDB, MongoDB Basics", "tags": "mongodb", "date": "2022-06-14 00:00:00 +0545", "snippet": "What is a MongoDB Database?A database, meaning a structured way to store and access data. More specifically, it is a NoSQL database. NoSQL databases mean that it doesn’t use the traditional approach of storing data in tables such as in SQL databases. This means that you’re storing your data in an organized way, but not in rows and columns like in a table. An example of NoSQL database can be anything from a library card catalog, to a more sophisticated store like MongoDB.MongoDB is a NoSQL document database which means that inside MongoDB data are stored inside documents. These documents are in turn stored in what we call collections of documents. MongoDB uses a structured way to store and access data.That is why MongoDB is categorized as a NoSQL document database.What is a Document?A document is a way to organize and store data as a set of field-value pairs.Example:{ &amp;lt;field&amp;gt; : &amp;lt;value&amp;gt;, &amp;lt;field&amp;gt; : &amp;lt;value&amp;gt;}Just like this, where the field is a unique identifier for some data point, and the value is data related to a given identifier.{ &#39;name&#39;: &#39;Babin&#39;, &#39;title&#39;: &#39;Software Engineer Associate Trainee&#39;, &#39;company&#39;: &#39;Fusemachines&#39;}In the above example, there is a field name, and the value is Babin.A collection would contain many such documents. A collection is an organized store of documents in MongoDB, usually with common fields between documents.And a database would contain multiple collections.What is MongoDB Atlas?The Atlas Cloud database is a fully managed database built for wide range of applications with MongoDB at its core. Atlas helps you visualize, analyze, export, and build applications with your data.It has many different services and tools availbale within it which uses MongoDB databse for data storage and retrieval.Atlas users can deploy clusters-which are groups of servers that store your data. These servers are configure in what we call a replicate set-which is a set of few connected MongoDB instances that store a same data. An instance is a single machine locally or in the cloud, running a certain software. This set up ensure that if something happens to one of the machines in the replica set, the data will remain intact and available for use by the applicaiton by the remaining working members of the replica set. So everytime you make changes to a document or a collection, redundant copies of that data are stored within the replica set." }, { "title": "SQL Cheatsheet", "url": "/posts/sql_cheatsheet/", "categories": "SQL, SQL CheatSheet", "tags": "sql, t-sql, cheat-sheet", "date": "2022-06-13 00:00:00 +0545", "snippet": "" }, { "title": "Pulling data from databases", "url": "/posts/shell3/", "categories": "Linux, Data Processing in Shell", "tags": "curl, wget", "date": "2022-05-20 00:00:00 +0545", "snippet": " sql2csv : documentation sql2csv: executes an SQL query on a large variety of SQL databases (e.g. MS SQL, MySQL, Oracle, PostgreSQL, Sqlite) outputs the result to a CSV file Documentation sql2csv -h sql2csv: querying against the database Sample Syntax: sql2csv --db &#39;sqlite:///SpotifyDatabase.db&#39; \\ --query &#39;SELECT * FROM Spotify_Popularity&#39; \\ &amp;gt; Spotify_Popularity.csv Establishing database connection: --db is followed by the database connection string SQLITE: starts with sqlite:/// and ends with .db Postgres &amp;amp; MySQL starts with postgres:/// or mysql:/// and with no .db Querying against the database: --query is followed by the SQL query string Use SQL syntax compatible with the database Write query in one line with no line breaks Saving the output: &amp;gt;: re-directs output to new local CSV file Otherwise, will only print query results to console Manipulating data using SQL syntax csvsql: documentation csvsql: applies SQL statements to one or more CSV files creates an in-memory SQL database that temporarily hosts the file being processed suitable for small to medium files only Documentation: csvsql -h csvsql: applying SQL to a local CSV file Sample Syntax: csvsql --query &quot;SELECT * FROM Spotify_MusicAtributes LIMIT 1&quot; Spotify_MusicAttributes.csv csvsql --query &quot;SELECT * FROM Spotify_MusicAtributes LIMIT 1&quot; Spotify_MusicAttributes.csv | csvlook csvsql --query &quot;SELECT * FROM Spotify_MusicAtributes LIMIT 1&quot; Spotify_MusicAttributes.csv &amp;gt; OneSongFile.csv csvsql: joining CSVs using SQL syntax Sample Syntax: csvsql --query &quot;SELECT * FROM file_a INNER JOIN file_b...&quot; file_a.csv file_b.csv Note: SQL Query must be written in one line, no breaks Indicate CSV files in order of appearence in SQL csvsql: documentation csvsql: execute SQL statements directly on a database supports both creating tables and inserting data. More option arguements: --insert --db --no-inference &amp;amp; --no-constraints csvsql: pushing data back to database Sample Syntax: csvsql --db &quot;sqlite:///SpotifyDatabase.db&quot; \\ --insert Spotify_MusicAttributes.csv Note: Line break is used to keep code clean and readable Use three forward slashes to initiate database name End with file extension .db for SQLITE database csvsql --no-inference --no-constraints \\ --db &quot;sqlite:///SpotifyDatabase.sb&quot; \\ --insert Spotify_MusicAttributes.csv " }, { "title": "Getting started with csvkit", "url": "/posts/shell2/", "categories": "Linux, Data Processing in Shell", "tags": "curl, wget", "date": "2022-05-20 00:00:00 +0545", "snippet": " What is csvkit? csvkit: is a suite of command-line tools is developed in Python by Wireservice offers data processing and cleaning capabilities on CSV files has data capabilities that rival Python, R, and SQL for documentation visit link: Documentation csvkit installation Install csvkit using Python package manager pip: pip install csvkit Upgrade csvkit to the latest version: pip install --upgrade csvkit in2csv: converting files to csv Syntax: in2csv SpotifyData.xlsx &amp;gt; SpotifyData.csv Prints the first sheet in Excel to console and does not save in2csv SpotifyData.xlsx &amp;gt; redirects the output and saves it as a new file SpotifyData.csv &amp;gt; SpotifyData.csv Use --names or -n option to print all sheet names in SpotifyData.xlsx in2csv -n SpotifyData.xlsx Use --sheet option followed by the sheet Worksheet1_Popularity to be converted. in2csv SpotifyData.xlsx --sheet &quot;Worksheet1_Popularity&quot; &amp;gt; Spotify_Popularity.csv csvlook: data preview on the command line csvlook: renders a cSV to the command line in a Markdown-compatible, fixed-width format Documentation: csvlook -h Syntax: csvlook Spotify_Popularity.csv csvstat: descriptive stats on CSV data files csvstat: prints descriptive summary statistison all columns in CSV (e.g. mean, median, unique valuescoutns) Documentation: csvstat - h Syntax: csvstat Spotify_Popularity.csv Filtering data using csvkit What does it mean to filter data? We can create a subset of the original data file by: Filtering the data by column Filtering the data by row csvcut: filters data using column name or position csvgrep: filters data by row value through exact match, pattern matching, or even regex csvcut: filtering data by column csvcut: fitlers and truncates CSV files by column name or column position Documentation: csvcut -h Use --names or -n option to print all column names in Spotify_MusicAttributes.csv. csvcut -n Spotify_MusicAttributes.csv To return the first column in the data, by position: csvcut -c 1 Spotify_MusicAttributes.csv To return only the first column in the data, by name: csvcut -c &quot;track_id&quot; Spotify_MusicAttributes.csv To return the second and third column in the data, by position: csvcut -c 2,3 Spotify_MusicAttributes.csv To return the second and third column in the data, by name: csvcut -c &quot;danceability&quot;, &quot;duration_ms&quot; Spotify_MusicAttributes.csv csvgrep: filtering data by row value csvgrep: fitlers by row using exact match or regex fuzzy matching must be paired with one of these options: -m: followed by the exact row value to filter -r: followed with a regex pattern -f: followed by the path to a file Documentation: csvgrep -h Example: Find in Spotify_Popularity.csv where track_id = 5RCPsfzmEpTXMCTNk7wEfQ csvgrep -c &#39;track_id` -m 5RCPsfzmEpTXMCTNk7wEfQ Spotify_Popularity.csv csvgrep -c 1 -m 5RCPsfzmEpTXMCTNk7wEfQ Spotify_Popularity.csv Stacking data and chaining commands with csvkit csvstack: stacking multiple CSV fies csvstak: stacks up the rows from two or more CSV files Documentation: csvstack -h Stack two similar files Spotify_Rank6.csv and Spotify_Rank7.csv into one file. Preview the data to check schema: csvlook Spotify_Rank6.csv Syntax: csvstack Spotify_Rank6.csv Spotify_Rank7.csv &amp;gt; Spotify_AllRanks.csv csvlook Spotify_AllRanks.csv csvstack -g &quot;Rank6&quot;, &quot;Rank7&quot; -n &quot;source&quot;\\ Spotify_Rank6.csv Spotify_Rank7.csv &amp;gt; Spotify_AllRanks.csv csvlook Spotify_AllRanks.csv chaining command-line commands ; links commands together and runs sequentially csvlook Spotify_All.csv; csvstat SpotifyData_All.csv &amp;amp;&amp;amp; links commands togethers,but only runs the 2nd command if the 1st succeeds csvlook SpotifyData_All.csv &amp;amp;&amp;amp; csvstat SpotifyData_All_data.csv &amp;gt; re-directs the output from the 1st command to the location indicated as the 2nd in2csv SpotifyData.xlsx &amp;gt; SpotifyData.csv | uses the output of the 1st command as input to the 2nd Example: Output of csvcut is not well informed: csvcut -c &quot;track_id&quot;, &quot;danceability&quot; Spotify_MusicAttributes.csv Re-format csvcut’s output by piping the output as input to csvlool: csvcut -c &#39;track_id&#39;, &#39;danceablity&#39; Spotify_Popularity.csv | csvlook " }, { "title": "Downloading data using curl", "url": "/posts/shell1/", "categories": "Linux, Data Processing in Shell", "tags": "curl, wget", "date": "2022-05-20 00:00:00 +0545", "snippet": " What is curl? curl: is short for Client for URLs is a Unix command line tool transfer data to and from a server is used to download data from HTTP(S) sites and FTP servers Checking curl installation Check curl installation: man curl If curl has not been installed, you will see: curl command not found. Learning curl Syntax Basic curl syntax: curl [option flags] [URL] URL is required.curl also supports HTTP, HTTPS, FTP, and SFTP.For a full list of the options available: curl --help Downloading a Single File Example: A single file is stored at: https://websitename.com/datafilename.txt Use the optional flag -0 to save the file with its original name: curl -0 https://websitename.com/datafilename.txt To rename the file, use the lower case -o + new filename: curl -o renamedatafilename.txt https://websitename.com/datafilename.txt Downloading Multiple Files using Wildcards Oftentimes, a server will host multiple data files, with similar filenames: https://websitename.com/datafilename001.txt https://websitename.com/datafilename002.txt . . . https://websitename.com/datafilename100.txt Using Wildcards(*) Download every file hosted on https://websitename.com/ that starts with datafilename and end in .txt: curl -O https://websitename.com/datafilename*.txt Downloading Multiple Files using Globbing Parset Continuing with the previous example: https://websitename.com/datafilename001.txt https://websitename.com/datafilename002.txt ... https://websitename.com/datafilename100.txt Using Globbing Parser The following will download every file sequentially starting with datafilename001.txt and ending with datafilename100.txt. curl -O https://websitename.com/datafilename[001-100].txt Increment through the files and download every Nth file (e.g.datafilename001.txt, datafilename020.txt,…, datafilename100.txt) curl -0 https://websitename.com/datafilename[001-100:10].txt Preemptive Troubleshooting curl has two particularly useful option flags in case of timeouts during download: -L : Redirects the HTTP URL if a 300 error code occurs. -C : Resumes a previous file transfer if it times out before completion. Putting everythin together: curl -L -O -C https://websitename.com/datafilename[001-100].txt All option flags come before the URL Order of the flags does not matter. (e.g: -L -C -O is fine) Downloading data using Wget What is Wget? Wget: Derives its name from World Wide Web and get Native to Linux but compatible for all operating systems used to download data from HTTP(s) and FTP better than curl at downloading multiple files recursively Learning Wget Syntax Basic Wget syntax: wget [option flags][URL] URL is required Wget also supports HTTP, HTTPS, FTP, and SFTP. For a full list of the option flags available, see: wget -- help Downloading a Single File Option flags unique to Wget: -b: Go to background immediately after startup -q: Turn off the Wget output -c: Resume broken download (i.e continue getting a partially-downloaded file) wget -bqc https://websitename.com/datafilename.txt Advanced downloading using Wget Multiple file downloading with Wget Save a list of file locations in a text file. cat url_list.txt Returns: https://websitename.com/datafilename001.txt https://websitename.com/datafilename002.txt ... Download from the URL locations stored within the file url_list.txt using -i. wget -i url_list.txt Setting download constraints for large files Set upper download bandwidth limit (by defaul in bytes per with second) with --limit-rate. Syntax: wget --limit-rate={rate}k {file_location} Example: wget --limit-rate=200k -i url_list.txt Curl versus Wget curl advantages: Can be used for downloading and uploading files from 20+ protocols. Easier to install across all operating systems. Wget advantages: Has many built-in functionalities for handling multiple file downloads. Can handle various file formats for download (e.g. file directory, HTML page) " }, { "title": "Basic functions in Bash", "url": "/posts/bash4/", "categories": "Linux, Bash Scripting", "tags": "curl, wget", "date": "2022-05-20 00:00:00 +0545", "snippet": "" }, { "title": "IF statements", "url": "/posts/bash3/", "categories": "Linux, Bash Scripting", "tags": "curl, wget", "date": "2022-05-20 00:00:00 +0545", "snippet": "" }, { "title": "Basic variables in Bash", "url": "/posts/bash2/", "categories": "Linux, Bash Scripting", "tags": "curl, wget", "date": "2022-05-20 00:00:00 +0545", "snippet": " Assigning Variables Similar to other languages, you can assign variables with the equal notation. var1=&#39;Moon; Then reference with $ notation. echo $var Result: Moon Assigning String Variables Name your variable as you like (something sensible!): firstname=&#39;Cynthia&#39; lastname=&#39;Liu&#39; echo &#39;Hi there&#39; $firstname $lastname Result: Hi there Cynthia Liu Missing the $ notation If you miss the $ notation - it isn’t a variable firstname=&#39;Cynthia&#39; lastname=&#39;Liu&#39; echo &#39;Hi there &#39; firstname lastname Result: Hi there firstname lastname (Not) assigning variables Bash is not very forgiving about spaces in variable creation. Beware of adding spaces! var1 = &quot;Moon&quot; echo $var1 script.sh: line 3: var1: command not found Don’t add spaces before and after the = sign. Single, double, backticks In Bash, using different quotation marks can mean different things. Both when creating variables and printing. Single quotes (&#39;sometext&#39;)=Shell interprets what is between literally Double quotes (&quot;sometext&quot;)=Shell interprets literally except using $ and backticks The last way creates a ‘shell-within-a-shell’, outlined below. Useful for calling command-line programs. This is done with backticks. Backticks(``) = Shell runs the command and captures STDOUT back into a variable. Different variable creation Let’s see the effect of different types of variable creation now_var=&#39;NOW&#39; now_var_singlequote=&#39;$now_var&#39; echo $now_var_singlequote Returns: $now_var now_var_doublequote=&quot;$now_var&quot; echo $now_var_doublequote Returns: NOW The date program The Date program will be useful for demonstrating backticks Normal output of this program: date Mon 2 Dec 2019 14:07:10 AEDT Shell within a shell Let’s use the shell-within-a-shell now: rightnow_doublequote=&quot;The date is `date`.&quot; echo $rightnow_doublequote Returns: The date is Mon 2 Dec 2019 14:13:35 AEDT. The date program was called, output captured and combined in-line with the echo call. We used a shell within a shell Parentheses vs backticks There is an equivalent to backtick notation: rightnow_doublequotes=&quot;The date is `date`.&quot; rightnow_parentheses=&quot;The date is $(date).&quot; echo $rightnow_doublequotes echo $rightnow_parentheses Returns: The date is Mon 2 Dec 2019 14:54:34 AEDT. The date is Mon 2 Dec 2019 14:54:34 AEDT. Both work the same though using blackticks is older. Parentheses is used more in modern applications. Numeric variables in Bash Numbers in other languages Numbers are not built in natively to the shell like most REPLs (console) such as R and Python In Python or R you may do: &amp;gt;&amp;gt;&amp;gt; 1 + 4 Returns: 5 Numbers in the shell Numbers are nto natively supported: (In the terminal) 1 + 4 bash: 1: command not found Introducing expr expr is a useful utility program (just like cat or grep) This will now work (in the terminal): expr 1 + 4 Returns: 5 expr limitations expr cannot natively handle decimal places; (In terminal) expr 1 + 2.5 expr: not a decimal number: ‘2.5’ We can get past this limiation using bc Getting numbers to bc Using bc without opening the calculator is possible by piping: echo &quot;5 + 7.5&quot; | bc 12.5 bc scale argument bc also has a scale argument for how many decimal places. echo &quot;10/3&quot; | bc 3 echo &quot;scale=3; 10/3&quot; | bc Note the use of ; to separate ‘lines’ in terminal3.333 Numbers in Bash Scripts We can assign numeric variables just like string variables: dog_name=&#39;Roger&#39; dog_age=6 echo &quot;My dog&#39;s name is $dog_name and he is $dog_age years old&quot; Beware that dog_age=&quot;6&quot; will work, but makes it a string! My dog’s name is Roger and he is 6 years old Double bracket notation A variant on single bracket variable notation for numeric variables: expr 5 + 7 echo $((5+7)) Returns: 12 12 Beawre this method uses expr, not bc Shell within a shell revisited Very useful for numeric variables: model1=87.65 model2=89.20 echo &quot;The total score is $(echo &quot;$model1 + $modle2&quot; | bc)&quot; echo &quot;The average score is $(echo &quot;($model1 + $modle2) / 2&quot; | bc)&quot; Returns The total score is 176.85 The average score is 88 Arrays in Bash What is an array? Two types of arrays in Bash: An array ‘Normal’ numerical-indexed structure. Called a ‘list’ in Python or ‘vector’ in R. In Python: my_list=[1,3,2,4] In R: my_vector &amp;lt;- c(1,3,2,4)` " }, { "title": "Introduction to Bash Scripting", "url": "/posts/bash1/", "categories": "Linux, Bash Scripting", "tags": "curl, wget", "date": "2022-05-20 00:00:00 +0545", "snippet": " Why Bash Scripting? (Bash) Bash stands for ‘Bourne Again Shell’ Developed in the 80’s but a very popular shell today. Default in many Unix systems, Macs AWS, Google, Microsoft all have CLI’s to their products Bash Scripting helps in ease of execution of shell commands (no need to copy-paste every time!) Powerful programming constructs Shell Commmands Refresher Some important shell commands: (e)grep filters input based on regex pattern matching catconcatenates file contents line-by-line tail \\ head give only the last -n (a flag) lines wc does a word or line count (with flags -w -l) sed does pattern-matched stringn replacement A reminder of REGEX ‘Regex’ or regular expressions are a vital skill for Bash scripting. You will often need to filter files, data within files, match arguements and a variety of otheruses. It is worth revisting this. To test your regex you can use helpful sites like regex101.com Some Shell Practice Let’s revise some shell commands in an example. Consider a text fiel fruits.txt with 3 lines of data: banana apple carrot If we ran grep `a` fruits.txt we would return: banana apple carrot But fi we ran grep `p` fruits.txt we would return: apple Recall that square parentheses are a matching set such as [eyfc]. Using ^ makes this an inverset set (not these letters/numbers) So we could run grep &#39;[pc]` fruits.txt we would return: apple carrot You have likely used ‘pipes’ before in terminal. If we had many many fruits in our life we could use sort | uniq -c The first will sort alphabetically, the second will do a count If we wanted the top n fruits we could then pipe to wc-l and use head cat new_fruits.txt | sort | uniq -c | head -n 3 14 apple 13 banana 12 carrot First Bash Script Bash Script Anatomy A Bash Script has a few key defining features: It usually begins with #!/usr/bash (on itsown line) So your interpreter knows it is a Bash script and to use Bash located in /usr/bash This could be a different path if you installed Bash somewhere else such as /bin/bash/ (type which bash to check) Middle lines contain code This may be line-by-line commands or programming constructs Bash script anatomry To save and run: It has a file extension .sh Technically not needed if first line has the she-bang and path to Bash (#!/usr/bash), but a convention Can be run in the termina using bash script_name.sh Or if you have mentioned first line (#!/usr/bash) you can simply run using ./script_name.sh Bash Script Example An example of a full script (called eg.sh) is: #!/usr/bash echo &quot;Hello world&quot; echo &quot;Goodbye world&quot; Could be run with the command ./eg.sh and woudl output: Hello world Goodbye world Bash and Shell Commands Each line of your Bash Script can be a shell command. Therefore, you can also include pipes in your Bash scripts. Consider a text file (animals.txt) magpie, bird emu, bird kangaroo, marsupial wallaby, marsupial shark, fish We want to count animals in each group In shell you could write a chained command in the terminal. Let’s instead put that into a script (group.sh) #!/usr/bash cat animals.txt | cut -d &quot; &quot; -f 2 | sort | uniq -c Now (after saving the script) runnign bash group.sh causes: 2 bird 1 fish 2 marsupial Standard streams &amp;amp; arguments STDIN-STDOUT-STDERR In Bash scripting, there are three ‘streams’ for your program: STDIN (standard input). A stream of data into the program STDOUT (standard output). A stream of data out of the program STDERR (standard error). Errors in your program By default, these streams will come from and write out to the terminal. Though you may see 2&amp;gt; /dev/null in script calls; redirecting STDERR to be deleted. (1&amp;gt; /dev/null) would be STDOUT STDIN example Consider a text file (sports.txt) with 3 lines of data. football basketball swiming The cat sport.txt &amp;gt; new_sports.txt command is an example of taking data from the file and writing STDOUT to anew file. See what happends if you cat new_sports.txt` football basketball swimming STDIN v ARGV A key concept in Bash scripting is arguments Bash scripts can take arguments to be used inside by adding aspace after the script execution call ARGV is the array of all the arguments given to the program Each argument can be accessed via the $ notation. The first as $1, the second as $2 etc. $@ and $* give all the arguments in ARGV $# gives the length (number) of arguments ARGV example Consider an example script (args.sh): #!/usr/bash echo $1 echo $2 echo $@ echo &quot;There are &quot; $# &quot;arguments&quot; Running the ARGV example Now running bash args.sh one two three four five one two one two three four five There are 5 arugments " }, { "title": "Deleting and Altering Triggers", "url": "/posts/9sql4/", "categories": "SQL, 9. Building & Optmizing Triggers in SQL Server", "tags": "sql, t-sql, deleting and altering triggers, drop trigger, disabling triggers, enabling triggers, altering triggers, troubleshooting triggers", "date": "2022-05-20 00:00:00 +0545", "snippet": " Deleting Table and View Triggers DROP TRIGGER PreventNewDiscounts; Deleting Database Triggers DROP TRIGGER PreventNewDiscounts; DROP TRIGGER PreventViewsModifications ON DATABASE: Deleting Server Triggers DROP TRIGGER PreventNewDiscounts; DROP TRIGGER PreventViewsModifications ON DATABASE: DROP TRIGGER DisallowLinkedServers ON ALL SERVER; Disabling Triggers DISABLE TRIGGER PreventNewDiscounts ON Discounts; DISABLE TRIGGER PreventViewsModifications ON DATABASE; DISABLE TRIGGER DisallowLinkedServers ON ALL SERVER; Enabling Triggers ENABLE TRIGGER PreventNewDiscounts ON Discounts; ENABLE TRIGGER PreventViewsModifications ON DATABASE; ENABLE TRIGGER DisallowLinkedServers ON ALL SERVER; Altering Triggers CREATE TRIGGER PreventDiscountsDelete ON Discounts INSTEAD OF DELETE AS PRINT &#39;You are not allowed to data from the Discounts table.&#39;; DROP TRIGGER PrevetnDiscountsDelete; CREATE TRIGGER PreventDiscountsDelete ON Discounts INSTEAD OF DELETE AS PRINT &quot;You are note allowed to remove data from the Discounts table.&quot; ALTER TRIGGER PreventDiscountsDelete ON Discounts INSTEAD OF DELETE AS PRINT &quot;You are not allowed to remove data from the Discounts table.&#39;; Trigger Management Getting info from sys.triggers SELECT * FROM sys.triggers; name trigger name object_id unique identifier of the trigger parent_class trigger type as integer parent_class_desc trigger type as text parent_id unique identifier of the parent object create_date date of creation modify_date date of last modification is_disabled current state is_instead_of_trigger INSTEAD OF or AFTER trigger Getting info from sys.server_triggers SELECT * FROM sys.server_triggers; Getting info from sys.trigger_events SELECT * FROM sys.trigger_events; object_id unique identifier of the trigger type event type as integer type_desc event type as text event_group_type event group type as integer event_group_type_desc event group type as text Getting info from sys.server_trigger_events SELECT * FROM sys.server_trigger_events; Troubleshooting triggers Tracking Trigger Executinos (system view) SELECT * FROM sys.dm_exec_trigger_stats; Tracking Trigger Executions (custom solution) CREATE TRIGGER PreventOrdersUpdate ON Orders INSTEAD OF UPDATE AS RAISERROR(&#39;Updates on &quot;Orders&quot; table are not permitted. Place a new order to add new products.&#39;, 16, 1); Identifying triggers attached to a table SELECT name AS TableName, object_id AS TableId FROM sys.objects WHERE name = &quot;Products&quot;; SELECT o.name AS TableName, o.object_id AS TableID, t.name AS TriggerName, t.object_id AS TriggerID, t.is_disabled AS IsDisabled, t.is_instead_of_trigger AS IsInsteadOf FROM sys.objects AS o INNER JOIN sys.triggers AS t ON t.parent_id = o.object_id WHERE o.name = &#39;Products&#39;; Identifying events capable of firing a trigger SELECT o.name AS TableName, o.object_id AS TableID, t.name AS TriggerName, t.object_id AS TriggerID, t.is_disabled AS IsDisabled, t.is_instead_of_trigger AS IsInsteadOf, te.type_desc AS FiringEvent FROM sys.objects AS o INNER JOIN sys.triggers AS t ON t.parent_id = o.object_id INNER JOIN sys.trigger_events AS te ON t.object_id = te.object_id WHERE o.name = &#39;Products&#39;; Viewing the trigger definitions SELECT o.name AS TableName, o.object_id AS TableID, t.name AS TriggerName, t.object_id AS TriggerID, t.is_disabled AS IsDisabled, t.is_instead_of_trigger AS IsInsteadOf, te.type_desc AS FiringEvent, OBJECT_DEFINITION(t.object_id) AS TriggerDefinition FROM sys.objects AS o INNER JOIN sys.triggers AS t ON t.parent_id = o.object_id INNER JOIN sys.trigger_events AS te ON t.object_id = te.object_id WHERE o.name = &#39;Products&#39;; " }, { "title": "Known Limitations of Triggers", "url": "/posts/9sql3/", "categories": "SQL, 9. Building & Optmizing Triggers in SQL Server", "tags": "sql, t-sql, after, instead of", "date": "2022-05-20 00:00:00 +0545", "snippet": " Advantages of Triggers Used for database integrity Enforece business rules directly in the database Control on which statements are allowed in a database Implementatino of complex business logic triggered by a single event Simple way to audit databases and user actions Disadvantages of Triggers Difficult to view and detect Invisible to client applications or when debugging code Hard to follow their logic when troubleshooting Can become an overhead on the server and make it run slower Finding server-level triggers SELECT * FROM sys.server_triggers; Finding database and table triggeers SELECT * From sys.triggers; Viewing a trigger definition (option 1) CREATE TRIGGER PreventOrdersUpdate ON Orders INSTEAD OF UPDATE AS RAISERROR (&#39;Updates on &quot;Orders&quot; table are not permitted. Place a new order to add new products.&#39;, 16, 1); Viewing a trigger definition (option 2) SELECT definition FROM sys.sql_modules WHERE object_id = OBJECT_ID (&#39;PreventOrdersUpdate&#39;); Viewing a trigger definition (option 3) SELECT OBJECT_DEFINITION (OBJECT_ID (&#39;PreventOrdersUpdate&#39;)); Viewing a trigger definition (option 4) EXECUTE sp_helptext @objanem = &#39;PreventOrderUpdate&#39;; Use Cases For After Triggers (DML) Keeping a history of row changes CREATE TRIGGER CopyCustomersToHistory ON Customers AFTER INSERT, UPDATE AS INSERT INTO CustomersHistory (Customer, ContractId, Address, PhoneNo) SELECT Customer, ContractID, Address, PhoneNo, GETDATE() FROM inserted; Table auditing using triggers CREATE TRIGGER OrdersAudit ON Orders AFTER INSERT, UPDATE, DELETE AS DECLARE @Insert BIT = 0 , @Delete BIT = 0; IF EXISTS (SELECT * FROM inserted) SET @Insert = 1; IF EXISTS (SELECT * FROM deleted) SET @Delete = 1; INSERT INTO [TablesAudit] ([TableName], [EventType], [UserAccount], [EventDate]) SELECT &#39;Orders&#39; AS [TableName] ,CASE WHEN @Insert = 1 AND @Delete = 0 THEN &#39;INSERT&#39; WHEN @Insert = 1 AND @Delete = 1 THEN &#39;UPDATE&#39; WHEN @Insert = 0 AND @Delete = 1 THEN &#39;DELETE&#39; END AS [Event] ,ORIGINAL_LOGIN() ,GETDATE(); Notifying Users CREATE TRIGGER NewOrderNotification ON Orders AFTER INSERT AS EXECUTE SendNotification @RecipientEmail = &#39;sales@freshfruit.com&#39;, @EmailSubject = &quot;New order place&quot;, @EmailBody = &#39;A new order was just placed.&#39;; Use cases for INSTEAD OF triggers (DML) General use of INSTEAD OF triggers Prevent operations from happening Control database statements Enforce data integrity Triggers that prevent changes CREATE TRIGGER PreventProductChanges ON Products INSTEAD OF UPDATE AS RAISERROR (&#39;Updates of products are not permitted. Contact the database administrator if a change is needed.&#39;, 16, 1); Triggers that prevent and notify ```sql CREATE TRIGGER PreventCustomersRemoval ON Customers INSTEAD OF DELETE AS DECLARE @EmailBodyText NVARCHAR(50) = (SELECT ‘User “’ + ORIGINAL_LOGIN() + ‘” tried to remove a customer from the database.’); RAISERROR (‘Customer entries are not subject to removal.’, 16, 1); EXECUTE SendNotification @RecipientEmail = &#39;admin@freshfruit.com&#39; ,@EmailSubject = &#39;Suspicious database behavior&#39; ,@EmailBody = @EmailBodyText; ``` Triggers with Conditional Logic CREATE TRIGGER ConfirmStock ON Orders INSTEAD OF INSERT AS IF EXISTS (SELECT * FROM Products AS p INNER JOIN inserted AS i ON i.Product = p.Product WHERE p.Quantity &amp;lt; i.Quantity) RAISERROR (&#39;You cannot place orders when there is no product stock.&#39;, 16, 1); ELSE INSERT INTO dbo.Orders (Customer, Product, Quantity, OrderDate, TotalAmount) SELECT Customer, Product, Quantity, OrderDate, TotalAmount FROM inserted; Use Cases for DDL Triggers DDL trigger capabilities Database Auditing CREATE TRIGGER DatabaseAudit ON DATABASE FOR DDL_TABLE_VIEW_EVENTS AS INSERT INTO [DatabaseAudit] ([EventType], [Database], [Object], [UserAccount], [Query], [EventTime]) SELECT EVENTDATA().value(&#39;(/EVENT_INSTANCE/EventType)[1]&#39;, &#39;NVARCHAR(50)&#39;), EVENTDATA().value(&#39;(/EVENT_INSTANCE/DatabaseName)[1]&#39;, &#39;NVARCHAR(50)&#39;), EVENTDATA().value(&#39;(/EVENT_INSTANCE/ObjectName)[1]&#39;, &#39;NVARCHAR(100)&#39;), EVENTDATA().value(&#39;(/EVENT_INSTANCE/LoginName)[1]&#39;, &#39;NVARCHAR(100)&#39;), EVENTDATA().value(&#39;(/EVENT_INSTANCE/TSQLCommand/CommandText)[1]&#39;, &#39;NVARCHAR(MAX)&#39;), EVENTDATA().value(&#39;(/EVENT_INSTANCE/PostTime)[1]&#39;, &#39;DATETIME&#39;); Preventing Server Changes CREATE TRIGGER PreventDatabaseDelete ON ALL SERVER FOR DROP_DATABASE AS PRINT &#39;You are not allowed to remove existing databases.&#39;; ROLLBACK; " }, { "title": "AFTER triggers", "url": "/posts/9sql2/", "categories": "SQL, 9. Building & Optmizing Triggers in SQL Server", "tags": "sql, t-sql, inserted tables, deleted tables, DDL triggers, Logon triggers", "date": "2022-05-20 00:00:00 +0545", "snippet": " Definition and properties Performs a set of actions when fired The actions are performed only after the DML event is finisehd Used with INSERT, UPDATE, and DELETE statements for tables or views AFTER trigger prerequisites Table or view needed for DML statements The trigger will be attached to the same table Output from table Products used for this example: AFTER trigger definition CREATE TRIGGER TrackRetiredProducts ON Products AFTER DELETE AS INSERT INTO RetiredProducts (Product, Measure) SELECT Product, Measure FROM deleted; “inserted” and “deleted” tables Special Tables are used by DML triggers Created automatically by SQL Server Special Table INSERT UPDATE DELETE inserted new rows new rows N/A deleted N/A updated rows removed rows The complete AFTER trigger CREATE TRIGGER TrackRetiredProducts ON Products AFTER DELETE AS INSERT INTO RetiredProducts (Product, Measure) SELECT Product, Measure FROM deleted; INSTEAD OF triggers(DML) Definition and Properties Performs a set of actions when fired The actions are performed instead of the DML event The DML event does not run anymore Used with INSERT, UPDATE, and DELETE statements for table or views INSTEAD OF trigger definition CREATE TRIGGER PreventOrdersUpdate ON Orders INSTEAD OF UPDATE AS RAISERROR(&#39;Updates on &quot;Orders&quot; table are not permitted. Plae a new order to add new products&#39;, 16, 1); DDL Triggers Definition and Properties | DML triggers | DDL triggers | | events associated with DML statements INSERT, UPDATE, DELETE | events associated with DDL statements CREATE, ALTER, DROP | | used with AFTER or INSTEAD OF | only used with AFTER | | attached to tables or views | attached to database or servers | | inserted and deleted special tables | no special tables | DDl trigger definition CREATE TRIGGER TrackTableChanges ON DATABASE FOR CREATE_TABLE, ALTER_TABLE, DROP_TABLE AS INSERT INTO TablesChangeLog(EventData, ChangedBy) VALUES (EVENTDATA(), USER); Preventing the triggering events for DML triggers CREATE TRIGGER PreventTableDeletion ON DATABASE FOR DROP_TABLE AS RAISERROR(&#39;You are not allowed to remove tables from this database.&#39;, 16, 1); ROLLBACK: Logon Triggers Definition and Properties Performs a set of actions when fired The actions are performed for LOGON events After authentication phase, but before the session establishment Logon trigger definition CREATE TRIGGER LogonAUdit ON ALL SERVER WITH EXECUTE AS &#39;sa&#39; FOR LOGON AS INSERT INTO ServerLogonLog (LoginName, LoginDate, SessionId, SourceIPAddress) SELECT ORIGINAL_LOGIN(), GETDATE(), @@SPID, client_net_address FROM SYS.DM_EXEC_CONNECTIONS WHERE session_id = @@SPID; Logon trigger definition summary CREATE TRIGGER LogonAudit ON ALL SERVER WITH EXECUTE AS &#39;sa&#39; FOR LOGON AS INSERT INTO ServerLogonLog (LoginName, LoginDate, SessionID, SourceIPAddress) SELECT ORIGINAL_LOGIN(), GETDATE(), @@SPID, client_net_address FROM SYS.DM_EXEC_CONNECTIONS WHERE session_id = @@SPID; " }, { "title": "Introduction to Triggers", "url": "/posts/9sql1/", "categories": "SQL, 9. Building & Optmizing Triggers in SQL Server", "tags": "sql, t-sql, trigger, types of trigger, after, instead of, trigger alternatives, computed columns", "date": "2022-05-20 00:00:00 +0545", "snippet": " What is a trigger? Special type of stored procedure Executed when an event occurs in the database server Types of Trigger(based on T-SQL commands) Data Manipulation Language (DML) triggers INSERT, UPDATE or DELETE statements Data Definition Language (DDL) triggers CREATE, ALTER or DROP statements Logon triggers LOGON events Types of Trigger(based on behavior) AFTER trigger The original statment executes Additional statements are triggered Examples of use cases: Rebuild an index after a large insert Notify the admin when data is updated INSTEAD OF trigger The original statement is prevented from execution A replacement statement is executed instead Examples of use cases: Prevent insertions Prevent updates Prevent deletions Prevent object modifications Notify the admin Trigger definition (with AFTER) -- Create the trigger by giving it a descriptive name CREATE TRIGGER ProductsTrigger -- The trigger needs to be attached to a table ON Products -- The trigger behavior type AFTER INSERT -- THe beginning of the trigger workflow AS -- The action executed by the trigger PRINT(&#39;An insert of data was made in the Products table.&#39;); Trigger definition(with ISTEAD OF) -- Create the trigger by giving it a descriptive name CREATE TRIGGER PreventDeleteFromOrders -- The trigger needs to be attached on a table ON Orders -- The trigger behavior type INSTEAD OF DELETE -- The beginning of the trigger workflow AS -- The action executed by the trigger PRINT(&#39;You are not allowed to delete rows from the Orders table.&#39;); AFTER vs. INSTEAD OF CREATE TRIGGER MyFirstAfterTrigger ON Table1 -- Triggered after -- the firing event (UPDATE AFTER UPDATE AS {trigger_actions_section}; CREATE TRIGGER MyFirstInsteadOfTrigger ON Table2 -- Triggered instead of -- the firing event (UPDATE) INSTEAD OF UPDATE AS {trigger_actions_section}; &amp;lt;/hr&amp;gt;How DML triggers are used Why should we use DML triggers? Initiating actions when manipulating data Preventing data manipulation Tracking data or database object changes User auditing and database security Deciding between AFTER and INSTEAD Of AFTER trigger INSTEAD OF trigger Initial event fires the trigger Initial event fires the trigger Initial event executes Initial event is not executed anymore The trigger actions execute The trigger actions execute AFTER trigger usage example CREATE TRIGGER SalesNewInfoTrigger ON Sales AFTER INSERT AS EXEC sp_cleansing @Tabel = &#39;Sales&#39;; EXEC sp_generateSalesReport; EXEC sp_sendnotification; Data is inserted into a sales table Start a data cleansing procedure Generate a table report with the procedure Notify the database administrator INSTEAD OF trigger usage example CREATE TRIGGER BulbsStockTrigger ON Bulbs INSTEAD OF INSERT AS IF EXISTS (SELECT * FROM Bulbs AS b INNER JOIN inserted AS i ON b.Brand = i.Brand AND b.Model = i.Model WHERE b.Stock = 0) BEGIN UPDATE b SET b.Power = i.Power, b.Stock = i.Stock FROM Bulbs AS b INNER JOIN inserted AS i ON b.Brand = i.Brand AND b.Model = i.Modle WHERE b.Stock = 0 END ELSE INSERT INTO Bulbs SELECT * FROM inserted; The power changes for some models Update only the products with no stock Add new rows for the products with stock Trigger Alternatives Triggers vs Stored Procedures Triggers Stored procedures Fired automatically by an event Run only when called explicitly sql INSERT INTO Orders [...]; sql EXECUTE sp_DailyMaintenance; Don’t allow parameters or transactions Accept input parameters and transactions Cannot return values as output Can return values as output Used for: 1. auditing 2. integrity enforcement Used for: 1. general tasks 2. user-specific needs Triggers vs computed columns Triggers Computed Columns calculate column values calculate column values use columns from other tables for calculations use columns only from the same table for calculations INSERT or UPDATE used to calculate calculatino defined when creating the table sql [...] UPDATE SET TotalAmount = Price * Quantity [... sql [...] TotalAmount AS Price * Quantity [...] Example of a computed column CREATE TABLE [SalesWithPrice] ( [OrderID] INT IDENTITY(1,1), [Customer] NVARCHAR(50), [Product] NVARCHAR(50), [Price] DECIMAL(10,2), [Currency] NVARCHAR(3), [Quantity] INT, [OrderDate] DATE DEFAULT (GETDATE()), [TotalAmount] AS [Quantity] * [Price] ) Using a trigger as a computed column CREATE TRIGGER [SalesCalculateTotalAmount] ON [SalesWithoutPrice] AFTER INSERT AS UPDATE [sp] SET [sp].[TotalAmount] = [sp].[Quantity] * [p].[Price] FROM [SalesWithoutPrice] AS [sp] INNER JOIN [Products] AS [p] ON [sp].Product = [p].[Product] WHERE [sp].[TotalAmount] IS NULL; " }, { "title": "Case Study EDA and Imputation", "url": "/posts/8sql4/", "categories": "SQL, 8. Writing Functions & Stored Procedures in SQL Server", "tags": "sql, t-sql", "date": "2022-05-19 00:00:00 +0545", "snippet": " Data Imputation Divide by zero error when calculating Avg Fare/TripDistance EDA uncovers hundreds of TaxiRide trip records with Trip Distance = 0 Data Imputation methods to resolve Mean Hot Deck Omission Mean Imputation Replace missing value with mean Doesn’t change the mean value Increases correlations with other columns CREATE PROCEDURE dbo.ImputeMean AS BEGIN DECLARE @AvgTripDuration AS float SELECT @AvgTripDuration = AVG(Duration) FROM CapitalBikeShare WHERE Duration &amp;gt; 0 UPDATE CapitalBikeShare SET Duration = @AvgTripDuration WHERE Duration = 0 END; Hot Deck Imputation Missing value set to randomly selected value TABLESAMPLE clause of FROM clause CREATE FUNCTION dbo.GetDurHotDeck() RETURNS decimal(18,4) AS BEGIN RETURN (SELECT TOP 1 Duration FROM CapitalBikeShare TABLESAMPLE(1000 rows) WHERE Duration &amp;gt; 0) END SELECT StartDate, &quot;TripDuration&quot; = CASE WHEN Duration &amp;gt; 0 THEN Duration ELSE dbo.GetDurHotDeck() END FRom CapitalBikeShare; Case Study of UDFs Conversion UDFs CREATE FUNCTION dbo.ConvertMileToMeter(@miles numeric) RETURNS numeric AS BEGIN RETURN (SELECT @miles * 1609.34) END CREATE FUNCTION dbo.ConvertCurrency(@Currency numeric, @ExchangeRate numeric) RETURNS numeric AS BEGIN RETURN (SELECT @ExchangeRate * @Currency) END Formatting Tools Before Formatting SELECT DATENAME(weekday, StartDate) AS &#39;DayOfWeek&#39;, SUM(Duration) AS TotalDuration FROM CapitalBikeShare GROUP BY DATENAME(weekday, StartDate) ORDER BY DATENAME(weekday, StartDate) Sort by logical weekday SELECT DATENAME(weekday, StartDate) as &#39;DayOfWeek&#39;, SUM(Duration) as TotalDuration FROM CapitalBikeShare GROUP BY DATENAME(WEEKDAY, StartDate) ORDER BY CASE WHEN Datename(WEEKDAY, StartDate) = &#39;Sunday&#39; THEN 1 WHEN Datename(WEEKDAY, StartDate) = &#39;Monday&#39; THEN 2 WHEN Datename(WEEKDAY, StartDate) = &#39;Tuesday&#39; THEN 3 WHEN Datename(WEEKDAY, StartDate) = &#39;Wednesday&#39; THEN 4 WHEN Datename(WEEKDAY, StartDate) = &#39;Thursday&#39; THEN 5 WHEN Datename(WEEKDAY, StartDate) = &#39;Friday&#39; THEN 6 WHEN Datename(WEEKDAY, StartDate) = &#39;Saturday&#39; THEN 7 END ASC; SELECT TOP 5 FORMAT(CAST(StartDate as Date), &#39;d&#39;, &#39;de-de&#39;) AS &#39;German Date&#39;, FORMAT(CAST(StartDate as Date), &#39;d&#39;, &#39;en-us&#39;) AS &#39;US Eng Date&#39;, FORMAT(Sum(Duration), &#39;n&#39;, &#39;de-de&#39;) AS &#39;German Duration&#39;, FORMAT(SUM(Duration), &#39;n&#39;, &#39;en-us&#39;) AS &#39;US Eng Duration&#39;, FORMAT(SUM(Duration),&#39;#,0.00&#39;) AS &#39;Custom Numeric&#39; FROM CapitalBikeShare GROUP BY CAST(StartDate as Date) SELECT DATENAME(weekday, StartDate) AS &#39;DayOfWeek&#39;, FORMAT(SUM(Duration),&#39;#,0.00&#39;) AS &#39;TotalDuration&#39; FROM CapitalBikeShare GROUP BY DATENAME(WEEKDAY, StartDate) ORDER BY CASE WHEN Datename(WEEKDAY, StartDate) = &#39;Sunday&#39; THEN 1 WHEN Datename(WEEKDAY, StartDate) = &#39;Monday&#39; THEN 2 WHEN Datename(WEEKDAY, StartDate) = &#39;Tuesday&#39; THEN 3 WHEN Datename(WEEKDAY, StartDate) = &#39;Wednesday&#39; THEN 4 WHEN Datename(WEEKDAY, StartDate) = &#39;Thursday&#39; THEN 5 WHEN Datename(WEEKDAY, StartDate) = &#39;Friday&#39; THEN 6 WHEN Datename(WEEKDAY, StartDate) = &#39;Saturday&#39; THEN 7 END ASC " }, { "title": "Stored Procedures", "url": "/posts/8sql3/", "categories": "SQL, 8. Writing Functions & Stored Procedures in SQL Server", "tags": "sql, t-sql, stored procedures, create procedure, exec", "date": "2022-05-19 00:00:00 +0545", "snippet": " What is a stored procedure? Routines that Accept input parameters Perform actinos (EXECUTE, SELECT, INSERT, UPDATE, DELETE, and other SP statments) Return status (success or failure) Return output parameters Why use stored procedures? Can reduce execution time Can reduce network traffic Allow for Modular Programming Improved Security What’s the difference? UDFs SPs Must return value - table value allowed Return value optional - No table valued Embeded SELECT execute allowed Cannot embed in SELECT to execute No output parameters Return output parameters &amp;amp; status No INSERT, UPDATE, DELETE INSERT, UPDATE, DELETE allowed Cannot execute SPs Can execute functions &amp;amp; SPs No Error Handing Error Handing with TRY....CATCH Create Procedure with Output parameter -- First four lines of code -- SP name must be unique CREATE PROCEDURE dbo.cuspGetRideHrsOneDay @DateParm date, @RideHrsOut numeric OUTPUT AS SET NOCOUNT ON BEGIN SELECT @RideHrsOut = SUM( DATEDIFF(second, PickupDate, DropoffDate) )/ 3600 FROM YellowTripData WHERE CONVERT(date, PickupDate) = @DateParm RETURN END; Output parameters vs return values Output Parameters Return Value Can be any data type Used to indicate success or failure Can declare multiple per SP Integer data type only Cannot be table-valued parameters 0 indicates success and non-zero indicates failure CRUD Why stored procedures for CRUD? Decouples SQL code from other application layers Improved Security Performance C for CREATE CREATE PROCEDURE dbo.cusp_TripSummaryCreate( @TripDate as date, @TripHours as numeric(18,0) ) AS BEGIN INSERT INTO dbo.TripSummary(Date, TripHours) VALUES (@TripDate, @TripHours) SELECT Date, TripHours FROM dbo.TripSummary WHERE Date = @TripDate END R for READ CREATE PROCEDURE cusp_TripSumaryRead (@TripDate as date) AS BEGIN SELECT Date, TripHours FROM TripSummary WHERE Date = @TripDate END; U for UPDATE CREATE PROCEDURE dbo.cusp_TripSummaryUpdate ( @TripDate as date, @TripHours as numeric(18,0) ) AS BEGIN UPDATE dbo.TripSummary SET DATE = @TripDate, TripHours = @TripHours WHERE DATE = @TripDate END; D for DELETE CREATE PROCEDURE cusp_TripSummaryDelete (@TripDate as date, @RowCountOut int OUTPUT) AS BEGIN DELETE FROM TripSummary WHERE Date = @TripDate SET @RowCountOut = @@ROWCOUNT END; Let’s EXEC! Ways to EXECute No output parameter or return value Store return value With output parameter With output parameter &amp;amp; store return value Store result set Example: DECLARE @RideHrs as numeric (18,0) EXEC dbo.cuspSumRideHrsOneDay @DateParm = &#39;1/5/2017&#39;, @RideHrsOut = @RideHrs OUTPUT SELECT @RideHrs as TotalRideHrs NEED TO ADD MORE…." }, { "title": "User Defined Functions", "url": "/posts/8sql2/", "categories": "SQL, 8. Writing Functions & Stored Procedures in SQL Server", "tags": "sql, t-sql, udf, table valued udf, alter function, schemabinding", "date": "2022-05-19 00:00:00 +0545", "snippet": " What are User Defined Functions (UDFs)? User Defined Functions (UDFs) are routines that can accept input parameters perform an action return result (single scalar value or table) Why use UDFs? Because it can reduce execution time it can reduce network traffix allow for modular programming What is modular programming? Software design technique Separates functionality into independent, interchangeable modules Allows code reuse Improves code readability Scalar UDF with no input parameter -- Scalar function with no input parameters CREATE FUNCTION GetTomorrow() RETURNS date AS BEGIN RETURN (SELECT DATEADD(day,1,GETDATE())) END Scalar UDF with one parameter -- Scalar function with one parameter CREATE FUNCTION GetRideHrsOneDay(@DateParm date) RETURNS numeric AS BEGIN RETURN ( SELECT SUM( DATEDIFF(second, PickupDate, DropoffDate) )/360 FROM YellowTripData WHERE CONVERT (date, PickupDate) = @DatePar ) END; Scalar UDF with two input parameters -- Scalar function with two input parameters CREATE FUNCTION GetRideHrsDateRange ( @StartDateParm datetime, @EndDateParm datetime ) RETURNS numeric AS BEGIN RETURN ( SELECT SUM( DATEDIFF(second, PickupDate, DropOffDate) )/ 3600 FROM YellowTripData WHERE PickupDate &amp;gt; @StartDateParm AND DropoffDate &amp;lt; @EndDateParm ) END; Table Values UDFs Inline Table Valued Functions (ITVF) CREATE FUNCTION SumLocationStats( @StartDate AS datetime = &#39;1/1/2017&#39; ) RETURNS TABLE AS RETURN SELECT PULocationID AS PickupLocation, COUNT(ID) AS RideCount, SUM(TripDistance) AS TotalTripDIstance FROM YellowTripData WHERE CAST(PickupDate AS Date) = @StartDate GROUP BY PULocationID; CREATE FUNCTION CountTripAvgFareDay ( @Month char(2), @Year char(4) ) RETURNS @TripCountAvgFare TABLE( DropOffDate date, TripCount int, AvgFare numeric ) AS BEGIN INSERT INTO @TripCountAvgFare SELECT CAST(DropOffDate as date), COUNT(ID), AVG(FareAmount) as AvgFareAmt FROM YellowTripData WHERE DATEPART(month, DropOffDate) = @Month AND DATEPART(year, DropOffDate) = @Year GROUP BY CAST(DropOffDate as date) RETURN END; Differences - ITVF vs. MSTVF | Inline | Multi Statement | | RETURN results of SELECT | DECLARE table variable to be returned | | Table column name in SELECT | BEGIN END block required | | No table variable | INSERT data into table variable | | No BEGIN END needed | RETURN last statement with BEGIN/END block | | No INSERT | | | Faster performance | | UDFs in action Execute scalar with SELECT -- Select with no parameters SELECT dbo.GetTomorrow() Execute scalar with EXEC &amp;amp; stoer result -- EXEC &amp;amp; store result in variable DECLARE @TotalRideHrs AS numeric EXEC @TotalRideHrs = dbo.GetRideHrsOneDay @DateParm = &#39;1/15/2017&#39; SELECT &#39;Total Ride Hours for 1/15/2017:&#39;, @TotalRideHrs SELECT parameter value $ sccalar UDF -- Declare parameter variable -- Set to oldest date in YellowTripData -- Pass to function with select DECLARE @DateParm as date = (SELECT TOP 1 CONVERT(date, PickupDate) FROM YellowTripData ORDER BY PickupDate DESC) SELECT @DateParm, dbo.GetRideHrsOneDay (@DateParm) DECLARE @CountTripAvgFareDay TABLE( DropOffDate date, TripCount int, AvgFare numeric) INSERT INTO @CountTripAvgFareDay SELECT TOP 10 * FROM dbo.CountTripAvgFareDay (01, 2017) ORDER BY DropOffDate ASC SELECT * FROM @CountTripAvgFareDay # Maintaining User Defined Functions ALTER Function ALTER FUNCTION SumLocationStats (@EndDate as datetime = &#39;1/01/2017&#39;) RETURNS TABLE AS RETURN SELECT PULocationID as PickupLocation, COUNT(ID) as RideCount, SUM(TripDistance) as TotalTripDistance FROM YellowTripData WHERE CAST(DropOffDate as Date) = @EndDate GROUP BY PULocationID; CREATE OR ALTER CREATE OR ALTER FUNCTION SumLocationStats ( @EndDate AS datetime = &#39;1/01/2017&#39;) RETURNS TABLE AS RETURN SELECT PULocationID as PickupLocation, COUNT(ID) AS RideCount, SUM(TripDistance) AS TotalTripDistance FROM YellowTripData WHERE CAST(DropOffDate AS Date) = @EndDate GROUP BY PULocationID; -- Delete function DROP FUNCTION dbo.CountTripAvgFareDay -- Create CountTripAvgFareDay as Inline TVF instead of MSTVF CREATE FUNCTION dbo.CountTripAvgFareDay( @Month char(2), @Year char(4) ) RETURNS TABLE AS RETURN ( SELECT CAST(DropOffDate as date) as DropOffDate, COUNT(ID) as TripCount, AVG(FareAmount) as AvgFareAmt FROM YellowTripData WHERE DATEPART(month, DropOffDate) = @Month AND DATEPART(year, DropOffDate) = @Year GROUP BY CAST(DropOffDate as date)); Determinism improves performance A function is deterministic when it returns the same result given the same input parameters the same database state Schemabinding Specifies the schema is bound to the database objects that it references Prevents changes to the schema if schema bound objects are referencing it " }, { "title": "EDA", "url": "/posts/8sql1/", "categories": "SQL, 8. Writing Functions & Stored Procedures in SQL Server", "tags": "sql, t-sql, temporal eda, date manipulation", "date": "2022-05-19 00:00:00 +0545", "snippet": " Temporal EDA Exploratory Data Analysis (EDA) Process Iterative No specific checklist for EDA questions SQL functions for EDA -- CONVERT Syntax: CONVERT (data_type[(length)], expression[,style]) -- Returns expression based on data_type --DATEPART Syntax DATEPART(datepart, date) -- Returns int -- DATENAME syntax DATENAME(datepart, date) -- Returns nvarchar -- DATEDIFF Syntax DATEDIFF(datepart, startdate, enddate) -- Retrurns int; can&#39;t use datepart weekday value -- datepart value s= year, quarter, month, dayofyear, day, week, weekday, hour, -- minute, second, microsecond, nanosecond -- CONVERT SELECT TOP 1 PickUpDate, CONVERT(DATE, PickupDate) AS DateOly FROM YellowTripData -- DATEPART SELECT TOP 3 COUNT(ID) AS NumberofRides, DATEPART(HOUR, PickupDate) AS Hour FROM YellowTripData GROUP BY DATEPART(HOUR, PickupDate) ORDER BY COUNT(ID) DESC -- DATENAME SELECT TOP 3 ROUND( SUM(FareAmount), 0 ) as TotalFareAmt, DATENAME(WEEKDAY, PickupDate) AS DayofWeek FROM YellowTripData GROUP BY DATENAME(WEEKDAY, PickupDate) ORDER BY SUM(FareAmount) DESC; --DATEDIFF SELECT AVG( DATEDIFF(SECOND, PickupDate, DropOffDate)/ 60 ) AS AvgRideLengthInMin FROM YellowTripData WHERE DATENAME(WEEKDAY, PickupDate) = &#39;Sunday&#39;; Variables for datetime data```sql -- DECLARE variable and assign initial value DECLARE @StartTime as time = &#39;08:00 AM&#39; -- DECLARE variabel and then SET value DECLARE @StarTime AS time SET @StartTIme = &#39;08:00 AM&#39; -- DECLARE variable then SET value DECLARE @BeginDate as date SET @BeginDate = ( SELECT TOP 1 PickupDate FROM YellowTripData ORDER BY PickupDate ASC )``` CASTing -- CAST syntax CAST (expression AS data_type [(length)]) -- Returns expression based on data_type -- DECLARE datetime variabel -- SET value to @BeginDate and @StartTime while CASTing DECLARE @StartDateTime as datetime SET @StartDateTime = CAST(@BeginDate as datetime) + CAST(@StartTime as datetime) -- DECLARE table variable with two columns DECLARE @TaxiRideDates TABLE ( StartDate date, EndDate date ) -- INSERT static values into table variable INSERT INTO @TaxiRideDates (StartDate, EndDate) SELECT &#39;3/1/2018&#39;, &#39;3/2/2018&#39; -- INSERT query result INSERT INTO @TaxiRideDates(StartDate, EndDate) SELECT DISTINCT CAST(PickuPDate as date), CAST(DropOffDate as date) FROM YellowTripData; Date manipulation GETDATE SELECT GETDATE() DECLARE @CurrentDateTime AS datetime SET @CurrentDateTime = GETDATE() SELECT @CurrentDateTIme -- DATEADD Syntax: DATEADD(datepart, number, date) -- Returns expression based on data_type -- Oneday after 2/27/2019 SELECT DATEADD(day,1,&#39;2/27/2019&#39;) DATEADD and GETDATE --Yesterday SELECT DATEADD(d,-1,GETDATE()) -- Yesterday&#39;s Taxi Passenger Count SELECT SUM(PassengerCount) FROM YellowTripData WHERE CAST(PickupDate as date) = DATEADD(d,-1,GETDATE()) DATEDIFF? SELECT DATEDIFF(day, &#39;2/27/2019&#39;,&#39;2/28/2019&#39;) SELECT DATEDIFF(year, &#39;12/31/2017&#39;, &#39;1/1/2019&#39;) -- First Day of Current Week SELECT DATEADD(week, DATEDIFF(week, 0, GETDATE()),0) -- First step GETDATE() -- How many weeks between today and 1/1/1900? SELECT DATEDIFF(week,0,GETDATE()) -- Add zero to the 6218nd week SELECT DATEADD(week, DATEDIFF(week,0,GETDATE()),0) " }, { "title": "Transaction Isolation Levels", "url": "/posts/7sql4/", "categories": "SQL, 7. Transactions And Error Handling in SQL Server", "tags": "sql, t-sql, concurrency, read committed, read uncommitted, repeatable read, serializable, snapshot", "date": "2022-05-19 00:00:00 +0545", "snippet": " What is concurrency? Concurrency: two or more transactions that read/change shared data at the same time.Isolate our transaction from other transactions Transaction isolation levels READ COMMITTED (default) READ UNCOMMITTED REPEATABLE READ SERIALIZABLE SNAPSHOT SET TRANSACTION ISOLATION LEVEL {READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE | SNAPSHOT} will add rest later…" }, { "title": "Transactions", "url": "/posts/7sql3/", "categories": "SQL, 7. Transactions And Error Handling in SQL Server", "tags": "sql, t-sql, transactions, Begin a transaction, commit a transaction, rollback a transaction, savepoints", "date": "2022-05-19 00:00:00 +0545", "snippet": " Transactions What is a transaction? Transaction: one or more statements, all or none of the statements are executed. Example: Transfer $100 account A -&amp;gt; account B Subtract $100 from account A Add $100 to account b Operation 2 FAILS -&amp;gt; Can’t subtract $100 from account A! Transaction statements - BEGIN a transaction BEGIN {TRAN|TRANSACTION} [ {transactino_name|@tran_name_variable} [WITH MARK [&#39;description&#39;]] ] [;] Transaction statements - COMMIT a transaction COMMIT [ {TRAN|TRANSACTION} [transactino_name | tran_name_variable]] [WITH (DELAYED_DURABILITY = {OFF|ON})][;] Transaction statements - ROLLBACK a transaction ROLLBACK {TRAN | TRANSACTION} [transaction_name|@tran_name_variable|savepoint_name|@savepoint_variable][;] Transaction - example Account 1 = $24,400 Account 5 = $35,300 BEGIN TRAN; UPDATE accounts SET current_balance = current_balance - 100 WHERE account_id = 1; INSERT INTO transaction VALUES(1,-100, GETDATE()); UPDATE accounts SET current_balance = current_balance + 100 WHERE account_id = 5; INSERT INTO transaction VALUES(5,100, GETDATE()); COMMIT TRAN; Transaction - example with TRY…CATCH Account 1 = $24,400 Account 5 = $35,300 BEGIN TRY BEGIN TRAN; UPDATE accounts SET current_balance = current_balance - 100 WHERE account_id = 1; INSERT INTO transaction VALUES(1,-100, GETDATE()); UPDATE accounts SET current_balance = current_balance + 100 WHERE account_id = 5; INSERT INTO transaction VALUES(5,100, GETDATE()); COMMIT TRAN; END TRY BEGIN CATCH ROLLBACK TRAN; END CATCH @@TRANCOUNT and savepointsNumber of BEGIN TRAN statemetns that are active in your current connection. Returns: greater than 0 -&amp;gt; open transaction 0 -&amp;gt; no open transaction Modified by: BEGIN TRAN -&amp;gt; @@TRANCOUNT + 1 COMMIT TRAN -&amp;gt; @@TRANCOUNT - 1 ROLLBACK TRAN -&amp;gt; @@TRANCOUNT = 0 (except with savepoint_name) Nested Transactions SELECT @@TRANCOUNT AS &#39;@@TRANCOUNT value&#39;; BEGIN TRAN; SELECT @@TRANCOUNT AS &#39;@@TRANCOUNT value&#39;; DELETE transactions; BEGIN TRAN SELECT @@TRANCOUNT AS &#39;@@TRANCOUNT value&#39;; DELETE accounts; -- If @@TRANCOUNT &amp;gt; 1 it doesn&#39;t commit! COMMIT TRAN; SELECT @@TRANCOUNT AS &#39;@@TRANCOUNT value&#39;; ROLLBACK TRAN; SELECT @@TRANCOUNT AS &#39;@@TRANCOUNT value&#39;; @@TRANCOUNT in a TRY…CATCH construct BEGIN TRY BEGIN TRAN; UPDATE accounts SET current_balance = current_balance - 100 WHERE account_id = 1; INSERT INTO transactions VALUES (1, -100, GETDATE()); UPDATE accounts SET current_balance = current_balance + 100 WHERE account_id = 5; INSERT INTO transactions VALUES (5, 100, GETDATE()); IF (@@TRANCOUNT &amp;gt; 0) COMMIT TRAN; END TRY BEGIN CATCH IF (@@TRANCOUNT &amp;gt; 0) ROLLBACK TRAN; END CATCH Savepoints Markers within a transaction Allow to rollback to the savepoints SAVE { TRAN | TRANSACTION } { savepoint_name | @savepoint_variable } [ ; ] BEGIN TRAN; SAVE TRAN savepoint1; INSERT INTO customers VALUES (&#39;Mark&#39;, &#39;Davis&#39;, &#39;markdavis@mail.com&#39;, &#39;555909090&#39;); SAVE TRAN savepoint2; INSERT INTO customers VALUES (&#39;Zack&#39;, &#39;Roberts&#39;, &#39;zackroberts@mail.com&#39;, &#39;555919191&#39;); ROLLBACK TRAN savepoint2; ROLLBACK TRAN savepoint1; SAVE TRAN savepoint3; INSERT INTO customers VALUES (&#39;Jeremy&#39;, &#39;Johnsson&#39;, &#39;jeremyjohnsson@mail.com&#39;, &#39;555929292&#39;); COMMIT TRAN; XACT_ABORT &amp;amp; XACT_STATE To be updated…" }, { "title": "Error Handling 2", "url": "/posts/7sql2/", "categories": "SQL, 7. Transactions And Error Handling in SQL Server", "tags": "sql, t-sql, error handling, raiserror, throw, formatmessage", "date": "2022-05-19 00:00:00 +0545", "snippet": "Raise Error Raise Errors Statements RAISERROR THROW Microsoft suggests THROW RAISEERROR ( {msg_str | msg_id | @local_variable_message}, severity, state, [arguement [,...m ]]) [ WITH option [, ...n]] RAISEERROR with message string IF NOT EXISTS (SELECT * FROM staff WHERE staff_id = 15) RAISEERROR(&#39;No staff member with such id.&#39;, 16, 1); Msg. 50000, Level 16, State 1, Line 3 No staff member with such id. IF NOT EXISTS (SELECT * FROM staff WHERE staff_id = 15) RAISERROR(&#39;No %s with id %d.&#39;, 16, 1, &#39;staff member&#39;, 15); Msg. 50000, Level 16, State 1, Line 3 No staff member with such id 15. RAISERROR(&#39;%d%% discount&#39;, 16, 1 50); Msg. 50000, Level 16, State 1, Line 1 50% discount Other characters: %i, %o, %x, %X, %u, … RAISERROR - Example with TRY…CATCH BEGIN TRY IF NOT EXISTS (SELECT * FROM staff WHERE staff_id = 15) RAISERROR(&#39;No staff member with such id.&#39;, 9, 1); END TRY BEGIN CATCH SELECT (&#39;You are in the CATCH block&#39;) AS message END CATCH THROW THROW syntax Recommended by Microsoft over the RAISERROR statment. Syntax: THROW [error_number, message, state][;] THROW - without parameters BEGIN TRY SELECT price/0 from orders; END TRY BEGIN CATCH THROW; SELECT &#39;This line is executed!&#39; as message; END CATCH THROW - ambiguity BEGIN TRY SELECT price/0 from orders; END TRY BEGIN CATCH SELECT &#39;This line is executed; THROW; END CATCH THROW - with parameters BEGIN TRY IF NOT EXISTS(SELECT * FROM staff WHERE staff_id = 15) THROW 51000, &#39;This is an example&#39;, 1; END TRY BEGIN CATCH SELECT ERROR_MESSAGE() AS message; END CATCH Customizing error messages in the THROW statement Parameter placeholders in RAISERROR and THROW RAISERROR(&#39;No %s with id %d.&#39;, 16, 1, &#39;staff member&#39;, 15); THROW 52000, &#39;No staff with id 15&#39;,1; Ways of customizing error messages Variable by concatenating string FORMATMESSAGE function Using a variable and the CONCAT function DECLARE @staff_id AS INT = 500; DECLARE @my_message NVARCHAR(500) = CONCAT(&#39;There is no staff member for id &#39;, @staff_id, &#39;. Try with anotherone.&#39;); IF NOT EXISTS (SELECT * FROM staff WHERE staff_id = @staff_id) THROW 50000, @my_message, 1; The FORMATMESSAGE function FORMATMESSAGE ( { &#39;msg_string&#39; | msg_number }, [ param_value [, ...n]] ) DECLARE @staff_id AS INT = 500; DECLARE @my_message NVARCHAR(500) = FROMATMESSAGE(&#39;There is no staff member for id %d. %s &#39;, @staff_id, &#39;Try with another one. &#39;); IF NOT EXISTS (SELECT * FROM staff WHERE staff_id = @staff_id) THROW 50000, @my_message, 1; " }, { "title": "Error Handling 1", "url": "/posts/7sql1/", "categories": "SQL, 7. Transactions And Error Handling in SQL Server", "tags": "sql, t-sql, error handling, try...catch, error anatomy, error_number, error_severity, error_state, error_line, error_procedure, error_message", "date": "2022-05-19 00:00:00 +0545", "snippet": "DatasetsProducts TableBuyers TableStaff TableOrders Table Getting an error CONSTRAINT unique_product_name UNIQUE(product_name); INSERT INTO products(product_name, stock, price) VALUES(&#39;Trek Powerfly 5 - 2018&#39;, 10, 3499.99); Violation of UNIQUE constraints ‘unique_product_name’. Cannot insert duplicate key in object ‘dbo.products’. The duplicate key value is (Trek Powerfly 5 - 2018). The TRY…CATCH syntax Syntax: BEGIN TRY {sql_statement | statement_block} END TRY BEGIN CATCH [{sql_statement | statement_block}] END CATCH [;] Enclose your statements within the TRY block Place your error handling code withint the CATCH block Error in the TRY block -&amp;gt; the CATCH block takes the control No Error in the TRY block -&amp;gt; the CATCH bock is skipped Example: BEGIN TRY INSERT INTO products(product_name, stock, price) VALUES(&#39;Trek Powerfly 5 - 2018&#39;, 10, 3499.99); SELECT &#39;Product inserted correctly!&#39; AS message; END TRY BEGIN CATCH SELECT &#39;An error occured! You are in the CATCH block&#39; AS message; END CATCH The above sql statement runs the code within the TRY block and retuns the corresponding messages depending upon if the code runs successfully or not. Nesting TRY…CATCH BEGIN TRY INSERT INTO products(product_name, stock, price) VALUES(&#39;Trek Powerfly 5 - 2018&#39;, 10, 3499.99); SELECT &#39;Product inserted correctly!&#39; AS message; END TRY BEGIN CATCH SELECT &#39;An error occured inserting the product! You are in the first CATCH block&#39; AS message; BEGIN TRY INSERT INTO myErrors VALUES (&#39;ERROR!&#39;); SELECT &#39;Error inserted correctly!&#39; AS message; END TRY BEGIN CATCH SELECT &#39;An error occured inserting the error! You are in the second CATCH block&#39; AS message; END CATCH END CATCH Error anatomy and uncatchable errors Error anatomy INSERT INTO products(product_name, stock, price) VALUES(&#39;Trek Powerfly 5 - 2018&#39;, 10, 3499.99); Uncatchable erros Severity lower than 11 (11-19 are catchable) Severity of 20 or higher that stop the connection Compilation errors: objects and columns that don’t exist Giving Information About Errors Error Functions ERROR_NUMBER() returns the number of the error. ERROR_SEVERITY() returns the error severity (11-19). ERROR_STATE() returns the state of the error. ERROR_LINE() returns the number of the line of the rror. ERROR_PROCEDURE() returns the name of stored procedure/trigger. NULL if there is not stored procedure/trigger. ERROR_MESSAGE() returns the text of the error message. BEGIN TRY INSERT INTO products(product_nae, stock, price) VALUES(&#39;Trek Powerfly 5 - 2018&#39;, 10, 3499.99); END TRY BEGIN CATCH SELECT ERROR_NUMBER() AS Error_number, ERROR_SEVERITY() AS Error_severity, ERROR_STATE() AS Error_state, ERROR_PROCEDURE() AS Error_procedure, ERROR_LINE() AS Error_line, ERROR_MESSAGE() AS Error_message END CATCH " }, { "title": "Database Roles and Access Control", "url": "/posts/6sql4/", "categories": "SQL, 6. Database Design", "tags": "sql, t-sql, grant privilege, revoke privilege, databse roles, Table Partitioning, Vertical Partitioning, Horizontal Partitioning, Data Integration", "date": "2022-05-18 00:00:00 +0545", "snippet": " Granting and revoking access to a view GRANT privilege(s) or REVOKE privilege(s) ON object TO role or FROM role Privileges: SELECT, INSERT, UPDATE, DELETE, etc. Obects: table, view, schema, etc. Roles: a database user or a group of database users GRANT UPDATE ON ratings TO PUBLIC; REVOKE INSERT ON films FROM db_user; Database Roles Manage database access permissions A database role is a entity that contains information that: Define the role’s privileges Can you login? Can you create databases? Can you write to tables? Interact with the client authentication system Password Roles can be assigned to one or more users Roles are global across a database cluster installation Create a Role Empty Role CREATE ROLE data_analyst; Roles with some attributes set CREATE ROLE intern WITH PASSWORD &#39;PasswordForIntern&#39; VALID UNTIL &#39;2020-01-01&#39;; CREATE ROLE admin CREATEDB; ALTER ROLE admin CREATEROLE; GRANT and REVOKE privileges from roles GRANT UPDATE ON ratings TO data_analyst; REVOKE UPDATE ON ratings FROM data_analyst; The available privileges in PostgreSQL are: SELECT, INSERT, UPDATE, DELETE, TRUNCATE, REFERENCES, TRIGGER, CREATE, CONNECT, TEMPORARY, EXECUTE, and USAGE Users and groups (are both roles) A role is an entity that can function as a user and/or a group User roles Group roles Group Role CREATE ROLE data_analyst; User Role CREATE ROLE intern WITH PASSWORD &#39;PasswordForIntern&#39; VALID UNTIL &#39;2020-01-01&#39;; GRANT data_analyst TO alex; REVOKE data_analyst FROM alex; Common PostgreSQL roles | Role | Allowed Access | | pg_read_all_settings | Read all configuration variables, even those normally visible only to superusers. | | pg_read_all_stats | Read all pg_stats_* views and use various statistis related extensions, even those normally visible only to superusers. | | pg_signal_backend | Send signals to other backendds (eg: cancel query, terminate). | | More.. | More .. | Benefits and pitfalls of roles Benefits Roles live on after users are deleted Roles can be created before user accounts Save DBAs time Pitfalls Sometimes a role gives a specific user too much access You need to pay attention Table partitioning Why partition? Problem: queries / updates become slower Because: e.g., indices don’t fit memory Solution: split table into smaller parts (=partitioning) Data modeling refresher Conceptual Data Model Logical Data Model For partitioning, logical data model is the same Physical Data Model Partitioning is part of physical data model Vertical Partitioning Horizontal Partitioning CREATE TABLE sales ( ... timestamp DATE NOT NULL ) PARTITION BY RANGE (timestamp); CREATE TABLE sales_2019_q1 PARTITION OF sales FOR VALUES FROM (&#39;2019-01-01&#39;) TO (&#39;2019-03-31&#39;); ... CREATE TABLE sales_2019_q4 PARTITION OF sales FOR VALUES FROM (&#39;2019-09-01&#39;) TO (&#39;2019-12-31&#39;); CREATE INDEX ON sales (&#39;timestamp&#39;); Pros/Cons of Horizontal Partitioning Pros Indices of heavily-used partitions fit in memory Move to specific medium: slower vs faster Used for both OLAP as OLTp Cons Partitioning existing table can be a hassle Some constraints can not be set Relation to sharding Data Integrations What is Data Integration? Data Integration combines data from different sources, formats, technologiges to provide users with a translated and unified view of that data. Choosing a Data Integration Tool Flexible Reliable Scalable Picking a Database Management System (DBMS) DBMS DBMS: DataBase Management System Create and maintain databases Data Database schema Database engine Iterface between database and end users DBMS types Choice of DBMS depends on database type Two Types: SQL DMBS NoSQL DBMS SQL DMBS Relational DataBase Management System (RDBMS) Based on the relational model of data Query Language: SQL Best option when: Data is structured and unchanging Data must be consistent Tools: Microsoft-SQL Server, PostgreSQL NoSQL DBMS Less structured Document-centered rather than table-centered Data doesn’t have to fit into well-defined rows and columns Best option when: Rapid growth No clear schema definitions Large quantities of data Types: key-value store, document store, columnar database, graph database NoSQL DBMS - key-value store Combinations of keys and values Key: unique identifier Value: anything Use case: managing the shopping cart for an on-line buyer Tools: Redis NoSQL DBMS - document store Similar to key-value Values(=documents) are structured Use case: content management Tools: mongoDB NoSQL DBMS - columnar database Store data in columns Scalable Use case: big data analytics where speed is important Tools: Cassandra NoSQL DMBS - graph database Data is interconnected and best represented as a graph Use case: social media data, recommendations Example: neo4j " }, { "title": "Database Views", "url": "/posts/6sql3/", "categories": "SQL, 6. Database Design", "tags": "sql, t-sql, views, create view, query a view, viewing views, grant, revoke, drop view, redefine view, materalized views, non-materialized views", "date": "2022-05-18 00:00:00 +0545", "snippet": " In a database, a view is the result set of a stored query on the data, which the database users can query just as they would in a persistent database collection object.Virtual table that is not part of the physical schema Query, not data, is stored in memory Data is aggregated from data in tables Can be queried like a regular database table No need to retype common queries or alter schemas Creating A View Syntax: CREATE VIEW view_name AS SELECT col1, col2 FROM table_name WHERE condition; Example: CREATE VIEW scifi_books AS SELECT title, author, genre FROM dim_book_sf JOIN dim_genre_sf ON dim_genre_sf.genre_id = dim_book_sf.genre_id JOIN dim_author_sf ON dim_author_sf.author_id = dim_book_sf.author_id WHERE dim_genre_sf.genre = &#39;science fiction&#39;; Querying a view (example) SELECT * FROM scifi_books Viewing Views SELECT * FROM information_schema.views; Includes system views SELECT * FROM information_schema.views WHERE table_schema NOT IN (&#39;pg_catalog&#39;, &#39;information_schema&#39;); Excludes system views Benefits of Views Doesn’t take up storage A form of access control Hide sensitive columns and restrict what user can see Masks complexity of queries Useful for highly normalized schemas Managing Views Creating more complex views Aggregation: SUM(), AVG(), COUNT(), MIN(), MAX(), GROUP BY, etc. Joins: INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL JOIN Conditionals: WHERE, HAVING, UNIQUE, NOT NULL, AND, OR, &amp;gt;, &amp;lt;, etc. Granting and revoking access to a view GRANT privilege(s) or REVOKE privilege(s) ON object TO role or FROM role Privileges: SELECT, INSERT, UPDATE, DELETE, etc. Objects: table, view, schema, etc. Roles: a database user or a group of database users Granting and Revoking Example GRANT UPDATE ON ratings TO PUBLIC; REVOKE INSERT ON films FROM db_user; Updating a view UPDATE films SET kind=&#39;Dramatic&#39; WHERE kind=&#39;Drama&#39;; Not all views are updatable View is made up of one table Doesn’t use a window or aggregate function Inserting into a view INSERT INTO films (code, title, did, date_prod, kind) VALUES(&#39;T_601&#39;, &#39;Yojimbo&#39;, 106, &#39;1961-06-16&#39;, &#39;Drama&#39;); Not all views are insertable Note: AvoiD Modifying Data Through Views Dropping a view DROP VIEW view_name [ CASCADE | RESTRICT ]; RESTRICT (default): returns an error if there are objects that depend on the view CASCADE: drops view and any object that depends on that view Redefining a view CREATE OR REPLACE VIEW view_name AS new_query If a view with view_name exists, it is replaced new_query must generate the same column names, order, and data types as the old query The column output may be different New columns may be added at the end If these criteria can’t be met, drop the existing view and create a new one Altering a view ALTER VIEW [ IF EXISTS ] name ALTER [ COLUMN ] column_name SET DEFAULT expression ALTER VIEW [ IF EXISTS ] name ALTER [ COLUMN ] column_name DROP DEFAULT ALTER VIEW [ IF EXISTS ] name OWNER TO new_owner ALTER VIEW [ IF EXISTS ] name RENAME TO new_name ALTER VIEW [ IF EXISTS ] name SET SCHEMA new_schema ALTER VIEW [ IF EXISTS ] name SET ( view_option_name [=view_option_value] [,...]) ALTER VIEW [ IF EXISTS ] name RESET (view_option_name [,...]) Materialized Views Two Types of Views Views Also known as non-materialized views Materialized Views Physically Materialized Stores the query results, not the query Querying a materialized view means accessing the stored query results Not running the query like a non-materialized view Refreshed or rematerialized when prompted or scheduled When to use materialized views Long running queries Underlying query results don’t change often Data warehouses because OLAP is not write-intensive Save on computational cost of frequent queries Implementing materialized views CREATE MATERIALIZED VIEW my_mv AS SELECT * FROM existing_table; REFRESH MATERIALIZED VIEW my_mv; Managing dependencies Materialized views often depend on other materialized views Creates a dependency chain when refreshing views Not the most efficient to refresh all views at the same time Tools for Managing Dependencies Use Directed Acyclic Graphs (DAGs) to keep track of views Pipeline scheduler tools like:- Apache Airflow, Luigi " }, { "title": "Database Schemas", "url": "/posts/6sql2/", "categories": "SQL, 6. Database Design", "tags": "sql, t-sql, star schema, snowflake schema, normalization, de-normalization, normal forms, 1nf, 2nf, 3nd, Data Anomalies, Insertion Anomaly, Deletion Anomaly, Update Anomaly", "date": "2022-05-18 00:00:00 +0545", "snippet": "Star and Snowflake Schema Star Schema Dimensional modeling: star schema Fact tables Holds records of a metric Changes regularly Connects to dimensions via foreign keys Example: Supply books to stores in USA and Canada Keep track of book sales Dimension Tables Holds descriptions of attributes Does not change as often Star Schema Example One dimension Snowflake Schema (an extension) More than one dimension because dimension tables are normalized What is Normalization? Database design technique Divides tables into smalller tables and connects them via relationships Goal reduce redundancy and increase data integrity Book dimension of the star schema Most likely to have repeating values: Author Publisher Genre Book dimension of the snowflake schema Normalized and Denormalized Databases Back to our book store example Denormalized Query Goal: get quantity of all Octavia E.Butler books sold in Vancouver in Q4 of 2018 SELECT SUM(quantity) FROM fact_booksales -- Join to get city INNER JOIN dim_store_star on fact_booksales.store_id = dim_store_star.store_id -- Join to get author INNER JOIN dim_book_star on fact_booksales.book_id = dim_book_star.book_id -- Join to get year and quarter INNER JOIN dim_time_star on fact_booksales.time_id = dim_time_star.time_id WHERE dim_store_star.city = &#39;Vancouver&#39; AND dim_book_star.author = &#39;Octavia E. Butler&#39; AND dim_time_star.year = 2018 AND dim_time_star.quarter = 4; Normalized Query SELECT SUM(fact_booksales.quantity) FROM fact_booksales -- Join to get city INNER JOIN dim_store_sf ON fact_booksales.store_id = dim_store_sf.store_id INNER JOIN dim_city ON dim_store_sf.city_id = dim_city_sf.city_id -- Join to get author INNER JOIN dim_book_sf ON fact_booksales.book_id = dim_book_sf.book_id INNER JOIN dim_author_sf ON dim_book_sf.author_id = dim_author_sf.author_id -- Join to get year and quarter INNER JOIN dim_time_sf ON fact_booksales.time_id = dim_time_sf.time_id INNER JOIN dim_month_sf ON dim_time_sf.month_id = dim_month_sf.month_id INNER JOIN dim_quarter_sf ON dim_month_sf.quarter_id = dim_quarter_sf.quarter_id INNER JOIN dim_year_sf ON dim_quarter_sf.year_id = dim_year_sf.year_id WHERE dim_city_sf.city = &#39;Vancouver&#39; AND dim_author_sf.author = &#39;Octavia E.Butler&#39; AND dim_year_sf.year = 2018 AND dim_quarter_sf.quarter = 4; Why Normalization? Normalization saves space as it eliminates data redundancy Normalization ensures better data integrity Enforces data consistency: Must respect naming conventions because of referential integrity, e.g:- ‘California’, not ‘CA’ or ‘california’ Safer updating, removing, and inserting: Less data redundancy = less records to alter Easier to redesign by extending: Smaller tables are easier to extend than larger tables Disadvantage of Normalization Complex queries require more CPU Normal Forms Normalization Idenfity repeating groups of data and create new tables for them A more formal definition:The goals of normalization are to: Be able to characterize the level of redundancy in a relational schema Provide mechanisms for transforming schemas in order to remove redundancy Ordered from least to most normalized: First Normal Form (1NF) Second Normal Form (2NF) Third Normal Form (3NF) Elementary Key Normal Form (EKNF) Boyce-Codd Normal Form (BCNF) Fourth Normal Form (4NF) Essential Tuple Normal Form (ETNF) Fifth Normal Form (5NF) Domain-Key Normal Form (DKNF) Sixth Normal Form (6NF) 1NF Rules Each record must be unique - no duplicate rows Each cell must hold one value 2NF Must satisfy 1NF AND If primary key is one column then automatically satisfies 2NF If there is a composite primary key then each non-key column must be dependent on all the keys 3NF Satisifes 2NF No transitive dependencies: non-key columns can’t depend on other non-key columns Data Anomalies What is risked if we don’t normalize enough? Update anomaly Insertion anomaly Deletion anomaly The more normalized the database, the less prone it will be to data anomalies Update Anomaly Data inconsistency caused by data redundancy when updating To update student 520s email: Need to update more than one record, otherwise, there will be inconsistency User updating needs to know about redundancy Insertion Anomaly Unable toadd a record due to missing atributes Unable to insert a student who has signed up but not enrolled in any courses Deletion Anomaly Deletion of record(s) causes unintentional loss of data If we delete Student 230, what happens to the data on Cleaning Data in R? " }, { "title": "OLTP and OLAP", "url": "/posts/6sql1/", "categories": "SQL, 6. Database Design", "tags": "sql, t-sql, oltp, olap, data warehouse, data lakes, etl, elt, er-diagram, fact tables, dimension tables", "date": "2022-05-18 00:00:00 +0545", "snippet": " How should we organize and manage data? Schemas: How should my data be logically organized? Normalization: Should my data have minimal dependency and redundancy? Views: What joins will be done most ofte? Access Control: Should all users of the data have the same level of access DBMS: How do I pick between all the SQL and noSQL options? Approaches to processing data OLTP OLAP Online Transaction Processing Online Analytical Processing - Find the price of a book - Calculate books with best profit margin - Update latest customer transaction - Find most loyal customers - Keep track of employee hours - Decide employee of the month OLVAP vs OLTP   OLTP OLAP Purpose support daily transactions report and analyze data Data up-to-date, operational consolidated, historical Size snapshot, gigabyters archive, terrabytes Queries simple transactions &amp;amp; frequent updates complex, aggregate queries &amp;amp; limited updates Users thousands hundreds Storing Data Structuring Data Structured data Follows a schema Defined data types &amp;amp; relationships E.g:- SQL, tables in a relational database Unstuctured Data Schemaless Makes up most of data in the world E.g: - Photos, chat logs, MP3 Semi-structued data Does not follow larger schema Self-describing structure E.g: - NoSQL, XML, JSON # Example of a JSON file &#39;user&#39;: { &#39;profile_use_background_image&#39;: true, &#39;statuses_count&#39;: 31, &#39;profile_background_color&#39;: &#39;CODEED&#39;, &#39;followers_count&#39;: 3066, ... } Storing data beyond traditional databases Traditional databases For storing real-time relational structured data? OLTP Data warehouses For analyzing achieved structured data? OLAP Data lakes For storing data of all structures = flexibility and scalability For analyzing big data Data Warehouses Optimized for analytics - OLAP Organized for reading/aggregating data Usually read-only Contains data from multiple sources Massively Parallel Processing (MPP) Typically uses a denormalized schema and dimensional modeling Data Marts Subset of data warehouses Dedicated to a specific topic Data Lakes Store all types of data at a lower cost E.g:- raw, operational databases, IoT device logs, real-time, relational and non-relational Retains all data and can take up petabytes Schema-on-read as opposed to schema-on-write Need to catalog data otherwise becomes a data swamp Run big data analytics using services such as Apache Spark and Hadoop Useful for deep learning and data discovery because activites requires so much data ETL ELT Database Design What is database design? Determines how data is logically stored How is data going to be read and updated? Uses *database models**: high-level specifications for database structure Most popular: relational model Some other options: NoSQL models, object-oriented model, network model Use schemas: blueprint of the database Defines tables, fields, relationships, indexes, and views When inserting data in relational databases, schemas must be respected Data Modeling Process of creating a data model for the data to be stored. Conceptual data model: describes entities, relationships, and attributes Tools: data structure diagrams, e.g:- entity-relational diagrams and UML diagrams Logical data modle: defines tables, columns, relationships Tools: database models and schemas, e.g:- relational model and star schema Physical data model: describes physical storage Tools: partitions, CPUs, indexes, backup systems and tablespaces Beyond the relational model-Dimensional Modeling Adaptation of the relational model for data warehouse design Optimized for OLAP queries: aggregate data, not updating (OLTP) Built using the star schema Easy to interpret and extend schema Element of dimensional modeling Organize by: What is being analyzed? How often do entities change? Fact tables Decided by business use-case Holds records of a metric Changes regularly Connects to dimensions via foreign keys Dimension tables Holds descriptions of attributes DOes not change as often " }, { "title": "Aggregate Arithmetic Functions", "url": "/posts/5sql4/", "categories": "SQL, 5. Functions for Manipulating Data in SQL-SERVER", "tags": "sql, t-sql, count, sum, max, min, avg, first_value, last_value, partition limits, lag, lead, abs, sign, ceiling, floor, round, square, sqrt, power", "date": "2022-05-17 00:00:00 +0545", "snippet": " COUNT() Returns the number of items found in a group. COUNT([ALL] expression) COUNT(DISTINCT expression) COUNT(*) Example: SELECT COUNT(ALL country) AS count_countries_all, COUNT(country) AS count_countries, COUNT(DISTINCT country) AS distinct_countries, COUNT(*) AS all_voters FROM voters; SUM() Return the sum of all values from a group SUM([ALL] expression) SUM(DISTINCT expression) Example: SELECT SUM(ALL_total_votes) AS tot_votes1, SUM(total_votes) AS tot_votes2, SUM(DISTINCT total_votes) AS dist FROM voters WHERE total_votes = 153; MAX() and MIN() Syntax: MAX([ALL] expression) MAX(DISTINCT expression) Returns the maximum value in the expression Syntax: MIN([ALL] expression) MIN(DISTINCT expression) Returns the minimum value in the expression Example: SELECT MIN(rating) AS min_rating, MAX(rating) AS max_rating FROM ratings; AVG() Returns the average of the values in the group. Syntax: AVG([ALL] expression) AVG(DISTINCT expression) SELECT AVG(rating) AS avg_rating, AVG(DISTINCT rating) AS avg_dist FROM ratings; Grouping Data SELECT company, AVG(rating) AS avg_rating FROM ratings GROUP BY company; Analytic Functions FIRST_VALUE() Syntax: FIRST_VALUE(numeric_expression) OVER([PARTITION BY column] ORDER BY column ROW_or_RANGE frame) Returns the first value in an ordered set. OVER clause components Component Status Description PARTITION by column optional divide the result set into partitions ORDER BY column mandatory order the result set ROW_or_RANGE frame optional set the partition limits LAST_VALUE() Syntax: LAST_VALUE(numeric_expression) OVER([PARTITION BY column] ORDER BY column ROW_or_RANGE frame) Returns the last value in an ordered set. Partition Limits RANGE BETWEEN start_boundary AND end_boundary ROWS BETWEEN start_boundary AND end_boundary Boundary Description UNBOUNDED PRECEDING first row in the partition UNBOUNDED FOLLOWING last row in the partition CURRENT ROW current row PRECEDING previous row FOLLOWING next row Example: SELECT first_name + &#39; &#39; + last_name AS name, gender, total_votes AS votes, FIRST_VALUE(total_votes) OVER (PARTITION BY gender ORDER BY total_votes) AS min_votes, LAST_VALUE(total_votes) OVER (PARTITION BY gender ORDER BY total_votes ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS max_votes FROM voters; LAG() AND LEAD() LAG(numeric_expression) OVER ([PARTITION BY column] ORDER BY column) Accesses date from a previous row in the same result set. LEAD(numeric_expression) OVER ([PARTITION BY column] ORDER BY column) Accesses data from a subsequent row in the sam result set. SELECT broad_bean_origin AS bean_origin, rating, cocoa_percent, LAG(cocoa_percent) OVER (ORDER BY rating) AS percen_lower_rating, LEAD(cocoa_percent) OVER (ORDER BY rating) AS percent_higher_rating FROM ratings WHERE company = &#39;Felchlin&#39; ORDER BY rating ASC; Mathematical Functions ABS(numeric_expresison) Returns the absolute value of an expression. Is the non-negative value of the expression. SELECT ABS(-50.4 * 3) AS negative, ABS(0.0) AS zero, ABS(73.2 + 15 + 8.4) AS positive; SIGN(numeric_expression) Returns the sign of an expression, as an integer: -1(negative numbers) 0 +1(positive numbers) SELECT SIGN(-50.4*3) AS negative, SIGN(0.0) AS zero, SIGN(73.2 + 15 + 8.4) AS positive; Rounding Functions CEILING(numeric_expression) Returns the smallest integer greater than or equal to the expression. FLOOR(numeric_expression) Returns the largest integer less tha or equal to the expressino ROUND(numeric_expression, length) Returns a numeric value, rounded to the specified length. Example: SELECT CEILING(-50.39) AS ceiling_neg, -- -50 CEILING(73.71) AS ceiling_pos; -- 74 SELECT CEILING(-50.49) AS ceiling_neg, -- 50 FLOOR (-50.49) AS floor_neg, -- -51 CEILING(73.71) AS ceiling_pos, -- 74 FLOOR(73.71) AS floor_ps -- 73 ROUND(-50.493,1) AS round_neg, -- -50.00 ROUND(73.715, 2) AS round_pos; Exponential functions `POWER(numeric_expression, power) Returns the expression raised to the specified power. SQUARE(numeric_expression) Returns the square of the expression. SQRT(numeric_expression) Returns the square root of the expression. Keep in mind: the type of the expression is float or can be implicitely converted to float. POWER() example SELECT POWER(2,10) AS pos_num, POWER(-2,10) AS neg_num_even_pow, POWER(-2,11) AS neg_num_odd_power, POWER(2.5,2) AS float_num, POWER(2, 2.72) AS float_pow; SQUARE() example SELECT SQUARE(2) AS pos_num, SQUARE(-2) AS neg_num, SQUARE(2.5) AS float_num; SQRT() example SELECT SQRT(2) AS int_num, SQRT(2.76) AS float_num; " }, { "title": "Functions for Positions", "url": "/posts/5sql3/", "categories": "SQL, 5. Functions for Manipulating Data in SQL-SERVER", "tags": "sql, t-sql, len, charindex, patindex, wildcard characters, lower, upper, left, right, ltrim, rtrim, trim, replace, substring, concat, concat_ws, string_agg, string_split", "date": "2022-05-17 00:00:00 +0545", "snippet": " Position Functions LEN() CHARINDEX() PATINDEX() LEN() Returns the number of characters of the provided string. Syntax: LEN(character_expression) LEN() example - constant parameter SELECT LEN(&#39;Do you know the length of this sentence?&#39;) AS length LEN() example - table column paramter SELECT DISTINCT TOP 5 bean_origin, LEN(bean_origin) AS length FROM ratings; CHARINDEX() Looks for a character expression in a givenstring. Returns its starting position Syntax: CHARINDEX(expression_to_find, expression_to_search, [, start_location]) SELECT CHARINDEX(&#39;chocolate&#39;, &#39;White chocoalte is not real chocolate&#39;), CHARINDEX(&#39;chocolate&#39;, ) PATINDEX() Similar to CHARINDEX() Returns the starting position of a pattern in an expression Syntax: PATINDEX(‘%pattern%’, expression, [location]) SELECT PATINDEX(&#39;%chocolate%&#39;, &#39;White chocolate is not real chocolate&#39;) AS position1, PATINDEX(&#39;%ch_c%&#39;, &#39;White chocolate is not real chocolate&#39;) AS position2; Wildcard Characters | Wildcard | Explanation | | % | Match any string of any length (including zero length) | | _ | Match on a single character | | [] | Match on any character in the [] brackets (for example, [abc] would match on a, b, or c characters) Functions for string transformation LOWER() and UPPER() LOWER(character_expression) Converts all characters from a string to lowercase. UPPER(character_expression) Converts all characters from a string to uppercase. ```sql SELECT country, LOWER(country) AS country_lowercase, UPPER(country) AS country_uppercase FROM voters; LEFT() and RIGHT() LEFT(character_expression, number_of_characters) Returns the specified number of characters from the beginning of the string RIGHT(character_expression, number_of_characters) Returnsthe specified number of characters from the end of the string SELECT country, LEFT(country,3) AS country_prefix, email, RIGHT(email, 4) AS email_domain FROM voters; LTRIM(), RTRIM(), and TRIM() LTRIM(character_expression) Returns a string after removing the leading blanks. RTRIM(character_expression) Returns a string after removing the trailing blanks. `TRIM([characters FROM] character_expression) Returns a string after removing the blanks or other specified characters. REPLACE() Syntax: REPLACE(character_expression, searched_expression, replacement_expression) - Returns a string where all occurences of an expression are replaced with another one. SELECT REPLACE(&#39;I like apples, apples are good.&#39;, &#39;apple&#39;, &#39;orange&#39;) AS result; SUBSTRING() Syntax: REPLACE(character_expression, start, number_of_characters) - Returns part of a string. SELECT REPLACE(&#39;123456789&#39;, 5, 3) AS result; Functions manipulating groups of strings CONCAT() and CONCAT_WS() CONCAT(string1, string2, [, stringN]) CONCAT_WS(separator, string1, string2, [, stringN]) Keep in mind: concatenating data with functions is better than using the “+” operator. SELECT CONCAT(&#39;Apples&#39;, &#39;and&#39;, &#39;oranges&#39;) AS result_concat, CONCAT_WS(&#39; &#39;, &#39;Apples&#39;, &#39;and&#39;, &#39;oranges&#39;) AS result_concat_ws, CONCAT_WS(&#39;***&#39;, &#39;Apples&#39;, &#39;and&#39;, &#39;oranges&#39;) AS result_concat_ws2; STRING_AGG() Syntax: STRING_AGG(expression, separator) [ &amp;lt;order_clause&amp;gt;] Concatenates the values of string expressions and places separator values between them. SELECT STRING_AGG(first_name, &#39;,&#39;) AS list_of_names FROM voters; SELECT STRING_AGG(CONCAT(first_name, &#39; &#39;, last_name, &#39; (&#39;, first_vote_date, &#39;)&#39;), CHAR(13)) AS list_of_names FROM voters; STRING_AGG() with GROUP BY SELECT YEAR(first_vote_date) AS voting_year, STRING_AGG(first_name, &#39;, &#39;) AS voters FROM voters GROUP BY YEAR(first_vote_date); STRING_AGG() with the optional SELECT YEAR(first_vote_date) AS voting_year, STRING_AGG(first_name, &#39;, &#39;) WITHIN GROUP (ORDER BY first_name ASC) AS voters FROM voters GROUP BY YEAR(first_vote_date); STRING_SPLIT() Syntax: STRING_SPLIT(string, separator) Divides a string into smaller pieces, based on a separator. Returns a single column table " }, { "title": "Functions that return system date and time", "url": "/posts/5sql2/", "categories": "SQL, 5. Functions for Manipulating Data in SQL-SERVER", "tags": "sql, t-sql, timezones, sysdatetime, sysutcdatetime, sysdatetimeoffset, getdate, getutcdate, current_timestamp, convert, year, month, datename, datepart, datefrompart, dateadd, datediff, isdate, set dateformat, set language", "date": "2022-05-17 00:00:00 +0545", "snippet": " Common mistakes when working with dates and time Inconsistent date time formats or patterns Arithmetic Operations Issues with time zones Time Zones in SQL Server Local Time Zone UTC Time Zone (Universal Time Coordinate) Functions that return the date and time of the operatins system Higher-precision SYSDATETIME() SYSUTCDATETIME() SYSDATETIMEOFFSET() Lower-precision GETDATE() GETUTCDATE() CURRENT_TIMESTAMP Higher-precision functions example SELECT SYSDATETIME() AS [SYSDATETIME], SYSDATETIMEOFFSET() AS [SYSDATETIMEOFFSET], SYSUTCDATETIME() AS [SYSUTCDATETIME]; Lower-precision functions example SELECT CURRENT_TIMESTAMP AS [CURRENT_TIMESTAMP], GETDATE() AS [GETDATE], GETUTCDATE() AS [GETUTCDATE]; Retrieving only the date SELECT CONVERT(date, SYSDATETIME()) AS [SYSDATETIME], CONVERT(date, SYSDATETIMEOFFSET()) AS [SYSDATETIMEOFFSET], CONVERT(date, SYSUTCDATETIME()) AS [SYSUTCDATETIME], CONVERT(date, CURRENT_TIMESTAMP) AS [CURRENT_TIMESTAMP], CONVERT(date, GETDATE()) AS [GETDATE], CONVERT(date, GETUTCDATE()) AS [GETUTCDATE; Retrieving only the time SELECT CONVERT(time, SYSDATETIME()) AS [SYSDATETIME], CONVERT(time, SYSDATETIMEOFFSET()) AS [SYSDATETIMEOFFSET], CONVERT(time, SYSUTCDATETIME()) AS [SYSUTCDATETIME], CONVERT(time, CURRENT_TIMESTAMP) AS [CURRENT_TIMESTAMP], CONVERT(time, GETDATE()) AS [GETDATE], CONVERT(time, GETUTCDATE()) AS [GETUTCDATE; Functions returning date and time parts YEAR(date) Returns the year from the specified date SELECT first_name, first_vote_date, YEAR(first_vote_date) AS first_vote_year FROM voters; MONTH(date) Returns the month from the specified date SELECT first_name, first_vote_date, YEAR(first_vote_date) AS first_vote_year, MONTH(first_vote_date) AS first_vote_month FROM voters; MONTH(date) Returns the day from the specified date SELECT first_name, first_vote_date, YEAR(first_vote_date) AS first_vote_year, MONTH(first_vote_date) AS first_vote_month, DAY(first_vote_date) AS first_vote_day FROM voters; DATENAME(datepart, date) Returns a character string representing the specified date part of the given date datepart abbreviations year yy,yyyy month mm, m dayofyear dy,y week wk,ww weekday dw,w DATENAME() example DECLARE @date datetime = &#39;2019-03-24&#39; SELECT YEAR(@date) AS year, DATENAME(YEAR, @date) AS year_name, MONTH(@date) AS month, DATENAME(MONTH, @date) AS month_name, DAY(@date) AS day, DATENAME(DAY, @date) AS day_name, DATENAME(WEEKDAY, @date) AS weekday DATEPART(datepart, date) It is similar to DATENAME() Returns an integer representing the specified part of the given date DECLARE @date datetime = &#39;2019-03-24&#39; SELECT DATEPART(YEAR, @date) AS year, DATENAME(YEAR, @date) AS year_name, DATEPART(MONTH, @date) AS month, DATENAME(MONTH, @date) AS month_name DATEFROMPARTS(year, month, day) Receives 3 parameters: year, month, and day values Generates a date SELECT YEAR(&#39;2019-03-05&#39;) AS date_year, MONTH(&#39;2019-03-05&#39;) AS date_month, DAY(&#39;2019-03-05&#39;) AS date_day, DATEFROMPARTS(YEAR(&#39;2019-03-05&#39;),MONTH(&#39;2019-03-05&#39;), DAY(&#39;2019-03-05&#39;)) AS reconstructed_date; Performing Arithmetic Operations On Dates Types of Operations With Dates Operations using arithmetic operators (+,-) Modify the value of a date - DATEADD() Return the difference between two dates - DATEDIFF() Arithmetic Operations DECLARE @date1 datetime = &#39;2019-01-01&#39;; DECLARE @date2 datetime = &#39;2020-01-01&#39;; SELECT @date2 + 1 AS add_one, @date2 -1 AS subtract_one, @date2 + @date1 AS add_dates, @date2 - @date1 AS subtract_date; DATEADD(datepart, number, date) Add or subtract a number of time units from a date SELECT first_name, birthdate, DATEADD(YEAR, 5, birthdate) AS fifth_birthday, DATEADD(YEAR,-5,birthdate) AS subtract_5years, DATEADD(DAY,30, birthdate) AS add_30days, DATEADD(Day, -30, birthdate) AS subtract_30days FROM voters; DATEDIFF(datepart, startdate, enddate) Returns the number of units between two dates SELECT first_name, birthdate, first_vote_date, DATEDIFF(YEAR, birthdate, first_vote_date) AS age_years, DATEDIFF(QUARTER, birthdate, first_vote_date) AS age_quarters, DATEDIFF(DAY, birthdate, first_vote_date) AS age_days, DATEDIFF(HOUR, birthdate, first_vote_date) AS age_hours FROM voters; Validating if an expression is a date ISDATE(expression) Determines whether an expression is a valid date data type ISDATE() expression Return Type date, time, datetime 1 datetime2 0 other type 0 ISDATE(expression) DECLARE @date1 NVARCHAR(20) = &#39;2019-05-05&#39; DECLARE @date2 NVARCHAR(20) = &#39;2019-01-XX&#39; DECLARE @date3 CHAR(20) = &#39;2019-05-05 12:45:59.9999999&#39; DECLARE @date4 CHAR(20) = &#39;2019-05-05 12:45:59&#39; SELECT ISDATE(@date1) AS valid_date, ISDATE(@date2) AS invalid_date, ISDATE(@date3) AS valid_datetime2, ISDATE(@date4) AS valid_datetime; SET DATEFORMAT SET DATEFORMAT {format} Sets the order of the date parts for interpreting strings as dates Valid formats: mdy, dmy, ymd, ydm, myd, dym DECLARE @date1 NVARCHAR(20) = &#39;12-30-2019&#39; DECLARE @date2 NVARCHAR(20) = &#39;30-12-2019&#39; SET DATEFORMAT dmy; SELECT ISDATE(@date1) AS invalid_dmy, ISDATE(@date2) AS valid_dmy; SET LANGUAGE SET LANGUAGE {language} Sets the language for the session Implicitly sets the setting of SET DATEFORMAT Valid languages: English, Italian, Spanish, etc. SET LANGUAGE English; SELECT ISDATE(&#39;12-30-2019&#39;) AS mdy, ISDATE(&#39;30-12-2019&#39;) AS dmy; " }, { "title": "Data Types and Conversion", "url": "/posts/5sql1/", "categories": "SQL, 5. Functions for Manipulating Data in SQL-SERVER", "tags": "sql, t-sql, data types and conversion, data type precedence, cast, convert", "date": "2022-05-17 00:00:00 +0545", "snippet": " Categories of Data Types Exact numerics Approximate numerics Date and Time Character Strings Unicode Character Strings Binary Strings Other data types Exact Numerics Whole Numbers smallint tinyint int bigint Decimal Numbers numeric decimal money smallmoney Exact numerics - integers Numbers without a decimal point Data Type Storage bigint 8 Bytes int 4 Bytes smallint 2 Bytes tinyint 1 Byte Exact numerics - decimals Approximate numerics Float Real Store approximate numeric values Date and Time Data Types Data Type Format Accuracy time hh:mm:ss[.nnnnnnn] 100 nanoseconds date YYYY-MM-DD 1 day smalldatetime YYYY-MM-DD hh:mm:ss 1 minute datetime YYYY-MM-DD hh:mm:ss[.nnn] 0.00333 second datetime2 YYYY-MM-DD hh:mm:ss[.nnnnnnn] 100 nanoseconds Character and Unicode Character Data Types Character data types store character strings (ASCII) char varchar text Unicode data types are used for storing Unicode data (non-ASCII) nchar nvarchar ntext Other data types binary image cursor rowversino uniqueidentifier xml Spatial Geometry / Geography Types Implicit Conversion Data Comparison Keep in mind : for comparing two values, they need to be of the same type. Otherwise: SQL Server converts from one type to another (IMPLICIT) The developer explicitly converts the data (EXPLICIT) Data Type Precedence user-defined data types (highest) datetime date float decimal int bit nvarchar (including nvarchar(max)) varchar (including varchar(max)) binary (lowest) Implicit conversion between data types Performance impact of implicit conversion Implicit conversion is done for each row of the query Implicit conversion can be prevended with a good database schema design. Explicit Conversion Implicit and Explicit Conversion IMPLICIT : performed automatically, behind the scenes EXPLICIT : performed with functions CAST() and CONVERT() CAST() and CONVERT() are used to convert from one data type to another CAST() Syntax: CAST(expression AS data_type [(length)]) SELECT CAST(3.14 AS INT) AS DECIMAL_TO_INT, CAST(&#39;3.14&#39; AS DECIMAL(3,2)) AS STRING_TO_DECIMAL, CAST(GETDATE() AS NVARCHAR(20)) AS DATE_TO_STRING, CAST(GETDATE() AS FLOAT) AS DATE_TO_FLOAT; CONVERT() Syntax: CONVERT(data_type [(length)], expression, [,style]) SELECT CONVERT(INT, 3.14) AS DECIMAL_TO_INT, CONVERT(DECIMAL(3,2), &#39;3.14&#39;) AS STRING_TO_DECIMAL, CONVERT(NVARCHAR(20), GETDATE(), 104) AS DATE_TO_STRING, CONVERT(FLOAT, GETDATE()) AS DATE_TO_FLOAT; CAST() vs CONVERT() CAST() comes from the SQL standard and CONVERT() is SQL Server specific CAST() is available in most database products CONVERT() performs slightly better in SQL Server " }, { "title": "Using Aggregation Functions Over Windows", "url": "/posts/4sql4/", "categories": "SQL, 4. Time Series Analysis With SQL Server", "tags": "sql, t-sql, row_number, rank, dense_rank, partition by, range, lag, lead, cte", "date": "2022-05-16 00:00:00 +0545", "snippet": "Using Aggregation Functions Over Windows Ranking Functions ROW_NUMBER(): Unique, ascending integer value starting from 1. RANK(): Ascending integer value starting from 1. Can have ties. Can skip numbers. DENSE_RANK(): Ascending integer value starting from 1. Can have ties. Will not skip numbers. Calculating Row Numbers SELECT s.RunsScored, ROW_NUMBER() OVER (ORDER BY s.RunsScored DESC) AS rn FROM dbo.Scores s ORDER BY s.RunsScored DESC; Calculating ranks and dense ranks SELECT s.Runscored, RANK() OVER (ORDER BY s.RunsScored DESC) AS rk, DENSE_RANK() OVER (ORDER BY s.RunsScored DESC) AS dr FROM dbo.Scores s ORDER BY s.RunsScored DESC; Partitions SELECT s.Team, s.RunsScored, ROW_NUMBER() OVEr (PARTITION BY s.Team ORDER BY s.RunsScored DESC) AS rn FROM dbo.Scores s ORDER BY s.RunsScored DESC: Alaises for Multiple Window Functions SELECT id, account_id, standard_qty, DATE_TRUNC(&#39;month&#39;, occurred_at) AS month, DENSE_RANK() OVER (PARTITION BY account_id ORDER BY DATE_TRUNC(&#39;month&#39;, occurred_at)) AS dense_rank, SUM(standard_qty) OVER main_window AS sum_standard_qty, COUNT(standard_qty) OVER main_window AS count_standard_qty, AVG(standard_qty) OVER main_window AS avg_standard_qty, MIN(standard_qty) OVER main_window AS min_standard_qty, MAX(standard_qty) OVER main_window AS max_standard_qty FROM demo.orders WINDOW main_window AS (PARTITION BY account_id ORDER BY DATE_TRUNC(&#39;month&#39;, occurred_at)) Aggregate Functions SELECT s.Team, s.RunsScored, MAX(s.RunsScored) OVER (PARTITION BY s.Team) AS MaxRuns FROM dbo.Scores s ORDER By s.RunsScored DESC; Aggregations with Empty Windows SELECT s.Team, s.RunsScored, MAX(s.RunsScored) OVER() AS MaxRuns FROM dbo.Scores s ORDER BY s.RunsScored DESC; Calculating Running Totals and Moving Averages Running Totals SELECT s.Team, s.Game, s.RunsScored, SUM(s.RunsScored) OVER(PARTITION BY s.Team ORDER BY s.Game ASC RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) AS TotalRuns FROM #Scores s; RANGE and ROWS RANGE Specify a range of results “Duplicates” processed all at once Only supports UNBOUNDED and CURRENT ROW ROWS Specify number of rows to include “Duplicates” processed a row at a time Supports UNBOUNDED, CURRENT ROW, and number of rows Calculating Moving Averages SELECT s.Team, s.Game, s.RunsScored, AVG(s.RunsScored) OVER (PARTITION BY s.Team ORDER BY s.Game ASC ROWS BETWEEN 1 PRECEDING AND CURRENT ROW) AS AvgRuns From #Scores s; Working with LAG() and LEAD() The LAG() window function SELECT dsr.CustomerId, dsr.MonthStartDate, LAG(dsr.NumberOfVisits) OVER(PARTITION BY dsr.CustomerID ORDER BY dsr.MonthStartDate) AS Prior, dsr.NumberOfVisits FROM dbo.DaySpaRollup dsr; The LEAD() Window Function SELECT dsr.CustomerId, dsr.MonthStartDate, dsr.NumberOfVisits, LEAD(dsr.NumberOfVisits) OVER (PARTITION BY dsr.CustomerID ORDER BY dsr.MonthStartDate) AS Next FROM dbo.DaySpaRollup dsr; Specifying number of rows back SELECT dsr.CustomerId, dsr.MonthStartDate, LAG(dsr.NumberOfVisits, 2) OVER (PARTITION BY dsr.CustomerID ORDER BY dsr.MonthStartDate) AS Prior2, LAG(dsr.NumberOfVisits,1) OVER (PARTITION BY dsr.CustomerID ORDER BY dsr.MonthStartDate) AS Prior1, dsr.NumberOfVisits FROM dbo.DaySpaRollup dsr; SELECT Date, LAG(Val,1) OVER (ORDER BY DATE) AS PriorVal, Val FROM t; SELECT Date, LAG(Val,1) OVER (ORDER BY DATE) As PriorVal, Val FROM t WHERE t.Dat &amp;gt; &#39;2019-01-02&#39;; Windows and filters and CTEs WITH records AS ( SELECT Date, LAG(Val,1) OVER (ORDER BY Date) AS PriorVal, VAL FROM t ) SELECT r.Date, r.PriorVal, r.Val FROM records r WHERE r.Date &amp;gt; &#39;2019-01-02&#39;; Finding Maximum levels of overlap …." }, { "title": "Basic Aggregate Functions", "url": "/posts/4sql3/", "categories": "SQL, 4. Time Series Analysis With SQL Server", "tags": "sql, t-sql, count, count_big, count(distinct), sum, min, max, distince, case, avg, stdev, stdevp, var, varp, downsampling, upsampling, rollup, grouping sets", "date": "2022-05-16 00:00:00 +0545", "snippet": " Key Aggregation Functions Counts COUNT() COUNT_BIG() COUNT(DISTINCT) Other Aggregates SUM() MIN() MAX() What counts with COUNT() Number of Rows COUNT(*) COUNT(1) COUNT(1/0) Non-NULL Values COUNT(d.YR) COUNT(NULLIF(d.YR, 1990)) Distinct Counts SELECT COUNT(DISTINCT c.CalendarYear) AS Years, COUNT(DISTINCT NULLIF(c.CalendarYear, 2010)) AS Y2 FROM dbo.Calendar c; Filtering aggregates with CASE SELECT MAX(CASE WHEN ir.IncidentTypeId = 1 THEN ir.IncidentDate ELSE NULL END) AS I1, MAX(CASE WHEN ir.IncidentTypeID = 2 THEN ir.IncidentDate ELSE NULL END) AS I2, FROM dbo.IncidentRollup ir; Statistical Aggregate Functions AVG() : Mean STDEV(): Standard Deviation STDEVP(): Population Stadard Deviation VAR(): Variance VARP(): Population Variance What about median? SELECT TOP(1) PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY l.SomeVal DESC) OVER () AS MedianIncidents FROM dbo.LargeTable l; The cost of median   Median Mean Est. Cost 95.7% 4.3% Duration 68.5s 0.37s CPU 68.5s 8.1s Reads 72,560,946 39,468 Writes 87,982 0 Downsampling and Upsampling Data Data in nature SELECT SomeDate FROM dbo.SomeTable Downsampling Data SELECT CAST(SomeDate AS DATE) AS SomeDate FROM dbo.SomeTable Further Downsampling SELECT DATEADD(HOUR, DATEDIFF(HOUR,0,SomeDate), 0) AS SomeDate FROM dbo.SomeTable DATEDIFF(HOUR,0,&#39;2019-08-11 06:21:16&#39;) = 1,048,470 DATEADD(HOUR&amp;lt; 1048748,0) = 2019-08-11 06:00:00 What about upsampling? Downsampling Upsampling Aggregate data Disaggregate data Can usually sum or coutn results Need an allocation rule Provides a higher-level picture of the data Provides artifical granularity Acceptable for most purposes Acceptable for data generation, calculated averages Grouping by ROLLUP, CUBE, and GROUPING SETS Hierarchial rollups with ROLLUP SELECT t.Month, t.Day, SUM(t.Events) AS Events FROM TABLE GROUP BY t.Month, t.Day WITH ROLLUP ORDER BY t.Month, t.Day; Cartesian aggregation with CUBE SELECT t.IncidentType, t.Office, SUM(t.Events) AS Events FROM TABLE GROUP BY t.IncidentType, t.Office WITH ROLLUP ORDER BY t.IncidentType, t.Office; Define grouping sets with GROUPING SETS SELECT t.IncidenType, t.Office, SUM(t.Events) AS Events FROM TABLE GROUP BY GROUPING SETS ( (t.IncidentType, t.Office), () ) ORDER BY t.IncidentType, t.Office; " }, { "title": "Basic Date From Parts", "url": "/posts/4sql2/", "categories": "SQL, 4. Time Series Analysis With SQL Server", "tags": "sql, t-sql, datefromparts, timefromparts, datetimefromparts, datetime2fromparts, smalldatetimefromparts, datetimeoffsetfromparts, set language, switchoffset, todatetimeoffset, try_cast, try_convert, try_parse", "date": "2022-05-16 00:00:00 +0545", "snippet": " Dates from parts DATEFROMPARTS(year,month,day) TIMEFROMPARTS(hour, minute, second, fraction, precision) DATETIMEFROMPARTS(year, month, day, hour, minute, seconds,ms) DATETIME2FROMPARTS(year, month, day, hour, minute, second, fraction, precision) SMALLDATETIMEFROMPARTS(year, month, day, hour, minute) DATETIMEOFFSETFROMPARTS(year, month, day, hour, minute, second, fraction, hour_offset, minute_offset, precision) Dates and times together SELECT DATETIMEFROMPARTS(1918,11,11,05,45,17,995) AS DT, DATETIME2FROMPARTS(1918,11,11,05,45,17,0,0) AS DT20, DATETIME2FROMPARTS(1918,11,11,05,45,17,995,3) AS DT23, DATETIME2FROMPARTS(1918,11,11,05,45,17,9951234,3) AS DT27; Working with Offets SELECT DATETIMEOFFSETFROMPARTS(2009,08,14,21,00,00,0,5,30,0) AS IST, DATETIMEOFFSETFROMPARTS(2009,08,14,21,00,00,0,5,30,0) AT TIME ZONE &#39;UTC&#39; AS UTC; Translating date strings Casting strings SELECT CAST(&#39;09/14/99&#39; AS DATE) AS USDate; Converting Strings SELECT CONVERT(DATETIME2(3), &#39;April 4, 2019 11:52:29.998 PM&#39;) AS April14; Parsing Strings SELECT PARSE(&#39;25 Dezember 2014&#39; AS DATE USING &#39;de-de&#39;) AS Wihnachten; The cost of parsing Function Conversions Per Second CONVERT() 251,997 CAST() 240,347 PARSE() 12,620 Setting Languages SET LANGUAGE &#39;FRENCH&#39; DECLARE @FrenchDate NVARCHAR(30) = N&#39;18 avril 2019&#39;, @FrenchNumberDate NVARCHAR(30) = N&#39;18/4/2019&#39;; SELECT CAST(@FrenchDate AS DATETIME), CAST(@FrenchNumberDate AS DATETIME); Working With Offsets Anatomy of a DATETIMEOFFSET Components Date Part Example Date 2019-04-10 Time 12:59:02.3908505 UTC Offset -04:00 Changing Offsets DECLARE @SomeDate DATETIMEOFFSET = &#39;2019-04-10 12:59:02.3908505 -04:00&#39;; SELECT SWITCHOFFSET(@SomeDate, &#39;-07:00&#39;) AS LATime; Converting to DATETIMEOFFSET DECLARE @SomeDate DATETIME2(3) = &#39;2019-04-10 12:59:02.390&#39;; SELECT TODATETIMEOFFSET(@SomeDate, &#39;-04:00&#39;) AS EDT; Time Zone Swaps with TODATETIMEOFFSET DECLARE @SomeDate DATETIME2(3) = &#39;2016-09-04 02:28:29.681&#39;; SELECT TODATETIMEOFFSET( DATEADD(HOUR, 7, @SomeDate), &#39;_02:00&#39;) AS BonnTime; Discovering time zones SELECT tzi.name, tzi.current_utc_offset, tzi.is_currently_dst FROM sys.time_zone_info tzi WHERE tzi.name LIKE &#39;%Time Zone%&#39;; Handling Invalid Dates Error-safe date conversion functions “Unsafe” Functions CAST() CONVERT() PARSE() “Safe” Functions TRY_CAST() TRY_CONVERT() TRY_PARSE() SELECT PARSE(&#39;01/13/2019&#39; AS DATE USING &#39;en-us&#39;) AS January13US, PARSE(&#39;01/13/2019&#39; AS DATE USING &#39;fr-fr&#39;) AS Smarch1FR; The above sql statements returns as error while converting string value ‘01/13/2019’ into data type date using culture ‘fr-fr’. SELECT TRY_PARSE(&#39;01/13/2019&#39; AS DATE USING &#39;en-us&#39;) AS January13US, TRY_PARSE(&#39;01/13/2019&#39; AS DATE USING &#39;fr-fr&#39;) AS Smarch1FR; The above sql statements will not return an error while converting string value ‘01/13/2019’ into data type date using culture ‘fr-fr’ but instead return NULL as the value for Smarch1FR " }, { "title": "Building Dates", "url": "/posts/4sql1/", "categories": "SQL, 4. Time Series Analysis With SQL Server", "tags": "sql, t-sql, getdate, getutcdate, sysdatetime, sysutcdatetime, datepart, datename, dateadd, datediff, cast, convert, format", "date": "2022-05-16 00:00:00 +0545", "snippet": "Building Dates Building a Date SELECT GETDATE() AS DateTime_LTz, GETUTCDATE() AS DateTime_UTC; SELECT SYSDATETIME() AS DateTime2_LTz, SYSUTCDATETIME() AS DateTime2_UTC; Breakding down a date DECLARE @SomeDate DATETIME(3) = &#39;2019-03-01 08:17:19.332&#39;; SELECT YEAR(@SomeDate); SELECT MONTH(@SomeDate); SELECT DAY(@SomeDate); The above sql statements returns YEAR = 2019, MONTH =3, and DAY = 1 Parsing Dates with Date Parts Parts Year / Month / Day Day of year Day of week Week of year ISO week of year Minute / Second Millisecond / Nanosecond Functions DATEPART() SELECT DATEPART(YEAR, @dt) AS TheYear; DATENAME() SELECT DATENAME(MONTH, @dt) AS TheMonth; Adding and Subtracting dates DECLARE @SomeTime DATETIME2(7) = &#39;1992-07-14 14:49:36.2294852&#39;; SELECT DATEADD(DAY, 1, @SomeTime) AS NextDay, DATEADD(Day, -1, @SomeTime) AS PriorDay; SELECT DATEADD(HOUR, -3, DATEADD(DAY,-4, @SomeTime)) AS Minus4Day3Hours; Comparing Dates DECLARE @StartTime DATETIME2(7) = &#39;2012-03-01 14:29:36&#39;, @EndTime DATETIME2(7) = &#39;2012-03-01 18:00:00&#39;; SELECT DATEDIFF(SECOND, @StartTime, @EndTime) AS SecondsElapsed, DATEDIFF(MINUTE, @StartTime, @EndTIme) AS MinutesElapsed, DATEDIFF(HOUR, @StartTime, @EndTime) AS HoursElapsed; Formatting Dates For Reporting Formatting Functions CAST() CONVERT() FORMAT() The CAST() function Useful for converting one data type to another data type, including date types No control over formatting dates to strings ANSI SQL Standard, meaning any relational and most non-relational databses have this function Using the CAST() function DECLARE @SomeDate DATETIME2(3) = &#39;1991-06-04 08:00:09&#39;, @SomeString NVARCHAR(30) = &#39;1991-06-04 08:00:09&#39;, @OldDateTime DATETIME = &#39;1991-06-04 08:00:09&#39;; SELECT CAST(@SomeDate AS NVARCHAR(30)) AS DateToString, CAST(@SomeString AS DATETIME2(3)) AS StringToDate, CAST(@OldDateTime AS NVARCHAR(30)) AS OldDateToString; The CONVERT() function Useful for converting one data type to another data type, including date types Some control over formattting from dates to strings using the style parameter Specific to T-SQL DECLARE @SomeDate DATETIME2(3) = &#39;1793-02-21 11:13:19.033&#39;; SELECT CONVERT(NVARCHAR(30), @SomeDate,0) AS DefaultForm, CONVERT(NVARCHAR(30), @SomeDate,1) AS UD_mdy, CONVERT(NVARCHAR(30), @SomeDate, 101) AS US_mdyyyy, CONVERT(NVARCHAR(30), @SomeDate, 120) AS ODBC_sec; Sample CONVERT() styles Style Code Format 1 / 101 United States m/d/y 3 / 103 British/French d/m/y 4 / 104 German d.m.y 11 / 111 Japanese y/m/d 12 / 112 ISO Standard yyyymmdd 20 / 120 ODBC Standard (121 for ms) 126 ISO8601 yyyy-mm-dd hh:mi:ss.mmm 127 yyyy-mm-ddThh:mi:ss.mmmZ The FORMAT() function Useful for formatting a date or number in a particular way for reporting Much more flexibility over formatting from dates to strings than either CAST() or CONVERT() Specific to T-SQL Can be slower as you process more rows DECLARE @SomeDate DATETIME2(3) = &#39;1793-02-21 11:13:19.033&#39;; SELECT FORMAT(@SomeDate, &#39;d&#39;, &#39;en-US&#39;) AS US_d, FORMAT(@SomeDate, &#39;d&#39;, &#39;de-DE&#39;) AS DE_d, FORMAT(@SomeDate, &#39;D&#39;, &#39;de-DE&#39;) AS DE_D, FORMAT(@SomeDate, &#39;yyyy-MM-dd&#39;) AS yMd, Working With Calendar Tables A calendar table is a permanent table containing a list of dates and various components of those dates. SELECT * FROM dbo.Calendar; Contents of a calendar table Building a Calendar Table CREATE TABLE dbo.Calendar ( DateKey INT NOT NULL, [Date] DATE NOT NULL, [Day] TINYINT NOT NULL, DayOfWeek TINYINT NOT NULL, DayName VARCHAR(10) NOT NULL, .... ) SELECT CAST(D.DateKey AS INT) AS DateKey, D.[DATE] AS [DATE], CAST(D.[day] AS TINYINT) AS [day], CAST(d.[dayofweek] AS TINYINT) AS [DayOfWeek], CAST(DATENAME(WEEKDAY, d.[Date]) AS VARCHAR(10)) AS [DayName], .... Using a Calendar Table SELECT c.Date FROM dbo.Calendar c WHERE c.MonthName = &#39;April&#39; AND c.DayName = &#39;Saturday&#39; AND c.CalendarYear = 2020 ORDER BY c.Date; A quick note on APPLY() SELECT FYStart = DATEADD(MONTH, -6, DATEADD(YEAR, DATEDIFF(YEAR, 0, DATEADD(MONTH, 6, d.[date])), 0)), FiscalDayOfYear = DATEDIFF(DAY, DATEADD(MONTH, -6, DATEADD(YEAR, DATEDIFF(YEAR, 0, DATEADD(MONTH, 6, d.[date])), 0)), d.[Date]) + 1, FiscalWeekOfYear = DATEDIFF(WEEK, DATEADD(MONTH, -6, DATEADD(YEAR, DATEDIFF(YEAR,0, DATEADD(MONTH,6,d.[date])), 0)), d.[Date]) +1 FROM dbo.Calendar d; SELECT fy.FYStart, FiscalDayOfYear = DATEDIFF(DAY, fy.FYStart, d.[Date])+1, FiscalWeekOfYear = DATEDIFF(WEEK, fy.FYStart, d.[Date]) +1 FROM dbo.Calendar d CROSS APPLY ( SELECT FYStart = DATEADD(MONTH, -6, DATEADD(YEAR, DATEDIFF(YEAR,0, DATEADD(MONTH, 6, d.[date])),0)) ) fy; " }, { "title": "Window Functions", "url": "/posts/3sql4/", "categories": "SQL, 3. Intermediate to SQL Server", "tags": "sql, t-sql, window functions, over, partition by, order by, first_value, last_value, lead, lag, row_number, stdev", "date": "2022-05-16 00:00:00 +0545", "snippet": " Grouping data in T-SQL SELECT SalesPerson, SalesYear, CurrentQuota, ModifiedDate FROM SaleGoal WHERE SalesYear = 2011 Window syntax in T-SQL Create the window with OVER caluse PARTITION BY creates the frame If you do not include PARTITION BY the frame is the entire table To arrange the results, use ORDER BY Allows aggregations to be created at the same time as the window -- Create a Window Data Grouping OVER (PARTITION BY SalesYear ORDER BY SalesYear) Window Functions (SUM) SELECT SalesPerson, SalesYear, CurrentQuota, SUM(CurrentQuota) OVER (PARTITION BY SalesYear) AS YearlyTotal, ModifiedDate AS ModDate FROM SaleGoal Window Functions (COUNT) SELECT SalesPerson, SalesYear, CurrentQuota, COUNT(CurrentQuota) OVER (PARTITION BY SalesYear) AS QuotaPerYear, ModifiedDate as ModDate FROM SaleGoal Common Window Functions FIRST_VALUE() and LAST_VALUE() FIRST_VALUE() returns the first value in the window LAST_VALUE() returns the last value in the window Note that for FIRST_VALUE and LAST_VALUE the ORDER BY command is required -- Select the columns SELECT SalesPerson, SalesYear, CurrentQuota, -- First value from every window FIRST_VALUE(CurrentQuota) OVER (PARTITION BY SalesYear ORDER BY ModifiedDate) AS StartQuota, -- Last value from every window LAST_VALUE(CurrentQuota) OVER (PARTITION BY SalesYear ORDER BY ModifiedDate) AS EndQuota, ModifiedDate as ModDate FROM SaleGoal Gettting the next value with LEAD() Provides the ability to query the value from the next row NextQuota Column is created by using LEAD() Requires the use of ORDER BY to order the rows SELECT SalesPerson, SalesYear, CurrentQuota, -- Create a window function to get the values from the next row LEAD(CurrentQuota) OVER (PARTITION BY SalesYear ORDER BY ModifiedDate) AS NextQuota, ModifiedDate AS ModDate FROM SaleGoal Getting the previous value with LAG() Provides the ability to query the value from the previous row PreviousQuota Column is created by using LAG() Requires the use of ORDER BY to order the rows SELECT SalesPerson, SalesYear, CurrentQuota, -- Create a window function to get the values from the previous row LAG(CurrentQuota) OVER (PARTITION BY SalesYear ORDER BY ModifiedDate) AS PreviousQuota, ModifiedDate AS ModDate FROM SaleGoal Increasing Window Complexity Reviewing Aggregations SELECT SalesPerson, SalesYear, CurrentQuota, SUM(CurrentQuota) OVER (PARTITION BY SalesYear) AS YearlyTotal, ModifiedDate AS ModDate FROM SaleGoal Adding ORDER BY to an aggregation SELECT SalesPerson, SalesYear, CurrentQuota, SUM(CurrentQuota) OVER (PARTITION BY SalesYear ORDER BY SalesPerson) AS YearlyTotal, ModifiedDate AS ModDate FROM SaleGoal Creating a running total with ORDER BY SELECT SalesPerson, SalesYear, CurrentQuota, SUM(CurrentQuota) OVER (PARTITION BY SalesYear ORDER BY ModifiedDate) AS RunningTotal, ModifiedDate AS ModDate FROM SaleGoal Adding row numbers ROW_NUMBER()sequentially numbers the rows in the window ORDER BY is required when using ROW_NUMBER() SELECT SalesPerson, SalesYear, CurrentQuota, ROW_NUMBER() OVER (PARTITION BY SalesPerson ORDER BY SalesYear) AS QuotabySalesPerson FROM SaleGoal Using Windows For Calculating Statistics Calculating the Standard Deviation Calculate standard deviation either for the entire table or for each window STDEV() calculates the standard deviation Calculating the standard deviation for the entire table SELECT SalesPerson, SalesYear, CurrentQuota, STDEV(CurrentQuota) OVER() AS StandardDev, ModifiedDate as ModDate FROM SaleGoal Calculating the standard deviation for each partition SELECT SalesPerson, SalesYeaer, CurrentQuota, STDEV(CurrentQuota) OVER (PARTITION BY SalesYear ORDER BY SalesYear) AS StDev, ModifiedDate AS ModDate FROM SaleGoal Calculating the Mode Mode is the value which appears the most often in your data To calculate mode: Create a CTE containing an ordered count of values using ROW_NUMBER Write a query using the CTE to pick the value with the highest row number WITH QuotaCount AS ( SELECT SalesPerson, SalesYear, CurrentQuota, ROW_NUMBER() OVER (PARTITION BY CurrentQuota ORDER BY CurrentQuota) AS QuotaList FROM SaleGoal ) SELECT CurrentQuota, QuotaList AS Mode FROM QuotaCount WHERE QuotaList In (SELECT MAX(QuotaList) FROM QuotaCount) Advanced Window Functinos " }, { "title": "Loops", "url": "/posts/3sql3/", "categories": "SQL, 3. Intermediate to SQL Server", "tags": "sql, t-sql, declare, set, loops, while, derived tables, sub query, common table expressions, cte", "date": "2022-05-16 00:00:00 +0545", "snippet": " Using Variables in T-SQL Variables are needed to set values DECLARE @variablename data_type Must start with the character @ Variable data types in T-SQL VARCHAR(n): variable length text field INT: integer values from -2,147,483,647 to +2,147,483,647 DECIMAL(p,s) or NUMERIC(p,s): p: total number of decimal digits that will be stored, both to the left and to the right of the decimal point s: number of decimal digits that will be stored to the right of the decimal point Declaring Variables in T-SQL Declare Snack as a VARCHAR with length 10 DECLARE @Snack VARCHAR(10) Assinging values to variables Use SET to set a value to the variable SET @Snack = &#39;Cookies&#39; Show the value SELECT @Snack WHILE Loops WHILE evalues a true or false condition After the WHILE, there should be a line with the keyword BEGIN Next include code to run until the condition in the WHILE loop is true After the code add the keyword END BREAK will cause an exit out of the loop CONTINUE will cause the loop to continue -- Declare ctr as an integer DECLARE @ctr INT --Assign 1 to ctr SET @ctr = 1 -- Specify the condition of the WHILE LOOP WHILE @ctr &amp;lt; 10 -- Begin the code to execute inside the WHILE loop BEGIN --keep incrementing the value of @ctr SET @ctr = @ctr + 1 --End WHILE loop END -- VIew the value after the loop SELECT @ctr -- Declare ctr as an integer DECLARE @ctr INT -- Assign 1 to cr SET @ctr = 1 -- Specify the condition of the WHILE LOOP WHILE @ctr &amp;lt; 10 -- Begin the code to execute inside while loop BEGIN -- Keep incrementing the value of @ctr SET @ctr = @ctr + 1 -- Check if ctr is equal to 4 IF @ctr = 4 -- When ctr is equal to 4, the loop will break BREAK -- End WHILE loop END Derived Tables Query which is treated like a temporary table Always contained within the main query They are specified in the FROM clause Can contain intermediate calculations to be used the main query or different joins than in the main query SELECT a.* FROM Kidney a JOIN ( SELECT AVG(Age) AS AverageAge FROM Kidney ) b ON a.Age = b.AverageAge Common Table Expressions (CTE) A Common Table Expression (CTE) is the result set of a query which exists temporarily and for use only within the context of a larger query. Much like a derived table, the result of a CTE is not stored and exists only for the duration of the query. This article will focus on non-recurrsive CTEs. Syntax: -- CTE definitions start with the keyword WITH -- Followed by the CTE names and the columns it contains WITH CTEName (Col1, Col2) AS -- Defint the CTE Query ( -- The two columns from the definition above SELECT Col1, Col2 FROM TableName ) -- Create a CTE to get the Maximum BloodPressure by Age WITH BloodPressueAge(Age, MaxBloodPressue) AS ( SELECT Age, MAX(BloodPressure) AS MaxBloodPressure FROM Kidney GROUP BY Age ) -- Create a query to use the CTE as a table SELECT a.Age, MIN(a.BloodPressure), b.MaxBloodPressure FROM Kidney a -- Join the CTE with the table JOIN BloodpressureAge b ON a.Age = b.Age GROUP BY a.Age, b.MaxBloodPressure " }, { "title": "Counts and Totals", "url": "/posts/3sql2/", "categories": "SQL, 3. Intermediate to SQL Server", "tags": "sql, t-sql, count, distinct, datepart, dateadd, datediff, round, abs, sqrt, square, log", "date": "2022-05-15 00:00:00 +0545", "snippet": " Examining Totals with Counts SELECT COUNT(*) FROM Incidents The above sql statments returns the total number of rows in the Incidents table. Count with DISTINCT SELECT COUNT(DISTINCT Country) AS Countries FROM Incidents The above sql statements returns the count for the number of unique country value from the Country field in Incidents table. Count Aggregations GROUP BY can be used with COUNT() in the same way as the other aggregation functions such as AVG(), MIN(), MAX() Use the ORDER BY command to sort the vaues ASC will return the smallest values first (default) DESC will return the largest values first COUNT With GROUP BY Count the rows, subtotaled by Country SELECT COUNT(*) AS TotalRowsByCountry, Country FROM Incidents GROUP BY Country COUNT With GROUP BY and ORDER BY Count the rows, subtotaled by Country SELECT COUNT(*) AS TotalRowsByCountry, Country FROM Incidents GROUP BY Country ORDER BY Country ASC Column totals with SUM SUM() provides numeric total of the values in a column It follows the same pattern as other aggregations Combine it with GROUP BY to get subtotals based on columns specified Adding Column Values Calculate the value subtotaled by Country SELECT SUM(DurationSeconds) AS TotalDuration, Country FROM Incidents GROUP BY Country Math with Dates DATEPART DATEPART is used to determine what part of the date you want to calculate. Some of the common abbreviations are: DD for Day MM for Month YY for Year HH for Hour Common data functions in T-SQL are: DATEADD(): Add or subtract datetime values Always returns a date DATEDIFF(): Obtain the difference between two datetime values Always returns a number DATEADD To Add or Subtract a value to get a new date use DATEADD() Syntax: DATEADD(DATEPART, number, date) DATEPART: Unit of measurement (DD, MM, etc.) number: An integer value to add date: A datetime value Date math with DATEADD What date is 30 days from June 21, 2020? SELECT DATEADD(DD, 30, &#39;2020-06-21&#39;) The above sql returns the date ‘2020-07-21’ What date is 30 days before June 21, 2020? SELECT DATEADD(DD, -30, &#39;2020-06-21&#39;) The above sql returns the date ‘2020-05-22’ DATEDIFF Returns a date after a number has been added or subtracted to a date Syntax: DATEDIFF(datepart, startdate, enddate) datepart: Unit of measurement (DD,MM,etc.) startdate: The starting date value enddate: An ending datetime value SELECT DATEDIFF(DD, &#39;2020-05-22&#39;, &#39;2020-06-21&#39;) AS Difference1, DATEDIFF(DD, &#39;2020-07-21&#39;, &#39;2020-06-21&#39;) AS Difference2, The above sql statement referes the difference in day between stardate and enddate provided. Rouding numbers Syntax: ROUND(number, length [, function]) SELECT DurationSeconds, ROUND(DurationSeconds, 0) AS RoundToZero, ROUND(DurationSeconds, 1) AS RoundToOne FROM Incidents Rounding on the left side of the decimal SELECT DurationSeconds, ROUND(DurationSeconds, -1) AS RoundToTen, ROUND(DurationSeconds, -2) AS RoundToHundred FROM Incidents Truncating with ROUND() The ROUND() function can be used to truncate values when you specify thethird argument Syntax: ROUND(number, length, [, function]) Set the third value to anon-zero number SELECT Profit, ROUND(DurationSeconds,0) AS RoundingtoWhole, ROUND(DurationSeconds, 0,1) AS Truncating FROM Incidents Math Functions Absolute Value Syntax: ABS(number) - User ABS() to return non-negative values SELECT ABS(-2.77), ABS(3), ABS(-2) SELECT DurationSeconds, ABS(DurationSeconds) AS AbsSeconds FROM Incidents Squares and Square Roots SELECT SQRT(9) AS Sqrt, SQUARE(9) AS Square Logs LOG() returns the natural logarithm Optionally, you can set the base, which if not set is 2.718281828 Syntax: LOG(number, [, BASE]) SELECT DurationSeconds, LOG(DurationSeconds, 10) AS LogSeconds FROM Incidents Log of 0 You cannot take the log of 0 as it will give you an error SELECT LOG(0,10) " }, { "title": "Intermediate To SQL Server", "url": "/posts/3sql1/", "categories": "SQL, 3. Intermediate to SQL Server", "tags": "sql, t-sql, min, max, avg, group by, having, missing values, is null, is not null, coalesce, case", "date": "2022-05-15 00:00:00 +0545", "snippet": " Exploring Data With Aggregation Reviewing summarized values for each column is a common first step in analyzing data If the data exists in a database, fastest way to aggregate is to use SQL Common Summary Statistics MIN() for the minimum value of a column MAX() for the maximum value of a column AVG() for the mean or average value of a column Example: SELECT AVG(InternetUse) AS MeanInternetUse, MIN(InternetUse) AS MINInternet, MAX(InternetUse) AS MAXInternet FROM EconomicIndicators Filtering Summary Data with Where This SQL query filters the aggregated values using a WHERE caluse SELECT AVG(InternetUse) AS MeanInternetUse, MIN(InternetUse) AS MINInternet, MAX(InternetUse) AS MAXInternet FROM EconomicIndicators WHERE Country = &#39;Solomon Islands&#39; Subtotaling Aggregations into Groups with GROUP BY SELECT Country, AVG(InternetUse) AS MeanInternetUse, MIN(InternetUse) AS MINInternet, MAX(InternetUse) AS MAXInternet FROM EconomicIndicators GROUP BY Country HAVING is the WHERE for Aggregations Cannot use WHEREwith GROUP BY as it will give errorThis throws as error ... GROUP BY WHERE MAX(InternetUse) &amp;gt; 100 Instead, use HAVINGThis is how you filter with a GROUP BY GROUP BY HAVING MAX(InternetUse) &amp;gt; 100 Example: SELECT Country, AVG(InternetUse) AS MeanInternetUse, MIN(GDP) AS SmallestGDP, MAX(InternetUse) AS MAXInternetUse FROM EconomicIndicators GROUP BY Country HAVING MAX(InternetUse) &amp;gt; 100 Finding and Resolving Missing Data Detecting Missing Values When you have no data, the empty database field contains the word NULL Because NULL is not a number, it is not possible to use =,&amp;lt;,&amp;gt; to find or compare missing values To determine if a column contains a NULL value, use IS NULL and IS NOT NULL Returning No NULL Values in T-SQL SELECT Country, InternetUse, Year FROM EconomicIndicators WHERE InternetUse IS NOT NULL The above SQL query returns records where the value is InternetUse field is not NULL Detecting NULLs in T-SQL SELECT Country, InternetUse, Year FROM EconomicIndicators WHERE InternetUse IS NULL The above SQL query returns records where the value is InternetUse field is NULL Blank Is Not Null A blank is not the same as a NULL value May show up in columns containing text An empty string &#39;&#39; can be used to find blank values The best way is to look for a column where the Length or LEN &amp;gt; 0 SELECT Country, GDP, Year FROM EconomicIndicators WHERE LEN(GDP) &amp;gt; 0 Substituting missing data with a specific value using ISNULL SELECT GDP, Country, ISNULL(Country, &#39;Unknown&#39;) AS NewCountry FROM EconomicIndicators The above SQL query returns a new column called NewCountry which has all the same values as the Country field but the only difference being every NULL value is replaced by Unknown keyword. Substituting missing data with a column using ISNULL Substituting values from one column or another with ISNULL SELECT TradeGDPPercent, ImportGoodPercent, ISNULL(TradeGDPPercent,ImportGoodPercent) AS NewPercent FROM EconomicIndicators Substituting NULL values using COALESCE COALESCE returns the first non-missing value Syntax: COALESCE(value_1, value_2, value_3,….,value_n) If value_1 is NULL and value_2 is not NULL, return value_2 If value_1 and value_2 are NULL and value_3 is not NULL, return value_3 Example: SELECT TradeGDPPercent, ImportGoodPercent COALESCE(TradeGDPPercent, ImportGoodPercent, &#39;N/A&#39;) AS NewPercent FROM EconomicIndicators Changing column values with CASE Syntax: CASE WHEN Boolean_expression THEN result_expression [ …n ] [ ELSE else_result_expression ] END Changing column values with CASE IN T-SQL SELECT Continent CASE WHEN Continent = &#39;Europe&#39; or Continent = &#39;Asia&#39; THEN &#39;Eurasia&#39; ELSE &#39;Other&#39; END AS NewContinent FROM EconomicIndicators Using CASE statements to create value groups We are binning the data here into discrete groups SELECT Country, LifeExp, CASE WHEN LifeExp &amp;lt; 30 Then 1 WHEN LifeExp &amp;gt; 29 AND LifeExp &amp;lt; 40 THEN 2 WHEN LifeExp &amp;gt; 39 AND LifeExp &amp;lt; 50 THEN 3 WHEN LifeExp &amp;gt; 49 AND LifeExp &amp;lt; 60 THEN 4 ELSE 5 END AS LifeExpGroup FROM EconomicIndicators WHERE Year = 2007 " }, { "title": "Model 1:N relationships with foreign keys", "url": "/posts/2sql4/", "categories": "SQL, 2. Introduction to Relational Database", "tags": "sql, t-sql, foreign keys, references, referential integrity violations, no action, cascade, restrict, set null, set default", "date": "2022-05-15 00:00:00 +0545", "snippet": " Implementing relationships with Foreign Keys A Foreign Key (FK) points to the Primary key (PK) of another table. Domain of FK must be equal to the domain of PK Each value of FK must exist in PK of the other table (FK constraint or ‘REFERENTIAL INTEGRITY) FKs are not actual keys. Specifying Foreign Keys CREATE TABLE manufacturers( name varchar(255) PRIMARY KEY ); INSERT INTO manufacturers VALUES (&#39;Ford&#39;),(&#39;VW&#39;), (&#39;GM&#39;); CREATE TABLE cars( model varchar(255) PRIMARY KEY, manufacturer_name varchar(255) REFERENCES manufacturers (name) ); INSERT INTO cars VALUES (&#39;Ranger&#39;, &#39;Ford&#39;), (&#39;Beetle&#39;, &#39;VW&#39;); Specifying Foreign Keys to Existing Tables ALTER TABLE a ADD CONSTRAINT a_fkey FOREIGN KEY (b_id) REFERENCES b (id); How to Implement N:M-relationships Create a table Add foreign keys for every connected table Add additional attributes CREATE TABLE affiliations ( professor_id integer REFERENCES professors (id), organization_id varchar(256) REFERENCES organizations (id), functino varchar(256) ); No primary key Referential Integrity A record referencing another table must refer to an existing record in that table. Specified between two tables Enforced through foreign keys Referential Integrity Violations Referential Integrity from table A to table B is violated…. … if a record in table B that is referenced from a record in table A i deleted. … if a record in table A referencing a non-existing record from table B is inserted. Foreing keys prevent violations! Dealing with violations CREATE TABLE a ( id integer PRIMARY KEY, column_a varchar(64), ..., b_id integer REFERENCES b (id) ON DELETE NO ACTION ); CREATE TABLE a ( id integer PRIMARY KEY, column_a varchar(64), ..., b_id integer REFERENCES b (id) ON DELETE CASCADE ); ON DELETE… NO ACTION: Throw an error CASCADE: Delete all referencing records RESTRICT: Throw an error SET NULL: Set the referencing column to NULL SET DEFAULT: Set the referencing column to its default value TO LEARN MORE ABOUT VIOLATING REFERENTIAL INTEGIRTY: Visit: https://www.gatevidyalay.com/referential-integrity-constraint-violation/" }, { "title": "Keys and Superkeys", "url": "/posts/2sql3/", "categories": "SQL, 2. Introduction to Relational Database", "tags": "sql, t-sql, primary keys, surrogate keys", "date": "2022-05-15 00:00:00 +0545", "snippet": " What is a Key? Attribute(s) that idenfity a record uniquely As long as attributes can be removed: superkey If no more attributes can be removed: minimal superky or key Primary Keys One primary key per database table, chosen from candidate keys Uniquely identifies records, e.g for referencing in other tables Unique and not-null constraints both apply Primary keys are time-invariant Example: CREATE TABLE products( product_no integer UNIQUE NOT NULL, name text, price numeric ); CREATE TABLE products( product_no integer PRIMARY KEY, name text, price numeric ); CREATE TABLE example( a integer, b integer, c integer, PRIMARY KEY (a,c) ); Specifying Primary Keys ALTER TABLE table_name ADD CONSTRAINT some_name PRIMARY KEY (column_name) Surrogate Keys (a.k.a Synthetic Primary Key) Surrogate Keys also called as a Synthetic Primary Key are the artifically created keys in order to uniquely identify a row in the table when there are no natural primay key in the table. Primary Keys should be built from as few columns as possible Primary Keys should never change over time Adding a surrogate key with serial data type ALTER TABLE cars ADD COLUMN id serial PRIMARY KEY; INSERT INTO cars VALUES (&#39;Volkswagen&#39;, &#39;Blitz&#39;, &#39;black&#39;); Another type of surrogate key ALTER TABLE table_name ADD COLUMN column_c varchar(256); UPDATE table_name SET column_c = CONCAT(column_a, column_b); ALTER TABLE table_name ADD CONSTRAINT pk PRIMARY KEY (column_c); " }, { "title": "Constraints", "url": "/posts/2sql2/", "categories": "SQL, 2. Introduction to Relational Database", "tags": "sql, t-sql, cast, alter column, not null, unique", "date": "2022-05-15 00:00:00 +0545", "snippet": " Integrity Constraints Attribute Constraints, e.g data types on columns Key Constraints, e.g primary keys Referential Integrity Constraints, enforced through foreign keys Why Constraints? Constraints give the data structure Constraints help with consistency, and thus data quality Data quality is a business advantage / data science prerequisite Data Types as Attribute Constraints Casting Data Types CREATE TABLE weather( temperature integer, wind_speed text ); SELECT temperature * wind_speed AS wind_chill FROM weather; The above sql statement results in error. Operator does not exist: integer * text HINT: No operator matches the given name and argument type(s). You might need to ass explicit type casts. SELECT temperature * CAST(wind_speed AS integer) AS wind_chill FROM weather; Most Common Data Types text: character strings of any length varchar[(x)]: a maximum of n characters char[(x)]: a fixed-length string of n characters boolean: can only take three states, e.g: TRUE, FALSE and NULL date, time and timestamp: various formats for date and time calculations numeric: arbitray precision numbers, e.g: 3.1457 integer: whole numbers in the range of -2147483648 and +2147483647 Creating columns with specific data types upon table creation CREATE TABLE students ( ssn integer, name varchar(64), dob date, average_grade numeric(3,2), --e.g 5.54 tuition_paid boolean ); Alter types after table creation ALTER TABLE students ALTER COLUMN name TYPE varchar(128); ALTER TABLE students ALTER COLUMN average_grade TYPE integer -- Turns 5.54 into 6, not 5, before type conversion USING ROUND(average_grade) Not-Null Constraints Disallow NULL values in a certain column Must hold true for the current state Must hold true for any future state Adding a Not-Null Constraints While creating a table CREATE TABLE students ( ssn integer not null, lastname varchar(54) not null, home_phone integer, office_phone integer ); After the table has been created ALTER TABLE students ALTER COLUMN home_phone SET NOT NULL; Removing a Not-Null Constraints ALTER TABLE students ALTER COLUMN ssn DROP NOT NULL: Unique Constraints Disallow duplicate values in a column Must hold true for the current state Must hold true for any future state Adding UNIQUE Constraints While creating a table CREATE TABLE table_name ( column_name UNIQUE ) After the table has been created ALTER TABLE table_name ADD CONSTRAINT some_name UNIQUE(column_name); " }, { "title": "Relational Databases", "url": "/posts/2sql1/", "categories": "SQL, 2. Introduction to Relational Database", "tags": "sql, t-sql, information schema, create table, insert into, rename column, drop column", "date": "2022-05-14 00:00:00 +0545", "snippet": " Using information schema SELECT table_schema, table_name FROM information_schema.tables; Taking the look at the columns of a certain table using information scehma SELECT table_name, column_name, data_type FROM information_schema.tables WHERE table_name = &#39;pg_config&#39;; Creating new tables with CREATE TABLE Syntax: CREATE TABLE table_name ( column_a data_type, column_b data_type, column_c data_type ); Example: CREATE TABLE weather ( clouds text, temperature numeric, weather_station char(5) ) INSERT INTO statemnt INSERT INTO table_name (column_a, column_b) VALUES (&quot;value_a&quot;, &quot;value_b&quot;); RENAME a column ALTER TABLE table_name RENAME COLUMN old_name TO new_name; DROP a column ALTER TABLE table_name DROP COLUMN column_name; " }, { "title": "CRUD Operations", "url": "/posts/1sql4/", "categories": "SQL, 1. Introduction to SQL Server", "tags": "sql, t-sql, create, insert, insert select, update, delete, truncate, declare, set, temporary tables", "date": "2022-05-14 00:00:00 +0545", "snippet": " CRUD OPERATION CREATE Databases, Tables or Views Users, prermissions and security groups READ Example: SELECT statements UPDATE Amend existing database records DELETE CREATE CREATE TABLE unique table name (column name, data type, size) CREATE TABLE test_table( test_date date, test_name, varchar(20), test_int int ) A few considerations when creating a table Table and column names Type of data each column will store Size or amount of data stored in the column INSERT Syntax: INSERT INTO table_name(col1, col2, col3) VALUES (&#39;value1&#39;, &#39;value2&#39;, value3) INSERT SELECT Syntax: INSERT INTO table_name (col1, col2, col3) SELECT column1, column2, column3 FROM other_table WHERE -- conditions apply Note: Don’t Use SELECT * Be specific in case table structure changes UPDATE Syntax: UPDATE table SET column = value, WHERE -- Condition(s); Don’t forget the WHERE clause! UPDATE table SET column1 = value1, column2 = value2 WHERE -- Coondition(s); DELETE Syntax: DELETE FROM table WHERE -- Conditions TRUNCATE It clears the entire table at once TRUNCATE TABLE table_name Declaring Variables We declare variables to avoid repition DECLARE @test_int INT DECLARE @my_artist VARCHAR(100) Using SET to set valuies to variables SET @test_int = 5 SET @my_artist = &#39;AC/CD&quot; Using variables to select columns from artist table where name is set to the value stored in @my_artist SELECT * FROM artist WHERE name = @my_artist; Temporary Tables SELECT col1, col2, col3 INTO #my_temp_table FROM my_existing_table WHERE -- Conditions #my_temp_table exists until connection or session ends -- Remove table manually DROP TABLE #my_temp_table " }, { "title": "Joining Tables", "url": "/posts/1sql3/", "categories": "SQL, 1. Introduction to SQL Server", "tags": "sql, t-sql, primary keys, foreign keys, inner join, left join, right join, union, union all", "date": "2022-05-14 00:00:00 +0545", "snippet": " PRIMARY KEYS: A primary key is used to ensure data in the specific column is unique. It is a column cannot have NULL values. It is either an existing table column or a column that is specifically generated by the database according to a defined sequence. FOREIGN KEYS: A foreign key is a column or group of columns in a relational database table that provides a link between data in two tables. It is a column (or columns) that references a column (most often the primary key) of another table. INNER JOIN Returns matching values in both tables Example: SELECT album_id, title, album.artist_id, name as artist_name FROM album INNER JOIN artist ON artist.artist_id = album.artist_id WHERE albumm.artist_id = 1; Syntax for INNER JOIN SELECT table_A.columnX, table_A.columnY, table_B.columnZ FROM table_A INNER JOIN table_B ON table_A.foreign_key = table_B.primary_key; Syntax for MULTIPLE INNER JOIN SELECT table_A.columnX, table_A.columnY, table_B.columnZ, table_C columnW FROM table_A INNER JOIN table_B on table_B.foreign_key = table_A.primary_key INNER JOIN table_C on table_C.foreign_key = table_B.primary_key; LEFT JOIN The LEFT JOIN keyword returns all records from the left table (table1), and the matching records from the right table (table2). The result is 0 records from the right side, if there is no match. Example: SELECT Admitted.Pattiend_ID, Admitted, Discharged FROM Admitted LEFT JOIN Discharged ON Discharged.Patient_ID = Admitted.Patient_ID; RIGHT JOIN The RIGHT JOIN keyword returns all records from the right table (table2), and the matching records from the left table (table1). The result is 0 records from the left side, if there is no match. Example: SELECT Admitted.Pattiend_ID, Admitted, Discharged FROM Admitted RIGHT JOIN Discharged ON Discharged.Patient_ID = Admitted.Patient_ID; Summary INNER JOIN: Only returns matching rows in both tables LEFT JOIN (or RIGHT JOIN): All rows from the main table plus match table NULL: Displayed if no match is found LEFT JOIN and RIGHT JOIN can be interchangeable UNION THE UNION operator is used to combine the result-set of two or more SELECT statements. Every SELECT statemnt within UNION must have the same number of columns. The columns must also have similar data types. The columns in every SELECT statement must also be in the same order Example: SELECT album_id, title, artist_id FROM album WHERE artist_id IN (1,3) UNION SELECT album_id, title, artisti_id FROM album WHERE artist_id IN (1,4,5); UNION ALL THE UNION operator selects only distinct values by default. To allow duplicate values, use UNION ALL. Syntax SELECT column_name(s) FROM table1 UNION ALL SELECT column_name(s) FROM table2; Example: SELECT album_id, title, artist_id FROM album WHERE artist_id IN (1,3) UNION ALL SELECT album_id, title, artist_id FROM album WHERE artist_id IN (1,4,5); Summary UNION or UNION ALL: Combines queries from the same table or different tables. If combining data from different tables: Select the same number of columns in the same order Columns should have the same data types If source tables have different column names Alias the column names UNION: Discards duplicates UNION ALL: Includes duplicates " }, { "title": "Aggregating Data", "url": "/posts/1sql2/", "categories": "SQL, 1. Introduction to SQL Server", "tags": "sql, t-sql, sum, count, count distinct, min, max, avg, len, left, right, charindex, substring, replace, group by, having", "date": "2022-05-13 00:00:00 +0545", "snippet": " SUM It is used to calculate the total amount of a column. Example: SELECT SUM(affected_customers) AS total_affected FROM grid; Returns the total of all the affected_customers SELECT SUM(affected_customers) AS total_affected, SUM(demand_loss_mw) AS total_loss FROM grid; Returns the total of all the affected_customers amd demand_loss_mw columns. COUNT It returns the total count of the records Example: SELECT COUNT(affected_customers) AS count_affected FROM grid; COUNT DISTINCT It returns the total count of the distinct records Example: SELECT COUNT(DISTINCT affected_customers) AS unique_count_affected FROM grid; MIN It returns the minimum value of a column Example: SELECT MIN(affected_customers) AS min_affected_customers FROM grid WHERE affected_customers &amp;gt; 0; It returns the minimum value of the affected_customer column from all the records where affected_customer &amp;gt; 0. MAX It returns the maximum value of a column Example: SELECT MAX(affected_customers) AS max_affected_customers FROM grid WHERE affected_customers &amp;gt; 0; It returns the maximum value of the affected_customer column from all the records where affected_customer &amp;gt; 0. AVG It returns the average value of a column Example: SELECT AVG(affected_customers) AS avg_affected_customers FROM grid; It returns the average value of the affected_customer column. LEN It returns the length of a string Example: SELECT description, LEN(description) AS description_length FROM grid; LEFT It returns the specified no of characters from the string from the left side. Example: SELECT description, LEFT(description, 20) AS first_20_left FROM grid; RIGHT It returns the specified no of characters from the string from the right side. Example: SELECT description, RIGHT(description, 20) AS last_20_ FROM grid; CHARINDEX The CHARINDEX() function searches for a substring from the left-side in a string, and returns the position. If the substring is not found, this function returns 0. Example: SELECT CHARINDEX(&#39;-&#39;, url) AS char_location, url FROM courses; SUBSTRING It returns the part of the string from the specified starting location to the specified end location. Syntax: REPLACE(string, start, end) Example: SELECT SUBSTRING(url, 12, 12) AS target_section, url FROM courses; REPLACE IT replaces all occurrences of a substring within a string, with a new substring. Syntax: REPLACE(string, old_string, new_string) Example: SELECT TOP(5) REPLACE(url, &#39;_&#39;,&#39;-&#39;) AS replace_with_hyphen FROM grid; GROUP BY The GROUP BY statement groups rows that have the same values into summary rows, like “find the number of customers in each country”. The GROUP BY statement is often used with aggregate functions (COUNT(), MAX(), MIN(), SUM(), AVG()) to group the result-set by one or more columns. Example: SELECT SUM(demand_loss_mw) AS lost_demand, description FROM grid GROUP BY description; The above SQL statement groups the records by description and for each group calculates the sum of demand_loss_mw. SELECT SUM(demand_loss_mw) AS lost_demand, description FROM grid WHERE description LIKE &#39;%storm&#39; AND demand_loss_mw IS NOT NULL GROUP BY description; The above SQL statement groups the records by description and filters the records with the conditions specified in WHERE clause and then aggregates the demand_loss_mw field using the SUM function from the result queires. HAVING The HAVING clause was added to SQL because the WHERE keyword cannot be used with aggregate functions. It can be used when filtering with groups. We normally use WHERE to filter data but what if we want to sum values based on groups and then filter on those groups? In such cases, we use HAVING. Example: SELECT SUM(demand_loss_mw) AS lost_demand, description FROM grid WHERE description LIKE &#39;%storm&#39; AND demand_loss_mw IS NOT NULL GROUP BY description HAVING SUM(demand_loss_mw) &amp;gt; 1000; The above SQL statement groups the records by description and filters the records with the conditions specified in WHERE clause and then aggregates using the SUM function from the result queires. Then the results after applying aggregating functions are filterd using the conditions specified in the Having clause. GROUP BY splits the data up into combination sof one or more values. WHERE filters on row values HAVING appears after the GROUP BY claues and filters on groups or aggregates " }, { "title": "Introduction to SQL Server", "url": "/posts/1sql1/", "categories": "SQL, 1. Introduction to SQL Server", "tags": "sql, t-sql, select, top, percent, distinct, order by, where, between, in, like", "date": "2022-04-06 00:00:00 +0545", "snippet": "A SQL Server is a relational database system developed by Microsoft. And a Transact-SQL (T-SQL) is a Microsoft’s implemenataion of SQL butwith added functionality.Queries SELECT Is used for retreiving data from the table. Example: SELECT description FROM grid; Selecting more than one column SELECT artist_id, artist_name FROM artist; SELECT TOP() Return top 5 rows SELECT TOP(5) artist FROM artists; SELECT TOP() PERCENT Return top 5% of rows SELECT TOP(5) PERCENT artist FROM artists; SELECT DISTINCT Returns unique rows SELECT DISTINCT nerc_region FROM grid; SELECT * Returns all rows SELECT * FROM grid; NOT suitable for large tables AS Aliasing column names Example: SELECT demand_loss_mw AS lost_demand FROM grid; SELECT description AS cause_of_courage FROM grid; ORDER BY Use ORDER BY if order is important. Example: SELECT TOP(10) product_id, year_intro FROM products ORDER BY year_intro, product_id; In the above example, first it is ordered by year_intro and then it is order by product_id SELECT TOP(10) product_id, year_intro FROM products ORDER BY year_intro DESC, product_id; In the above example, firs it is ordered by year_intro in the descending order and then by product_id WHERE Use Where to return rows that meet certain criteria. Example: SELECT customer_id, total FROM invoice WHERE total &amp;gt; 15; The above SQL statement returns records where total is greater than 15. SELECT customer_id, total FROM invoice WHERE total &amp;lt;&amp;gt; 10; The above SQL statement returns records where total is not equal to 10. SELECT * FROM songlist WHERE release_year = 1994 AND artist = &#39;Green Day&#39; AND song = &#39;Basket Case&#39;; The above SQL statement returns records where all the given conditions are satisfied. SELECT song, artist, release_year FROM songlist WHERE release_year = 1994 OR release_year &amp;gt;2000; The above SQL statement returns records where at least one of the given condition is satisfied. BETWEEN Use Between to return where the values lies between the specified range. Example: SELECT customer_id, total FROM invoice WHERE total BETWEEN 20 AND 30; Returns those records where the value of total is in between the range of 20-30 inclusive. SELECT customer_id, total FROM invoice WHERE total NOT BETWEEN 20 AND 30; Returns those records where the value of total is not in between the range of 20-30 inclusive. NULL It indicates that there is no value for that record It helps to highlight gaps in our data. Example: SELECT TOP(6) total, billing_state FROM invoice WHERE billing_state IS NULL; It returns records where billing_state is NULL. SELECT TOP(6) total, billing_state FROM invoice WHERE billing_state IS NOT NULL; It returns records where billing_state is NOT NULL. IN It is used to return records where the values are inside the list followed by the IN keyword. Example: SELECT song, release_year FROM songlist WHERE release_year IN (1985, 1991, 1992); Returns records where release_year is in the list followed by the IN keyword. LIKE It is used to return records where the values match the pattern specified after the LIKE keyword Example: SELECT artist FROM songlist WHERE artist LIKE &#39;f%&#39;; Returns those records where values of artist column begins with an f. " }, { "title": "Convolutional Neural Network (CNN / ConvNet)", "url": "/posts/Convolutinal-Neural-Network/", "categories": "Deep Learning, CNN", "tags": "cnn", "date": "2022-03-13 00:00:00 +0545", "snippet": "Convolutional Neural Networks (CNN/ ConvNet)A Convolutional Neural Network, also known as CNN or ConvNet, is a class of neural networks that specailizes in processing data that has a grid-like topology such as an image. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity.They are very similar to ANN in the sense that they too are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. It takes an input image, assigns importances to various aspects/objects in the image and eb able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough traning, ConvNets have the abilitiy to learn these filters/characteristics.Why ConvNets over Feed-Forward Neural Nets?An image is nothing but a matrix of pixel values. So can we not just flatten the image and feed it to a Multi-Level Perceptron for classification purpose as shown in the above figure? No Because, in cases of extremely basic binary images, the method might show an average precision score while performing prediction of classes but would have little to no accuracy when it comes to complex images having pixel dependencies throughout.A ConvNet is able to successfully capture the Spatial and Temporal dependencies in an image through the application of relevant filters. The architecture performs a better fitting to the image dataset due to the reduction in the number of parameters involved and reusability of weights. In other words, the network can be trained to understand the sophistication of the image better.Layers used to build ConvNetsA simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through a differentiable functions. There are mainly 3 types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully Connected Layer. Convolutional Layer: A Convolutional Layers will compute the output of neurons that are connected to the local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume using the filters/kernels. Pooling Layer: A Pooling Layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume. Fully Connected Layer: A fully-connected layer will compute the class scores. As with ordinary Neural Networks and as the naem implies, each neuron in this layer will be connected to all the nubers in the previous volume. Need of ConvNetLet us consider we have an RGB image which has been separated by its three color planes -Red, Green, and Blue. Now, we can imagine how computationally intensive things would get once the images reach dimensions, say 8k. The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction.Convolutional LayerThe Convolutional Layer is the core building block of the CNN. It carries the main portion of the network’s computational load. The layer performs a dot product between two matrices, where one matrix is the set of learnable parameters otherwise known as kernel, and the other matrix is the restricted portion of the receptive field. The kernel is spatially smaller than an image but is more in-depth. This means that, if the image is composed of three (RGB) channels, the kernel height and width will be spatially small, but the depth extends up to all three channels.During the forward pass, the kernel slides across the height and width of the image-producing the image representation of that receptive region. This produces a two-dimensional representation of the image known as an activation map that gives the response of the kernel at each spatial position of the image. The sliding size of the kernel is called a stride.During the forward pass, the kernel slides across the height and width of the image-producing the image representation of that receptive region. This produces a two-dimensional representation of the image known as an activation map that gives the response of the kernel at each spatial position of the image. The sliding size of the kernel is called a stride.\\[W_{out} = \\frac{W-F+2P}{S}+1\\]Convolution OperationWorking of Convolutional LayerIn the above image, we have the Input Dimension = 5 (Height) x 5 (Breadth) x 1 (Number of channels, eg:- RGB)In the above demonstration, the green section resembles our 5x5x1 input image, I. The element involved in carrying out the convolution operation in the first part of a Convolutional Layer is called the Kernel/Filter, K, represented in the color yellow. We have selected K as a 3x3x1 matrix.The Kernel shifts 9 times because of Stride Length = 1 (Non-Strided), every time performing a matrix multiplication operation between K and the portion P of the image over which the kernel is hovering.The filter moves to the right with a certain Stride Value till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same Stride Value and repeats the process until the entire image is reversed.Convolution Operation for RBG color channelIn the case of images with multiple channels (e.g. RGB), the Kernel has the same depth as that of the input image. Matrix Multiplication is performed between Kn and In stack ([K1, I1]; [K2, I2]; [K3, I3]) and all the results are summed with the bias to give us a squashed one-depth channel Convoluted Feature Output.The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. ConvNets need not be limited to only one Convolutional Layer.There are two types of results to the operation — one in which the convolved feature is reduced in dimensionality as compared to the input, and the other in which the dimensionality is either increased or remains the same. This is done by applying Valid Padding in case of the former, or Same Padding in the case of the latter.When we augment the 5x5x1 image into a 6x6x1 image and then apply the 3x3x1 kernel over it, we find that the convolved matrix turns out to be of dimensions 5x5x1. Hence the name — Same Padding.On the other hand, if we perform the same operation without padding, we are presented with a matrix which has dimensions of the Kernel (3x3x1) itself — Valid Padding.Pooling LayerThe pooling layer replaces the output of the network at certain locations by deriving a summary statistic of the nearby outputs. This helps in reducing the spatial size of the representation, which decreases the required amount of computation and weights. The pooling operation is processed on every slice of the representation individually.Similar to the Convolutional Layer, the Pooling layer is responsible for reducing the spatial size of the Convolved Feature. This is to decrease the computational power required to process the data through dimensionality reduction. Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.There are several pooling functions such as the average of the rectangular neighborhood, L2 norm of the rectangular neighborhood, and a weighted average based on the distance from the central pixel. However, the most popular process are Max Pooling and Average Pooling. Max Pooling returns the maximum value from the portion of the image covered by the Kernel. On the other hand, Average Pooling returns the average of all the values from the portion of the image covered by the Kernel.Types Of PoolingNote:- Max Pooling also performs as a Noise Suppressant. It discards the noisy activations altogether and also performs de-noising along with dimensionality reduction. On the other hand, Average Pooling simply performs dimensionality reduction as a noise suppressing mechanism. Hence, we can say that Max Pooling performs a lot better than Average Pooling.If we have an activation map of size W x W x D, a pooling kernel of spatial size F, and stride S, then the size of output volume can be determined by the following formula:\\(W_{out}=\\frac{W-F}{S}+1\\)This will yield an output volume of size Wout x Wout x D.Note: -In all cases, pooling provides some translation invariance which means that an object would be recognizable regardless of where it appears on the frame.The Convolutional Layer and the Pooling Layer, together form the i-th layer of a Convolutional Neural Network. Depending on the complexities in the images, the number of such layers may be increased for capturing low-levels details even further, but at the cost of more computational power.Fully Connected LayerAfter going through the above process, we have successfully enabled the model to understand the features. Moving on, we are going to flatten the final output and feed it to a regular Neural Network for classification purposes.Neurons in this layer have full connectivity with all neurons in the preceding and succeeding layer as seen in regular FCNN. This is why it can be computed as usual by a matrix multiplication followed by a bias effect. This layer helps to map the representation between the input and the output.Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.Now that we have converted our input image into a suitable form for our Multi-Level Perceptron, we shall flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them.Some popular CNN ArchitecturesThere are various architectures of CNNs available which are listed below: LeNet AlexNet VCGNet GoogLeNet ResNet ZFNetReferences d2l-CNN" }, { "title": "Back Propagation", "url": "/posts/Back-Propagation/", "categories": "Deep Learning, ANN, CNN", "tags": "ann, cnn", "date": "2022-03-13 00:00:00 +0545", "snippet": "What is Backpropagation?Backpropagation is a supervised learning algorithm, for training Multi-layer Perceptrons (Artificial Neural Networks).It refers to the method of calculating the gradient of neural network parameters. In short, the method traverses the network in reverse order, from the output to the input layer, according to the chain rule from calculus. It looks for the minimum value of the error function in weight space using a technqiue called the delta rule or gradient descent. The weights that minimize the error function is then considered to be a solution to the learning problem.Why do we need Backpropagation?When training a neural network we also assign random values for weights and biases. Therefore, the random weights might not fit the model the best due to which the output of our model may be very different than the actual output. This results in high error values. Thereore, we need to tune those weights so as to reduce the error. To tune the errors we need to guide the model to change the weights such that the error becomes minimum.Weights vs ErrorWorking of BackpropagationLet’s us consider the Neural Network Below: Values: \\ x1= 0.05, x2= 0.10 b1= 0.35, b2= 0.6 w1 = 0.15, w2 = 0.20, w3 = 0.25, w4 = 0.30 w5 = 0.40, w6 = 0.45, w7 = 0.50, w8 =0.55 The above neural network contains the follwowing: One Input Layer Two Input Neurons One Hidden Layer Two Hidden Neurons One Output Layer Two Output Neurons Two BiasesFollowing are the steps for the weight update using Backpropagation:- Note: We will be using Sigmoid Activation Function. \\(\\text{Sigmoid Activation Function}= \\frac{1}{1+e^{-x}}\\) Step 1: Forward Propagation Net Input for h1: \\(\\text{h1} = x1 * w1+x2 * w2+b1\\) \\(\\text{h1} = 0.05 * 0.15 + 0.10 * 0.20 + 0.35\\) \\(\\text{h1} = 0.3775\\) Output of h1: \\(\\text{out h1} =\\frac{1}{1+e^{-h1}}\\) \\(\\text{out h1} = \\frac{1}{1+e^{-0.3775}}\\) \\(\\text{out h1} = 0.5932\\) Similary, Output of h2: \\(\\text{out h2} = 0.5968\\) Repeat the process for the output layre neurons, using the output from the hidden layer as input for the output neurons. Input for y1: \\(\\text{y1} = \\text{out h1} * w5+ \\text{out h2} * w6+b2\\) \\(\\text{y1} = 0.5932 * 0.40 + 0.5968 * 0.45 + 0.6\\) \\(\\text{y1} = 1.1059\\) Output of y1: \\(\\text{out y1} =\\frac{1}{1+e^{-h1}}\\) \\(\\text{out y1} = \\frac{1}{1+e^{-1.1959}}\\) \\(\\text{out y1} = 0.7513\\) Similary, Output of y2: \\(\\text{out y2} = 0.7729\\) Calculating Total Error: $E_{total} = E_{1} + E_{2}$ $E_{total} = \\sum_{}^{}\\frac{1}{2}(target-output)^{2}$$E_{total} = \\frac{1}{2}(T_{1}-\\text{Out y1})^{2}$ + $\\frac{1}{2}(T_{2}-\\text{Out y2})^{2}$$E_{total} = \\frac{1}{2}(0.01-0.751)^{2}$ + $\\frac{1}{2}(0.99-0.772)^{2}$ $E_{total} = 0.29837$ Step 2: Backward Propagation Now, we will reduce the error by updating the values weights and biases using back-propagation. To update Weights, let us consider $w5$ for which we will calculate the rate of change of error w.r.t change in weight $w5$\\[\\text{Error at w5} = \\frac{dE_{total}}{dw5}\\] Now,\\[\\frac{dE_{total}}{dw5} = \\frac{dE_{total}}{\\text{douty1}} * \\frac{\\text{douty1}}{dy1} * \\frac{dy1}{dw5}\\] Since, we are propagating backwards, first thing we need to do is calculate the change in total errors w.r.t to the output y1 and y2\\[E_{total} = \\frac{1}{2}(T_{1}-\\text{out y1})^2 + \\frac{1}{2}(T_{2}-\\text{out y2})^2\\]\\[\\frac{dE_{total}}{d\\text{outy1}} = \\frac{1}{2} * 2 * (T_{1}-\\text{out y1})^{2-1} * (0-1) + 0\\]\\[\\frac{dE_{total}}{d\\text{outy1}} = (T_{1}-\\text{out y1})* (-1)\\]\\[\\frac{dE_{total}}{d\\text{outy1}} = -T_{1}+\\text{out y1}\\]\\[\\frac{dE_{total}}{d\\text{outy1}} = -0.01 + 0.7513\\]\\[\\frac{dE_{total}}{d\\text{outy1}} = 0.7413\\] Now, we will propate further backwards and calculate change in output y1 w.r.t its initial input\\[\\frac{\\text{douty1}}{dy1} = \\text{out y1} * (1-\\text{out y1})\\]\\[\\frac{\\text{douty1}}{dy1} = 0.18681\\] Now, we will se how much y1 changes w.r.t change int w5:\\[\\frac{dy1}{dw5} = 1 * \\text {out h1} * w_{5}^{1-1} + 0 + 0\\]\\[\\frac{dy1}{dw5} = \\text {out h1}\\]\\[\\frac{dy1}{dw5} = 0.5932\\] Step 3: Putting all the values together and calculating the updated weight value. Now, putting all the values together: \\[\\frac{dE_{total}}{dw5} = \\frac{dE_{total}}{\\text{douty1}} * \\frac{\\text{douty1}}{dy1} * \\frac{dy1}{dw5} =0.0821\\] Now, updating the w5\\(w5 = w5 - \\eta * \\frac{dE_{total}}{dw5}\\)\\[w5 = 0.4 - 0.5 * 0.0821\\]\\[w5 = 0.3589\\] Similarly, we can calculate the other weight values as well $w6 = 0.4808$ $w7 = 0.5113$ $w8 = 0.0613$ Now, at hidden layer updating w1, w2, w3, and w4: \\[\\frac{dE_{total}}{dw1} = \\frac{dE_{total}}{\\text{dout h1}} * \\frac{\\text{dout h1}}{dh1} * \\frac{dh1}{dw1} \\text{ where},\\]\\[\\frac{dE_{total}}{\\text{dout h1}} = \\frac{dE_{1}}{\\text{dout h1}} + \\frac{dE_{2}}{\\text{dout h1}} \\text{ where}\\]\\[\\frac{dE_{1}}{\\text{dout h1}} = \\frac{dE_{1}}{dy1} * \\frac{dy1}{\\text{dout h1}} \\text{ where}\\] \\(\\frac{dE_{1}}{dy1} = \\frac{dE_{1}}{\\text{dout y1}} * \\frac{\\text{dout y1}}{dy1}\\)s\\[\\frac{dE_{1}}{dy1} = 0.7413 * 0.1868\\]\\[\\frac{dE_{1}}{dy1} = 0.1384\\]\\[\\frac{dy1}{\\text{dout h1}} = 0.05539\\] Using above values we can calculate the $\\frac{dE_{1}}{\\text{dout h1}}$ and similarly $\\frac{dE_{2}}{\\text{dout h2}}$ which in turn can be used to calculate the value of $\\frac{dE_{total}}{\\text{dout h1}}$. Similarly,calculate the value of $\\frac{\\text{dout h1}}{dh1}$ and $ \\frac{dh1}{dw1}$ to get the change of error w.r.t to change in weight w1. We repeat this process for all the remaining weights. After that we will again propagate forward and calculate the output. We will again calculate the error. If the error is minimum, we will stop right there, else we will again propagate backwards and upate the weight values. This process will keep on repeating until error becomes minimum. " }, { "title": "Artificial Neural Network (ANN / MLP)", "url": "/posts/Artificial-Neural-Network/", "categories": "Deep Learning, ANN", "tags": "ann, activation functions", "date": "2022-03-13 00:00:00 +0545", "snippet": "To understand the concept of the architecture of an artificial neural network, we have to understand what a neural network consists of. In order to define a neural network that consists of a large number of artificial neurons, which are termed units arranged in a sequence of layers. Lets us look at various types of layers available in an artificial neural network. The easiesy way to do this is to stack many fully-connected lyaers on top pof each other. Each layer feeds into layer above it, until we generate outputs. We can think of the first L-1 layers as our representation and the final layer as our linear predictor. This architecture is commonly called a multilayer perceptron, often abbreviated as MLP. Below, we depcit an MLP diagramatically which consists of three layers:An MLP with a hidden layer of 5 hidden unitsInput Layer: As the name suggests, it accepts inputs in several different formats provided by the programmer. The above MLP has 4 input layer units.Hidden Layer: The hidden layer presets in-between input and output layers. It performs all the calculations to find the hidden features and patterns. The above MLP has 5 hidden layer units.Output Layer: The input goes through a series of transformations using the hidden layer, which finally results in output that is conveyed using this layer.The above MLP has 3 output layer units.Since the input layer does not involve any calculations, producing outputs with this network requires implementing the computations for both the hidden and output layers; thus, the number of layers in this MLP is 2. Note that these layers are both fully connected. Every input influences every neuron in the hidden layer, and each of these in turn influences every neuron in the output layer.The artificial neural network takes input and computes the weighted sum of the inputs and includes a bias. This computation is represented in the form of a transfer function. \\(\\sum_{i=1}^{n}W_{i}X_{i}+b\\)It determines weighted total is passed as an input to an activation function to produce the output. Activation functions choose whether a node should fire or not. Only those who are fired make it to the output layer. There are distinctive activation functions available that can be applied upon the sort of task we are performing.Activation FunctionsVisualization: Visualizes Activation FunctionsActivation Functions: The activation function refers to the set of transfer functions used to achieve the desired output. They decide whether a neuron should be activated or not by calculating the weighted sum and further adding bias with it. They are differentiable operators to transform input signals to outputs, while most of them add non-linearity.Some of the commonly used sets of activation functions are:- ReLU Activation Function: The most popular choice, due to both simplicity of implementation and its good performance on a variety of predictive tasks, is the rectified linear unit (ReLU). ReLU provides a very simple nonlinear transformation. Given an element x, the function is defined as the maximum of that element and 0: $ReLU(x) = max(x,0)$ Informally, the ReLU function retains only positive elements and discards all negative elements by setting the corresponding activations to 0. When the input is negative, the derivative of the ReLU function is 0, and when the input is positive, the derivative of the ReLU function is 1. Note that the ReLU function is not differentiable when the input takes value precisely equal to 0. In these cases, we default to the left-hand-side derivative and say that the derivative is 0 when the input is 0. We can get away with this because the input may never actually be zero. There is an old adage that if subtle boundary conditions matter, we are probably doing (real) mathematics, not engineering. That conventional wisdom may apply here. We plot the derivative of the ReLU function plotted below. The reason for using ReLU is that its derivatives are particularly well behaved: either they vanish or they just let the argument through Advantage: Solves Vanishing Gradient Problem Disadvantagne: Dead neuron condition, Derivation of x with respect to x gives 1 but a derivation of negative weight with respect to x gives 0, thus there is no new weight to add in backpropagation resulting in dying relu or dead neuron condition. Sigmoid Activation Function: The sigmoid function transforms its inputs, for which values lie in the domain R, to outputs that lie on the interval (0, 1). For that reason, the sigmoid is often called a squashing function: it squashes any input in the range (-inf, inf) to some value in the range (0, 1):\\(sigmoid(x) = \\frac{1}{1+exp(-x)}\\)In short, Whatever input it receives output values will between 0 and 1. Sigmoids are still widely used as activation functions on the output units, when we want to interpret the outputs as probabilities for binary classification problems (you can think of the sigmoid as a special case of the softmax). The derivative of the sigmoid function is given by the following equation:The derivative of the sigmoid function is plotted below. Note that when the input is 0, the derivative of the sigmoid function reaches a maximum of 0.25. As the input diverges from 0 in either direction, the derivative approaches 0.s Disadvantages: The curve is not normally distributed so the network is computationally expensive and reaching global minima takes a lot of time. Vanishing gradient problem will happen in backpropagation due to which new weight that has to be added will be equal to the old weight and learning of the network is diminished. 3.Tanh Activation Function: Like the sigmoid function, the tanh (hyperbolic tangent) function also squashes its inputs, transforming them into elements on the interval between -1 and 1. Simply, whatever input it receives, output values will between -1 and 1. \\(tanh(x) = \\frac{1-exp(-2x)}{1+exp(-2x)}\\)We plot the tanh function below. Note that as the input nears 0, the tanh function approaches a linear transformation. Although the shape of the function is similar to that of the sigmoid function, the tanh function exhibits point symmetry about the origin of the coordinate system.The derivative of the tanh function is:\\(\\frac{d}{dx}tanh(x) = 1- tanh^{2}(x)\\)The derivative of tanh function is plotted below. As the input nears 0, the derivative of the tanh function approaches a maximum of 1. And as we saw with the sigmoid function, as the input moves away from 0 in either direction, the derivative of the tanh function approaches 0.4.Softmax Activation Function: Softmax is an activation function that transforms numbers/logits into probabilites. The output of a Softmax is a vector (say v) with probabilities of each possible outcome. The probabilities in vector v sums to one for all possible outcomes or classes. Mathematically, Softmax is defined as, \\(\\sigma(y)_{i} =\\frac{e^{a_{i}}}{\\sum_{}^{j}e^{a_{j}}}\\)\\(Derivative: \\frac{d\\sigma(a)}{da_{j}} = \\sigma(a)(\\delta_{ij}-\\delta_{j}(a))\\text{ where,} \\delta_{ij}=1\\text{ if i=j, otherwise, 0}\\) Note: The softmax transformation transforms a bunch of arbitrarily large or small numbers into a valid probability distribution.Training Error and Generalization ErrorTraining Error: The training error is the error of our model as calculated on the training dataset.Generalization Error: Generalization error is the expectation of our model’s error were we to apply it to an infinite stream of additional data examples drawn from the same underlying data distribution as our original sample. we must estimate the generalization error by applying our model to an independent test set constituted of a random selection of data examples that were withheld from our training set.Validation DatasetIn principle we should not touch our test set until after we have chosen all our hyperparameters. Were we to use the test data in the model selection process, there is a risk that we might overfit the test data. Then we would be in serious trouble. If we overfit our training data, there is always the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever know?Thus, we should never rely on the test data for model selection. And yet we cannot rely solely on the training data for model selection either because we cannot estimate the generalization error on the very data that we use to train the model.In practical applications, the picture gets muddier. While ideally we would only touch the test data once, to assess the very best model or to compare a small number of models to each other, real-world test data is seldom discarded after just one use. We can seldom afford a new test set for each round of experiments.The common practice to address this problem is to split our data three ways, incorporating a validation dataset (or validation set) in addition to the training and test datasets. The result is a murky practice where the boundaries between validation and test data are worryingly ambiguous. Unless explicitly stated otherwise, in the experiments in this book we are really working with what should rightly be called training data and validation data, with no true test sets. Therefore, the accuracy reported in each experiment of the book is really the validation accuracy and not a true test set accuracy.Underfitting or OverfittingUnderfitting: When we compare the training and validation errors, we want to be mindful of two common situations. First, we want to watch out for cases when our training error and validation error are both substantial but there is a little gap between them. If the model is unable to reduce the training error, that could mean that our model is too simple (i.e., insufficiently expressive) to capture the pattern that we are trying to model. Moreover, since the generalization gap between our training and validation errors is small, we have reason to believe that we could get away with a more complex model. This phenomenon is known as underfitting.Overfitting: On the other hand, as we discussed above, we want to watch out for the cases when our training error is significantly lower than our validation error, indicating severe overfitting. Note that overfitting is not always a bad thing. With deep learning especially, it is well known that the best predictive models often perform far better on training data than on holdout data. Ultimately, we usually care more about the validation error than about the gap between the training and validation errors.RegularizationRegularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the model’s performance on the unseen data as well.sRegularization in general penalizes the coefficient i.e it penalizes the weight matrices of the nodes.Assume that our regularization coefficient is so high that some of the weight matrices are nearly equal to zero.This will result in a much simpler linear network and slight underfitting of the training data.Such a large value of the regularization coefficient is not that useful. We need to optimize the value of regularization coefficient in order to obtain a well-fitted model as shown in the image below.Different Regularization Techniques in Deep LearningNow that we have an understanding of how regularization helps in reducing overfitting, we’ll learn a few different techniques in order to apply regularization in deep learning.L2 &amp;amp; L1 RegularizationL1 and L2 are the most common types of regularization. These update the general cost function by adding another term known as the regularization term.\\[\\text{Cost function = Loss() + Regularization Term}\\]Due to the addition of this regularization term, the values of weight matrices decrease because it assumes that a neural network with smaller weight matrices leads to simpler models. Therefore, it will also reduce overfitting to quite an extent.However, this regularization term differs in L1 and L2.In L2, we have\\[Cost function = Loss + \\frac{\\lambda}{2}*\\Sigma||w||^2\\]Here, lambda is the regularization parameter. It is the hyperparameter whose value is optimized for better results. L2 regularization is also known as weight decay as it forces the weights to decay towards zero (but not exactly zero).In L1, we have\\[Cost function = Loss + \\frac{\\lambda}{2m}*\\Sigma||w||\\]In this, we penalize the absolute value of the weights. Unlike L2, the weights may be reduced to zero here. Hence, it is very useful when we are trying to compress our model. Otherwise, we usually prefer L2 over it.DropoutThis is the one of the most interesting types of regularization techniques. It also produces very good results and is consequently the most frequently used regularization technique in the field of deep learning.To understand dropout, let’s say our neural network structure is shown as below: So what does dropout do? At every iteration, it randomly selects some nodes and removes them along with all of their incoming and outgoing connections as shown below.So each iteration has a different set of nodes and this results in a different set of outputs. It can also be thought of as an ensemble technique in machine learning.Ensemble models usually perform better than a single model as they capture more randomness. Similarly, dropout also performs better than a normal neural network model.This probability of choosing how many nodes should be dropped is the hyperparameter of the dropout function. As seen in the image above, dropout can be applied to both the hidden layers as well as the input layers.OptimizersAny deep learning model tries to generalize the data using an algorithm and tries to make predictions on the unseen data. We need an algorithm that maps the examples of inputs to that of the outputs and an optimization algorithm. An optimization algorithm finds the value of the parameters(weights) that minimize the error when mapping inputs to outputs. These optimization algorithms or optimizers widely affect the accuracy of the deep learning model. They as well as affect the speed training of the model.While training the deep learning model, we need to modify each epoch’s weights and minimize the loss function. An optimizer is a function or an algorithm that modifies the attributes of the neural network, such as weights and learning rate. Thus, it helps in reducing the overall loss and improve the accuracy.The problem of choosing the right weights for the model is a daunting task, as a deep learning model generally consists of millions of parameters. It raises the need to choose a suitable optimization algorithm for your application. 4You can use different optimizers to make changes in your weights and learning rate. However, choosing the best optimizer depends upon the application.As a beginner, one evil thought that comes to mind is that we try all the possibilities and choose the one that shows the best results. This might not be a problem initially, but when dealing with hundreds of gigabytes of data, even a single epoch can take a considerable amount of time. So randomly choosing an algorithm is no less than gambling with your precious time that you will realize sooner or later in your journey.Some of the most common optimizers are: - Gradient Descent Stochastic Gradient Descent Stochastic Gradient Descent with Momentum Mini-Batch Gradient Descent Adagrad RMSProp AdaDelta AdamSome of the terms that we need to be familiar with before diving into Optimizers: - Epoch – The number of times the algorithm runs on the whole training dataset. Sample – A single row of a dataset. Batch – It denotes the number of samples to be taken to for updating the model parameters. Learning rate – It is a parameter that provides the model a scale of how much model weights should be updated. Cost Function/Loss Function – A cost function is used to calculate the cost that is the difference between the predicted value and the actual value. Weights/ Bias – The learnable parameters in a model that controls the signal between two neurons. Gradient Descent Deep Learning OptimizerGradient Descent optimization algorithm uses calculus to modify the values consistently and to achieve the local minimum. In simple terms, consider you are holding a ball resting at the top of a bowl. When you lose the ball, it goes along the steepest direction and eventually settles at the bottom of the bowl. A Gradient provides the ball in the steepest direction to reach the local minimum that is the bottom of the bowl.\\(x_{new} = x - \\alpha * f&#39;(x)\\)The above equation means how the gradient is calculated. Here alpha is step size that represents how far to move against each gradient with each iteration.Working of Gradient Descent It starts with some coefficients, sees their cost, and searches for cost value lesser than what it is now. It moves towards the lower weight and updates the value of the coefficients. The process repeats until the local minimum is reached. A local minimum is a point beyond which it can not proceed.Disadvantages of Gradient Descent: Gradient descent works best for most purposes. However, it has some downsides too. It is expensive to calculate the gradients if the size of the data is huge. Gradient descent works well for convex functions but it doesn’t know how far to travel along the gradient for nonconvex functions.Stochastic Gradient DescentAt the end of the previous section, you learned why using gradient descent on massive data might not be the best option. To tackle the problem, we have stochastic gradient descent. The term stochastic means randomness on which the algorithm is based upon. In stochastic gradient descent, instead of taking the whole dataset for each iteration, we randomly select the batches of data. That means we only take few samples from the dataset.\\(w = w- \\eta\\bigtriangledown Q_i(w)\\)The procedure is first to select the initial parameters w and learning rate n. Then randomly shuffle the data at each iteration to reach an approximate minimum.Since we are not using the whole dataset but the batches of it for each iteration, the path took by the algorithm is full of noise as compared to the gradient descent algorithm. Thus, SGD uses a higher number of iterations to reach the local minima. Due to an increase in the number of iterations, the overall computation time increases. But even after increasing the number of iterations, the computation cost is still less than that of the gradient descent optimizer. So the conclusion is if the data is enormous and computational time is an essential factor, stochastic gradient descent should be preferred over batch gradient descent algorithm.Stochastic Gradient Descent With MomentumAs discussed in th earlier section, we have learned that Stochastic Gradient Descent takes a much more noisy path than the Gradient Descent Algorithm.Due to this reason, it requires a more significant number of iterations to reach the optimal minimum and hence computation time is very slow. To overcome the problem, we use stochastic gradient descent with a momentum algorithm.What the momentum does is helps in faster convergence of the loss function. Stochastic gradient descent oscillates between either direction of the gradient and updates the weights accordingly. However, adding a fraction of the previous update to the current update will make the process a bit faster. One thing that should be remembered while using this algorithm is that the learning rate should be decreased with a high momentum term.In the above image, the left part shows the convergence graph of the stochastic gradient descent algorithm. At the same time, the right side shows SGD with momentum. From the image, you can compare the path chosen by both the algorithms and realize that using momentum helps reach convergence in less time.You might be thinking of using a large momentum and learning rate to make the process even faster. But remember that while increasing the momentum, the possibility of passing the optimal minimum also increases. This might result in poor accuracy and even more oscillations.Mini Batch Gradient DescentIn this variant of gradient descent instead of taking all the training data, only a subset of the dataset is used for calculating the loss function. Since we are using a batch of data instead of taking the whole dataset, fewer iterations are needed. That is why the mini-batch gradient descent algorithm is faster than both stochastic gradient descent and batch gradient descent algorithms. This algorithm is more efficient and robust than the earlier variants of gradient descent. As the algorithm uses batching, all the training data need not be loaded in the memory, thus making the process more efficient to implement. Moreover, the cost function in mini-batch gradient descent is noisier than the batch gradient descent algorithm but smoother than that of the stochastic gradient descent algorithm. Because of this, mini-batch gradient descent is ideal and provides a good balance between speed and accuracy.Disadvantages of Mini-Batch Gradient Descent: Despite, all that, the mini-batch gradient descent algorithm has some downsides too. It needs a hyperparameter that is “mini-batch-size”, which needs to be tuned to achieve the required accuracy. Although, the batch size of 32 is considered to be appropriate for almost every case. Also, in some cases, it results in poor final accuracy. Due to this, there needs a rise to look for other alternatives too.Adagrad (Adaptive Gradient Descent)The adaptive gradient descent algorithm is slightly different from other gradient descent algorithms. This is because it uses different learning rates for each iteration. The change in learning rate depends upon the difference in the parameters during training. The more the parameters get change, the more minor the learning rate changes. This modification is highly beneficial because real-world datasets contain sparse as well as dense features. So it is unfair to have the same value of learning rate for all the features. The Adagrad algorithm uses the below formula to update the weights.\\(w_{t} = w_{t-1}-\\eta&#39;_{t}\\frac{\\delta L}{\\delta w(t-1)}\\)\\(\\eta&#39;_{t} = \\frac{\\eta}{sqrt(\\alpha_{t}+\\epsilon)}\\)Hwere, the $\\alpha(t)$ denotes the different learning rates at each iteration, n is a constant, and E is a small positive to avoid division by 0.The benefit of using Adagrad is that it abolishes the need to modify the learning rate manually. It is more reliable than gradient descent algorithms and their variants, and it reaches convergence at a higher speed.Disadvantages of AdaGrad: One downside of AdaGrad optimizer is that it decreases the learning rate aggressively and monotonically. There might be a point when the learning rate becomes extremely small. This is because the squared gradients in the denominator keep accumulating, and thus the denominator part keeps on increasing. Due to small learning rates, the model eventually becomes unable to acquire more knowledge, and hence the accuracy of the model is compromised.RMS Prop (Root Mean Square)RMS prop is one of the popular optimizers among deep learning enthusiasts. This is maybe because it hasn’t been published but still very well know in the community. RMS prop is ideally an extension of the work RPPROP. RPPROP resolves the problem of varying gradients. The problem with the gradients is that some of them were small while others may be huge. So, defining a single learning rate might not be the best idea. RPPROP uses the sign of the gradient adapting the step size individually for each weight. In this algorithm, the two gradients are first compared for signs. If they have the same sign, we’re going in the right direction and hence increase the step size by a small fraction. Whereas, if they have opposite signs, we have to decrease the step size. Then we limit the step size, and now we can go for the weight update.The problem with RPPROP is that it doesn’t work well with large datasets and when we want to perform mini-batch updates. So, achieving the robustness of RPPROP and efficiency of mini-batches at the same time was the main motivation behind the rise of RMS prop. RMS prop can also be considered an advancement in AdaGrad optimizer as it reduces the monotonically decreasing learning rate.The algorithm mainly focuses on accelerating the optimization process by decreasing the number of function evaluations to reach the local minima. The algorithm keeps the moving average of squared gradients for every weight and divides the gradient by the square root of the mean square.\\[v(w,t):=\\gamma v(w,t-1)+(1-\\gamma)(\\bigtriangledown Q_{i}(w))^2\\]where, $\\gamma$ is the forgetting factor. Weights are updated by the below formula\\(w:= w - \\frac{\\eta}{\\sqrt{v(w,t)}}\\bigtriangledown Q_{i}(w)\\)In simpler terms, if there exists a parameter due to which the cost function oscillates a lot, we want to penalize the update of this parameter.Suppose you built a model to classify a variety of fishes. The model relies on the factor ‘color’ mainly to differentiate between the fishes. Due to which it makes a lot of errors. What RMS Prop does is, penalize the parameter ‘color’ so that it can rely on other features too. This prevents the algorithm from adapting too quickly to changes in the parameter ‘color’ compared to other parameters. This algorithm has several benefits as compared to earlier versions of gradient descent algorithms. The algorithm converges quickly and requires lesser tuning than gradient descent algorithms and their variants.Disadvantages of RMSProp: The problem with RMS Prop is that the learning rate has to be defined manually and the suggested value doesn’t work for every application.AdaDeltaAdaDelta can be seen as a more robust version of AdaGrad optimizer. It is based upon adaptive learning and is designed to deal with significant drawbacks of AdaGrad and RMS prop optimizer. The main problem with the above two optimizers is that the initial learning rate must be defined manually. One other problem is the decaying learning rate which becomes infinitesimally small at some point. Due to which a certain number of iterations later, the model can no longer learn new knowledge.To deal with these problems, AdaDelta uses two state variables to store the leaky average of the second moment gradient and a leaky average of the second moment of change of parameters in the model.Here, $S_{t}$ and $\\delta X_t$ denotes the state variables, $g’t$ denotes the rescaled gradient, $\\delta X{t-1}$ denotes squares rescaled gradients, and epsilon represents a small positive integer to handle divison by 0.Adam OptimizerThe name adam is derived from adaptive moment estimation. This optimization algorithm is a further extension of stochastic gradient descent to update network weights during training. Unlike maintaining a single learning rate through training in SGD, Adam optimizer updates the learning rate for each network weight individually. The creators of the Adam optimization algorithm know the benefits of AdaGrad and RMSProp algorithms, which are also extensions of the stochastic gradient descent algorithms. Hence the Adam optimizers inherit the features of both Adagrad and RMS prop algorithms. In adam, instead of adapting learning rates based upon the first moment(mean) as in RMS Prop, it also uses the second moment of the gradients. We mean the uncentred variance by the second moment of the gradients(we don’t subtract the mean).The adam optimizer has several benefits, due to which it is used widely. It is adapted as a benchmark for deep learning papers and recommended as a default optimization algorithm. Moreover, the algorithm is straightforward to implement, has faster running time, low memory requirements, and requires less tuning than any other optimization algorithm.\\[m_t = \\beta_1 m_{t-1} + (1-\\beta_1)[\\frac{\\delta L}{\\delta w_t}]v_t= \\beta_2v_{t-1} + (1-\\beta_2)[\\frac{\\delta L}{\\delta w_t}]^2\\]The above formula represents the working of adam optimizer. Here $\\beta_1$ and $\\beta_2$ represent the decay rate of the average of the gradients.Notes: If the adam optimizer uses the good properties of all the algorithms and is the best available optimizer, then why shouldn’t you use Adam in every application? And what was the need to learn about other algorithms in depth? This is because even Adam has some downsides. It tends to focus on faster computation time, whereas algorithms like stochastic gradient descent focus on data points. That’s why algorithms like SGD generalize the data in a better manner at the cost of low computation speed. So, the optimization algorithms can be picked accordingly depending upon the requirements and the type of data." } ]
